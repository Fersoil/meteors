{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"meteors","text":"<p>draft of the package for creating explanations of hyperspectral and multispectral images</p> <p>Documentation: https://xai4space.github.io/meteors/latest/</p>"},{"location":"#how-to-run","title":"How to Run","text":"<pre><code>rye pin &lt;python version between '3.9' and '3.12'&gt;\nrye sync\n</code></pre>"},{"location":"changelog/","title":"Changelog \ud83d\udcdd","text":""},{"location":"changelog/#hyperxai-001","title":"HyperXAI 0.0.1","text":"<ul> <li>Prepared a simple draft of package along with some ideas and sample files for implementation of LIME for hyperspectral images.</li> <li>Segmentation mask for LIME using slic</li> <li>Spatial attributions using LIME</li> </ul>"},{"location":"changelog/#hyperxai-002","title":"HyperXAI 0.0.2","text":"<ul> <li>Refined package structure - simple modules for models and visualisation, installation using toml file</li> <li>Spectral attributions using LIME</li> <li>CUDA compatibility of LIME</li> </ul>"},{"location":"changelog/#hyperxai-meteors-001","title":"HyperXAI -&gt; meteors 0.0.1","text":"<ul> <li>Renamed package to meteors</li> </ul>"},{"location":"how-to-guides/","title":"How-to guide","text":"<p>TODO: Add a how-to guide here.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>TODO: Add a quickstart guide here.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Structure:</p> <ul> <li>API Reference   \u00a0 - Meteors   \u00a0 \u00a0 - Attribution Methods   \u00a0 \u00a0 \u00a0 - Lime   \u00a0 \u00a0 \u00a0 - Lime Base   \u00a0 \u00a0 \u00a0 - Integrated Gradients   \u00a0 \u00a0 \u00a0 - InputXGradients   \u00a0 \u00a0 \u00a0 - Occlusion   \u00a0 \u00a0 \u00a0 - Saliency   \u00a0 \u00a0 \u00a0 - Noise Tunnel   \u00a0 \u00a0 \u00a0 - Hyper Noise Tunnel   \u00a0 \u00a0 - HyperSpectral Image</li> </ul>"},{"location":"reference/#meteors","title":"Meteors","text":"<p>The architecture of the package can be seen on the UML diagram: </p>"},{"location":"reference/#attribution-methods","title":"Attribution Methods","text":"<p>This module consists of all the package attribution methods. It is heavily inspired by the <code>attr</code> module of the <code>captum</code> package but is adapted to the multispectral images.</p>"},{"location":"reference/#src.meteors.attr.Explainer","title":"<code>Explainer</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Explainer class for explaining models.</p> <p>Parameters:</p> Name Type Description Default <code>callable</code> <code>ExplainableModel | Explainer</code> <p>The explainable model to be explained, or another explainer - used for chaining explainers such as NoiseTunnel.</p> required Source code in <code>src/meteors/attr/explainer.py</code> <pre><code>class Explainer(ABC):\n    \"\"\"Explainer class for explaining models.\n\n    Args:\n        callable (ExplainableModel | Explainer): The explainable model to be explained, or another explainer - used for chaining explainers such as NoiseTunnel.\n    \"\"\"\n\n    def __init__(self, callable: ExplainableModel | Explainer):\n        if not isinstance(callable, ExplainableModel) and not isinstance(callable, Explainer):\n            raise TypeError(f\"Expected ExplainableModel or Explainer as callable, but got {type(callable)}\")\n        self.chained_explainer = None\n        self._attribution_method: Attribution | None = (\n            None  # the inner attribution method coming from the captum library\n        )\n\n        self._is_final_explainer = (\n            True  # flag to check whether there is a situation: Explainer(Explainer(Explainer(ExplainableModel)))\n        )\n\n        if isinstance(callable, Explainer):\n            self._is_final_explainer = False\n            if isinstance(callable.explainable_model, Explainer) or not callable._is_final_explainer:\n                raise ValueError(\"Cannot chain Explainer with another Explainer. The maximum depth of chaining is 1\")\n            self.chained_explainer = callable\n            self.explainable_model: ExplainableModel = callable.explainable_model\n            logger.debug(\n                f\"Initializing {self.__class__.__name__} explainer on model {callable.explainable_model} chained with {callable.__class__.__name__}\"\n            )\n        else:\n            self.explainable_model = callable\n            logger.debug(f\"Initializing {self.__class__.__name__} explainer on model {callable}\")\n\n        self.forward_func = self.explainable_model.forward_func\n\n    attribute: Callable\n    \"\"\"Default method to attribute the input to the model.\n\n    Args:\n        hsi (HSI): The input hsi to be explained.\n        target (int | None, optional): The target class index to be explained. Defaults to None.\n        *args: Variable length argument list.\n        **kwargs: Arbitrary keyword arguments.\n\n    Returns:\n        HSIAttributes: The hsi attributes.\n    \"\"\"\n\n    def has_convergence_delta(self) -&gt; bool:\n        \"\"\"Check if the explainer has a convergence delta.\n\n        Returns:\n            bool: True if the explainer has a convergence delta, False otherwise.\n        \"\"\"\n        if self._attribution_method is not None and hasattr(self._attribution_method, \"has_convergence_delta\"):\n            return self._attribution_method.has_convergence_delta()\n        return False\n\n    @classmethod\n    def get_name(cls: Type[\"Explainer\"]) -&gt; str:\n        \"\"\"Get the name of the explainer.\n\n        Returns:\n            str: The name of the explainer.\n        \"\"\"\n        return \"\".join([char if char.islower() or idx == 0 else \" \" + char for idx, char in enumerate(cls.__name__)])\n\n    def compute_convergence_delta(self):\n        \"\"\"Compute the convergence delta of the explainer.\n\n        Raises:\n            NotImplementedError: _description_\n        \"\"\"\n        raise NotImplementedError(\"Convergence delta computation not implemented in the explainer base class\")\n\n    @property\n    def multiplies_by_inputs(self) -&gt; bool:\n        if self._attribution_method is not None and hasattr(self._attribution_method, \"multiplies_by_inputs\"):\n            return self._attribution_method.multiplies_by_inputs\n        return False\n\n    @cached_property\n    def device(self) -&gt; torch.device:\n        \"\"\"Get the device on which the explainable model is located.\n\n        Returns:\n            torch.device: The device on which the explainable model is located.\n        \"\"\"\n        try:\n            device = next(self.explainable_model.forward_func.parameters()).device  # type: ignore\n        except Exception:\n            logger.debug(\"Could not extract device from the explainable model, setting device to cpu\")\n            logger.warning(\"Not a torch model, setting device to cpu\")\n            device = torch.device(\"cpu\")\n        return device\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Move the explainable model to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the explainable model to.\n\n        Returns:\n            Self: The updated Explainer instance.\n        \"\"\"\n        self.explainable_model = self.explainable_model.to(device)\n        return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.Explainer.attribute","title":"<code>attribute: Callable</code>  <code>instance-attribute</code>","text":"<p>Default method to attribute the input to the model.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hsi to be explained.</p> required <code>target</code> <code>int | None</code> <p>The target class index to be explained. Defaults to None.</p> required <code>*args</code> <p>Variable length argument list.</p> required <code>**kwargs</code> <p>Arbitrary keyword arguments.</p> required <p>Returns:</p> Name Type Description <code>HSIAttributes</code> <code>Callable</code> <p>The hsi attributes.</p>"},{"location":"reference/#src.meteors.attr.Explainer.device","title":"<code>device: torch.device</code>  <code>cached</code> <code>property</code>","text":"<p>Get the device on which the explainable model is located.</p> <p>Returns:</p> Type Description <code>device</code> <p>torch.device: The device on which the explainable model is located.</p>"},{"location":"reference/#src.meteors.attr.Explainer.compute_convergence_delta","title":"<code>compute_convergence_delta()</code>","text":"<p>Compute the convergence delta of the explainer.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>description</p> Source code in <code>src/meteors/attr/explainer.py</code> <pre><code>def compute_convergence_delta(self):\n    \"\"\"Compute the convergence delta of the explainer.\n\n    Raises:\n        NotImplementedError: _description_\n    \"\"\"\n    raise NotImplementedError(\"Convergence delta computation not implemented in the explainer base class\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.Explainer.get_name","title":"<code>get_name()</code>  <code>classmethod</code>","text":"<p>Get the name of the explainer.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the explainer.</p> Source code in <code>src/meteors/attr/explainer.py</code> <pre><code>@classmethod\ndef get_name(cls: Type[\"Explainer\"]) -&gt; str:\n    \"\"\"Get the name of the explainer.\n\n    Returns:\n        str: The name of the explainer.\n    \"\"\"\n    return \"\".join([char if char.islower() or idx == 0 else \" \" + char for idx, char in enumerate(cls.__name__)])\n</code></pre>"},{"location":"reference/#src.meteors.attr.Explainer.has_convergence_delta","title":"<code>has_convergence_delta()</code>","text":"<p>Check if the explainer has a convergence delta.</p> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the explainer has a convergence delta, False otherwise.</p> Source code in <code>src/meteors/attr/explainer.py</code> <pre><code>def has_convergence_delta(self) -&gt; bool:\n    \"\"\"Check if the explainer has a convergence delta.\n\n    Returns:\n        bool: True if the explainer has a convergence delta, False otherwise.\n    \"\"\"\n    if self._attribution_method is not None and hasattr(self._attribution_method, \"has_convergence_delta\"):\n        return self._attribution_method.has_convergence_delta()\n    return False\n</code></pre>"},{"location":"reference/#src.meteors.attr.Explainer.to","title":"<code>to(device)</code>","text":"<p>Move the explainable model to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the explainable model to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated Explainer instance.</p> Source code in <code>src/meteors/attr/explainer.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Move the explainable model to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the explainable model to.\n\n    Returns:\n        Self: The updated Explainer instance.\n    \"\"\"\n    self.explainable_model = self.explainable_model.to(device)\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSIAttributes","title":"<code>HSIAttributes</code>","text":"<p>               Bases: <code>BaseModel</code></p> <p>Represents an object that contains Hyperspectral image attributes and explanations.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation. Used only for LIME attributes</p> <code>approximation_error</code> <code>float</code> <p>Approximation error of the explanation. Used only for IG attributes</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>model_config</code> <code>ConfigDict</code> <p>Configuration dictionary for the model.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSIAttributes(BaseModel):\n    \"\"\"Represents an object that contains Hyperspectral image attributes and explanations.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation. Used only for LIME attributes\n        approximation_error (float): Approximation error of the explanation. Used only for IG attributes\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        model_config (ConfigDict): Configuration dictionary for the model.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    hsi: Annotated[\n        HSI,\n        Field(\n            description=\"Hyperspectral image object for which the explanations were created.\",\n        ),\n    ]\n    attributes: Annotated[\n        torch.Tensor,\n        BeforeValidator(validate_and_convert_attributes),\n        Field(\n            description=\"Attributions (explanations) for the hsi.\",\n        ),\n    ]\n    attribution_method: Annotated[\n        str | None,\n        BeforeValidator(validate_attribution_method),\n        Field(\n            description=\"The method used to generate the explanation.\",\n        ),\n    ] = None\n    score: Annotated[\n        float | None,\n        Field(\n            validate_default=True,\n            le=1.0,\n            ge=-1.0,\n            description=\"R^2 score of interpretable model used for the explanation. Used only for LIME attributes\",\n        ),\n    ] = None\n    approximation_error: Annotated[\n        float | None,\n        Field(\n            description=\"Approximation error of the explanation. Also known as convergence delta. Used only for IG attributes\",\n        ),\n    ] = None\n    mask: Annotated[\n        torch.Tensor | None,\n        BeforeValidator(validate_and_convert_mask),\n        Field(\n            description=\"`superpixel` or `superband` mask used for the explanation.\",\n        ),\n    ] = None\n    device: Annotated[\n        torch.device,\n        BeforeValidator(resolve_inference_device_attributes),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=(\n                \"Device to be used for inference. If None, the device of the input hsi will be used. \"\n                \"Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes.\n\n        This method should be implemented in the subclass.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        raise NotImplementedError(\"This method should be implemented in the subclass\")\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @property\n    def orientation(self) -&gt; tuple[str, str, str]:\n        \"\"\"Returns the orientation of the hsi.\n\n        Returns:\n            tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.\n        \"\"\"\n        return self.hsi.orientation\n\n    def _validate_hsi_attributions_and_mask(self) -&gt; None:\n        \"\"\"Validates the hsi attributions and performs necessary operations to ensure compatibility with the device.\n\n        Raises:\n            ValueError: If the shapes of the attributes and hsi tensors do not match.\n        \"\"\"\n        validate_shapes(self.attributes, self.hsi)\n\n        self.attributes = self.attributes.to(self.device)\n        if self.device != self.hsi.device:\n            self.hsi.to(self.device)\n\n        if self.mask is not None:\n            validate_shapes(self.mask, self.hsi)\n            self.mask = self.mask.to(self.device)\n\n    @model_validator(mode=\"after\")\n    def validate_hsi_attributions(self) -&gt; Self:\n        \"\"\"Validates the hsi attributions.\n\n        This method performs validation on the hsi attributions to ensure they are correct.\n\n        Returns:\n            Self: The current instance of the class.\n        \"\"\"\n        self._validate_hsi_attributions_and_mask()\n        return self\n\n    @model_validator(mode=\"after\")\n    def validate_score_and_error(self) -&gt; Self:\n        \"\"\"Validates the score and error attributes.\n\n        This method validates the score and error attributes based on the attribution method.\n\n        Returns:\n            Self: The current instance of the class.\n        \"\"\"\n        if (self.attribution_method is None or self.attribution_method.title() != \"Lime\") and self.score is not None:\n            logger.warning(\"Score should not be provided for non-LIME attributes\")\n        if self.attribution_method is not None and self.attribution_method.title() == \"Lime\" and self.score is None:\n            raise ValueError(\"Score must be provided for LIME attributes\")\n        if (\n            self.attribution_method is None or self.attribution_method.title() != \"Integrated Gradients\"\n        ) and self.approximation_error is not None:\n            logger.warning(\"Approximation error should not be provided for non-IG attributes\")\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Move the hsi and attributes tensors to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the tensors to.\n\n        Returns:\n            Self: The modified object with tensors moved to the specified device.\n\n        Examples:\n            &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n            &gt;&gt;&gt; attrs.to(\"cpu\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cpu')\n            &gt;&gt;&gt; attrs.to(\"cuda\")\n            &gt;&gt;&gt; attrs.hsi.device\n            device(type='cuda')\n            &gt;&gt;&gt; attrs.attributes.device\n            device(type='cuda')\n        \"\"\"\n        self.hsi = self.hsi.to(device)\n        self.attributes = self.attributes.to(device)\n        self.device = self.hsi.device\n        return self\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n        Returns:\n            Self: The updated Image object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        current_orientation = self.orientation\n        hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n        if inplace:\n            attrs = self\n        else:\n            attrs = self.model_copy()\n            attrs.hsi = hsi\n\n        # now change the orientation of the attributes\n        if current_orientation == target_orientation:\n            return attrs\n\n        permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n        attrs.attributes = attrs.attributes.permute(permute_dims)\n\n        if attrs.mask is not None:\n            attrs.mask = attrs.mask.permute(permute_dims)\n        return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSIAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes.</p> <p>This method should be implemented in the subclass.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.HSIAttributes.orientation","title":"<code>orientation: tuple[str, str, str]</code>  <code>property</code>","text":"<p>Returns the orientation of the hsi.</p> <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>tuple[str, str, str]: The orientation of the hsi corresponding to the attributes.</p>"},{"location":"reference/#src.meteors.attr.HSIAttributes.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the image data along with the attributions to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>tuple[str, str, str] | list[str] | str</code> <p>The target orientation for the attribution data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated Image object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the image data along with the attributions to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str] | list[str] | str): The target orientation for the attribution data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the data in place or return a new object.\n\n    Returns:\n        Self: The updated Image object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    current_orientation = self.orientation\n    hsi = self.hsi.change_orientation(target_orientation, inplace=inplace)\n    if inplace:\n        attrs = self\n    else:\n        attrs = self.model_copy()\n        attrs.hsi = hsi\n\n    # now change the orientation of the attributes\n    if current_orientation == target_orientation:\n        return attrs\n\n    permute_dims = [current_orientation.index(dim) for dim in target_orientation]\n\n    attrs.attributes = attrs.attributes.permute(permute_dims)\n\n    if attrs.mask is not None:\n        attrs.mask = attrs.mask.permute(permute_dims)\n    return attrs\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSIAttributes.to","title":"<code>to(device)</code>","text":"<p>Move the hsi and attributes tensors to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the tensors to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The modified object with tensors moved to the specified device.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n&gt;&gt;&gt; attrs.to(\"cpu\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cpu')\n&gt;&gt;&gt; attrs.to(\"cuda\")\n&gt;&gt;&gt; attrs.hsi.device\ndevice(type='cuda')\n&gt;&gt;&gt; attrs.attributes.device\ndevice(type='cuda')\n</code></pre> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Move the hsi and attributes tensors to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the tensors to.\n\n    Returns:\n        Self: The modified object with tensors moved to the specified device.\n\n    Examples:\n        &gt;&gt;&gt; attrs = HSIAttributes(hsi, attributes, score=0.5)\n        &gt;&gt;&gt; attrs.to(\"cpu\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cpu')\n        &gt;&gt;&gt; attrs.to(\"cuda\")\n        &gt;&gt;&gt; attrs.hsi.device\n        device(type='cuda')\n        &gt;&gt;&gt; attrs.attributes.device\n        device(type='cuda')\n    \"\"\"\n    self.hsi = self.hsi.to(device)\n    self.attributes = self.attributes.to(device)\n    self.device = self.hsi.device\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSIAttributes.validate_hsi_attributions","title":"<code>validate_hsi_attributions()</code>","text":"<p>Validates the hsi attributions.</p> <p>This method performs validation on the hsi attributions to ensure they are correct.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The current instance of the class.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_hsi_attributions(self) -&gt; Self:\n    \"\"\"Validates the hsi attributions.\n\n    This method performs validation on the hsi attributions to ensure they are correct.\n\n    Returns:\n        Self: The current instance of the class.\n    \"\"\"\n    self._validate_hsi_attributions_and_mask()\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSIAttributes.validate_score_and_error","title":"<code>validate_score_and_error()</code>","text":"<p>Validates the score and error attributes.</p> <p>This method validates the score and error attributes based on the attribution method.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The current instance of the class.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_score_and_error(self) -&gt; Self:\n    \"\"\"Validates the score and error attributes.\n\n    This method validates the score and error attributes based on the attribution method.\n\n    Returns:\n        Self: The current instance of the class.\n    \"\"\"\n    if (self.attribution_method is None or self.attribution_method.title() != \"Lime\") and self.score is not None:\n        logger.warning(\"Score should not be provided for non-LIME attributes\")\n    if self.attribution_method is not None and self.attribution_method.title() == \"Lime\" and self.score is None:\n        raise ValueError(\"Score must be provided for LIME attributes\")\n    if (\n        self.attribution_method is None or self.attribution_method.title() != \"Integrated Gradients\"\n    ) and self.approximation_error is not None:\n        logger.warning(\"Approximation error should not be provided for non-IG attributes\")\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSISpatialAttributes","title":"<code>HSISpatialAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents spatial attributes of an hsi used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral image object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>model_config</code> <code>ConfigDict</code> <p>Configuration dictionary for the model.</p> <code>segmentation_mask</code> <code>Tensor</code> <p>Spatial (Segmentation) mask used for the explanation.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpatialAttributes(HSIAttributes):\n    \"\"\"Represents spatial attributes of an hsi used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral image object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        model_config (ConfigDict): Configuration dictionary for the model.\n        segmentation_mask (torch.Tensor): Spatial (Segmentation) mask used for the explanation.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    @property\n    def segmentation_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns the 3D spatial segmentation mask that has the same size as the hsi image.\n\n        Returns:\n            torch.Tensor: The segmentation mask tensor.\n        Raises:\n            ValueError: If the segmentation mask is not provided in the attributes.\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Segmentation mask is not provided\")\n        return self.mask\n\n    @property\n    def flattened_segmentation_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns the flattened segmentation mask as a flattened 2D tensor, with removed repeated dimensions.\n\n        This method selects the segmentation mask along the specified dimension (spectral axis)\n        and returns the first index.\n\n        Returns:\n            torch.Tensor: The spatial segmentation mask.\n\n        Examples:\n            &gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n            &gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n            &gt;&gt;&gt; attrs.segmentation_mask\n            tensor([[0., 0.],\n                    [0., 0.]])\n        \"\"\"\n        return self.segmentation_mask.select(dim=self.hsi.spectral_axis, index=0)\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes, with removed repeated dimensions.\n\n        In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        &gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n        &gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n        &gt;&gt;&gt; attrs.flattened_attributes\n            tensor([[0., 0.],\n                    [0., 0.]])\n        \"\"\"\n        return self.attributes.select(dim=self.hsi.spectral_axis, index=0)\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSISpatialAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes, with removed repeated dimensions.</p> <p>In the case of spatial attributes, the flattened attributes are 2D spatial attributes of shape (rows, columns) and the spectral dimension is removed.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p> <p>segmentation_mask = torch.zeros((3, 2, 2)) attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask) attrs.flattened_attributes     tensor([[0., 0.],             [0., 0.]])</p>"},{"location":"reference/#src.meteors.attr.HSISpatialAttributes.flattened_segmentation_mask","title":"<code>flattened_segmentation_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the flattened segmentation mask as a flattened 2D tensor, with removed repeated dimensions.</p> <p>This method selects the segmentation mask along the specified dimension (spectral axis) and returns the first index.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The spatial segmentation mask.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; segmentation_mask = torch.zeros((3, 2, 2))\n&gt;&gt;&gt; attrs = HSISpatialAttributes(hsi, attributes, score=0.5, segmentation_mask=segmentation_mask)\n&gt;&gt;&gt; attrs.segmentation_mask\ntensor([[0., 0.],\n        [0., 0.]])\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSISpatialAttributes.segmentation_mask","title":"<code>segmentation_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns the 3D spatial segmentation mask that has the same size as the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask tensor.</p> <p>Raises:     ValueError: If the segmentation mask is not provided in the attributes.</p>"},{"location":"reference/#src.meteors.attr.HSISpectralAttributes","title":"<code>HSISpectralAttributes</code>","text":"<p>               Bases: <code>HSIAttributes</code></p> <p>Represents an hsi with spectral attributes used for explanation.</p> <p>Attributes:</p> Name Type Description <code>hsi</code> <code>HSI</code> <p>Hyperspectral hsi object for which the explanations were created.</p> <code>attributes</code> <code>Tensor</code> <p>Attributions (explanations) for the hsi.</p> <code>score</code> <code>float</code> <p>R^2 score of interpretable model used for the explanation.</p> <code>device</code> <code>device</code> <p>Device to be used for inference. If None, the device of the input hsi will be used. Defaults to None.</p> <code>model_config</code> <code>ConfigDict</code> <p>Configuration dictionary for the model.</p> <code>band_mask</code> <code>Tensor</code> <p>Band mask used for the explanation.</p> <code>band_names</code> <code>dict[str, int]</code> <p>Dictionary that translates the band names into the band segment ids.</p> <code>attribution_method</code> <code>str | None</code> <p>The method used to generate the explanation. Defaults to None.</p> Source code in <code>src/meteors/attr/attributes.py</code> <pre><code>class HSISpectralAttributes(HSIAttributes):\n    \"\"\"Represents an hsi with spectral attributes used for explanation.\n\n    Attributes:\n        hsi (HSI): Hyperspectral hsi object for which the explanations were created.\n        attributes (torch.Tensor): Attributions (explanations) for the hsi.\n        score (float): R^2 score of interpretable model used for the explanation.\n        device (torch.device): Device to be used for inference. If None, the device of the input hsi will be used.\n            Defaults to None.\n        model_config (ConfigDict): Configuration dictionary for the model.\n        band_mask (torch.Tensor): Band mask used for the explanation.\n        band_names (dict[str, int]): Dictionary that translates the band names into the band segment ids.\n        attribution_method (str | None): The method used to generate the explanation. Defaults to None.\n    \"\"\"\n\n    band_names: Annotated[\n        dict[str, int],\n        Field(\n            description=\"Dictionary that translates the band names into the band segment ids.\",\n        ),\n    ]\n\n    @property\n    def flattened_band_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened band mask - a band mask with removed repeated dimensions\n        The flattened_band_mask is a 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.\n\n        The method selects the appropriate dimensions from the `band_mask` tensor\n        based on the `axis_to_select` and returns a flattened version of the selected\n        tensor.\n\n        Returns:\n            torch.Tensor: The flattened band mask tensor.\n\n        Examples:\n            &gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n            &gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, band_mask=band_mask)\n            &gt;&gt;&gt; attrs.flattened_band_mask\n            torch.tensor([0, 1, 2])\n        \"\"\"\n        axis_to_select = HSI_AXIS_ORDER.copy()\n        axis_to_select.remove(self.hsi.spectral_axis)\n        return self.band_mask.select(dim=axis_to_select[0], index=0).select(dim=axis_to_select[1], index=0)\n\n    @property\n    def band_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a band mask that has the full size - the same size as the hsi image.\n\n        Returns:\n            torch.Tensor: The band mask tensor.\n        Raises:\n            ValueError: If the band mask is not provided in the attributes.\n\n        \"\"\"\n        if self.mask is None:\n            raise ValueError(\"Band mask is not provided\")\n        return self.mask\n\n    @property\n    def flattened_attributes(self) -&gt; torch.Tensor:\n        \"\"\"Returns a flattened tensor of attributes with removed repeated dimensions.\n\n        In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.\n\n        Returns:\n            torch.Tensor: A flattened tensor of attributes.\n        \"\"\"\n        axis = list(range(self.attributes.ndim))\n        axis.remove(self.hsi.spectral_axis)\n        return self.attributes.select(dim=axis[0], index=0).select(dim=axis[1] - 1, index=0)\n</code></pre>"},{"location":"reference/#src.meteors.attr.HSISpectralAttributes.band_mask","title":"<code>band_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a band mask that has the full size - the same size as the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The band mask tensor.</p> <p>Raises:     ValueError: If the band mask is not provided in the attributes.</p>"},{"location":"reference/#src.meteors.attr.HSISpectralAttributes.flattened_attributes","title":"<code>flattened_attributes: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened tensor of attributes with removed repeated dimensions.</p> <p>In the case of spectral attributes, the flattened attributes are 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A flattened tensor of attributes.</p>"},{"location":"reference/#src.meteors.attr.HSISpectralAttributes.flattened_band_mask","title":"<code>flattened_band_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a flattened band mask - a band mask with removed repeated dimensions The flattened_band_mask is a 1D tensor of shape (num_bands, ), where num_bands is the number of bands in the hsi image.</p> <p>The method selects the appropriate dimensions from the <code>band_mask</code> tensor based on the <code>axis_to_select</code> and returns a flattened version of the selected tensor.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The flattened band mask tensor.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; band_names = {\"R\": 0, \"G\": 1, \"B\": 2}\n&gt;&gt;&gt; attrs = HSISpectralAttributes(hsi, attributes, score=0.5, band_mask=band_mask)\n&gt;&gt;&gt; attrs.flattened_band_mask\ntorch.tensor([0, 1, 2])\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic explanation method that explains the predictions of a black-box model by approximating it with a simpler interpretable model.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel</code> <p>The explainable model to be explained.</p> required <code>interpretable_model</code> <code>InterpretableModel</code> <p>The interpretable model used to approximate the black-box model. Defaults to <code>SkLearnLasso</code> with alpha parameter set to 0.08.</p> <code>SkLearnLasso(alpha=0.08)</code> <code>similarity_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The similarity function used by Lime. Defaults to None.</p> <code>None</code> <code>perturb_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The perturbation function used by Lime. Defaults to None.</p> <code>None</code> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>class Lime(Explainer):\n    \"\"\"Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic\n    explanation method that explains the predictions of a black-box model by approximating it with a simpler\n    interpretable model.\n\n    Args:\n        explainable_model (ExplainableModel): The explainable model to be explained.\n        interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            Defaults to `SkLearnLasso` with alpha parameter set to 0.08.\n        similarity_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The similarity function used by Lime.\n            Defaults to None.\n        perturb_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The perturbation function used by Lime.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        interpretable_model: InterpretableModel = SkLearnLasso(alpha=0.08),\n        similarity_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model)\n        self.interpretable_model = interpretable_model\n        self._attribution_method: LimeBase = self._construct_lime(\n            self.explainable_model.forward_func, interpretable_model, similarity_func, perturb_func\n        )\n\n    @staticmethod\n    def _construct_lime(\n        forward_func: Callable[[torch.Tensor], torch.Tensor],\n        interpretable_model: InterpretableModel,\n        similarity_func: Callable | None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None,\n    ) -&gt; LimeBase:\n        \"\"\"Constructs the LimeBase object.\n\n        Args:\n            forward_func (Callable[[torch.Tensor], torch.Tensor]): The forward function of the explainable model.\n            interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            similarity_func (Callable | None): The similarity function used by Lime.\n            perturb_func (Callable[[torch.Tensor], torch.Tensor] | None): The perturbation function used by Lime.\n\n        Returns:\n            LimeBase: The constructed LimeBase object.\n        \"\"\"\n        return LimeBase(\n            forward_func=forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=similarity_func,\n            perturb_func=perturb_func,\n        )\n\n    @staticmethod\n    def get_segmentation_mask(\n        hsi: HSI,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n        Args:\n            hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n            segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n                Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n        Returns:\n            torch.Tensor: The segmentation mask as a tensor.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If an unsupported segmentation method is specified.\n\n        Examples:\n            &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n            &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n            torch.tensor([[1, 1],\n                          [1, 1]])\n            &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n            torch.tensor([[2, 2],\n                          [2, 2]])\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        if segmentation_method == \"slic\":\n            return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n        elif segmentation_method == \"patch\":\n            return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n        else:\n            raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n\n    @staticmethod\n    def get_band_mask(\n        hsi: HSI,\n        band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n        band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n        band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Generates a band mask based on the provided hsi and band information.\n\n        Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n        If you provide more than one, the band mask will be created using only one using the following priority:\n        band_names &gt; band_wavelengths &gt; band_indices.\n\n        Args:\n            hsi (HSI): The input hyperspectral image.\n            band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n                The names of the spectral bands to include in the mask. Defaults to None.\n            band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n                The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n            band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n                The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n            device (str | torch.device | None, optional):\n                The device to use for computation. Defaults to None.\n            repeat_dimensions (bool, optional):\n                Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n        Returns:\n            tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n            mapping band names to segment IDs.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If no band names, indices, or wavelengths are provided.\n\n        Examples:\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"R\": 1, \"G\": 2}\n            &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n            &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        assert (\n            band_names is not None or band_indices is not None or band_wavelengths is not None\n        ), \"No band names, indices, or wavelengths are provided.\"\n\n        # validate types\n        dict_labels_to_segment_ids = None\n        if band_names is not None:\n            logger.debug(\"Getting band mask from band names of spectral bands\")\n            if band_wavelengths is not None or band_indices is not None:\n                ignored_params = [\n                    param\n                    for param in [\"band_wavelengths\", \"band_indices\"]\n                    if param in locals() and locals()[param] is not None\n                ]\n                ignored_params_str = \" and \".join(ignored_params)\n                logger.info(\n                    f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                )\n            try:\n                validate_band_names(band_names)\n                band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                    hsi.wavelengths, band_names\n                )\n            except Exception as e:\n                raise ValueError(f\"Incorrect band names provided: {e}\") from e\n        elif band_wavelengths is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n            if band_indices is not None:\n                logger.info(\n                    \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                )\n            validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n            try:\n                band_groups = Lime._get_band_indices_from_band_wavelengths(\n                    hsi.wavelengths,\n                    band_wavelengths,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                ) from e\n        elif band_indices is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n            validate_band_format(band_indices, variable_name=\"band_indices\")\n            try:\n                band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                ) from e\n\n        return Lime._create_tensor_band_mask(\n            hsi,\n            band_groups,\n            dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n            device=device,\n            repeat_dimensions=repeat_dimensions,\n            return_dict_labels_to_segment_ids=True,\n        )\n\n    @staticmethod\n    def _make_band_names_indexable(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Converts a list of strings into a tuple of strings if necessary to make it indexable.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The segment name to be converted.\n\n        Returns:\n            tuple[str, ...] | str: The converted segment name.\n\n        Raises:\n            ValueError: If the segment_name is not of type list or string.\n        \"\"\"\n        if (\n            isinstance(segment_name, tuple) and all(isinstance(subitem, str) for subitem in segment_name)\n        ) or isinstance(segment_name, str):\n            return segment_name\n        elif isinstance(segment_name, list) and all(isinstance(subitem, str) for subitem in segment_name):\n            return tuple(segment_name)\n        raise ValueError(f\"Incorrect segment {segment_name} type. Should be either a list or string\")\n\n    @staticmethod\n    # @lru_cache(maxsize=32) Can't use with lists as they are not hashable\n    def _extract_bands_from_spyndex(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Extracts bands from the given segment name.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The name of the segment.\n                Users may pass either band names or indices names, as in the spyndex library.\n\n        Returns:\n            tuple[str, ...] | str: A tuple of band names if multiple bands are extracted,\n                or a single band name if only one band is extracted.\n\n        Raises:\n            ValueError: If the provided band name is invalid.\n                The band name must be either in `spyndex.indices` or `spyndex.bands`.\n        \"\"\"\n        if isinstance(segment_name, str):\n            segment_name = (segment_name,)\n        elif isinstance(segment_name, list):\n            segment_name = tuple(segment_name)\n\n        band_names_segment: list[str] = []\n        for band_name in segment_name:\n            if band_name in spyndex.indices:\n                band_names_segment += list(spyndex.indices[band_name].bands)\n            elif band_name in spyndex.bands:\n                band_names_segment.append(band_name)\n            else:\n                raise ValueError(\n                    f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                )\n\n        return tuple(set(band_names_segment)) if len(band_names_segment) &gt; 1 else band_names_segment[0]\n\n    @staticmethod\n    def _get_indices_from_wavelength_indices_range(\n        wavelengths: torch.Tensor, ranges: list[tuple[int, int]] | tuple[int, int]\n    ) -&gt; list[int]:\n        \"\"\"Converts wavelength indices ranges to list indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[int, int]] | tuple[int, int]): The wavelength indices ranges.\n\n        Returns:\n            list[int]: The indices of bands corresponding to the wavelength indices ranges.\n        \"\"\"\n        validated_ranges_list = validate_segment_format(ranges)\n        validated_ranges_list = adjust_and_validate_segment_ranges(wavelengths, validated_ranges_list)\n\n        return list(\n            set(\n                chain.from_iterable(\n                    [list(range(int(validated_range[0]), int(validated_range[1]))) for validated_range in ranges]  # type: ignore\n                )\n            )\n        )\n\n    @staticmethod\n    def _get_band_wavelengths_indices_from_band_names(\n        wavelengths: torch.Tensor,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n        \"\"\"Extracts band wavelengths indices from the given band names.\n\n        This function takes a list or dictionary of band names or segments and extracts the list of wavelengths indices\n        associated with each segment. It returns a tuple containing a dictionary with mapping segment labels into\n        wavelength indices and a dictionary mapping segment labels into segment ids.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]):\n                A list or dictionary with band names or segments.\n\n        Returns:\n            tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n                A tuple containing the dictionary with mapping segment labels into wavelength indices and the mapping\n                from segment labels into segment ids.\n        \"\"\"\n        if isinstance(band_names, list):\n            logger.debug(\"band_names is a list of segments, creating a dictionary of segments\")\n            band_names_hashed = [Lime._make_band_names_indexable(segment) for segment in band_names]\n            dict_labels_to_segment_ids = {segment: idx + 1 for idx, segment in enumerate(band_names_hashed)}\n            segments_list = band_names_hashed\n        elif isinstance(band_names, dict):\n            dict_labels_to_segment_ids = band_names.copy()\n            segments_list = tuple(band_names.keys())  # type: ignore\n        else:\n            raise ValueError(\"Incorrect band_names type. It should be a dict or a list\")\n        segments_list_after_mapping = [Lime._extract_bands_from_spyndex(segment) for segment in segments_list]\n        band_indices: dict[tuple[str, ...] | str, list[int]] = {}\n        for original_segment, segment in zip(segments_list, segments_list_after_mapping):\n            try:\n                segment_indices_ranges: list[tuple[int, int]] = []\n                for band_name in segment:\n                    segment_indices_ranges += Lime._convert_wavelengths_to_indices(\n                        wavelengths, (spyndex.bands[band_name].min_wavelength, spyndex.bands[band_name].max_wavelength)\n                    )\n\n                segment_list = Lime._get_indices_from_wavelength_indices_range(wavelengths, segment_indices_ranges)\n                band_indices[original_segment] = segment_list\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {original_segment} and bands {segment}\") from e\n        return band_indices, dict_labels_to_segment_ids\n\n    @staticmethod\n    def _convert_wavelengths_to_indices(\n        wavelengths: torch.Tensor, ranges: list[tuple[float, float]] | tuple[float, float]\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Converts wavelength ranges to index ranges.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[float, float]] | tuple[float, float]): The wavelength ranges.\n\n        Returns:\n            list[tuple[int, int]]: The index ranges corresponding to the wavelength ranges.\n        \"\"\"\n        indices = []\n        if isinstance(ranges, tuple):\n            ranges = [ranges]\n\n        for start, end in ranges:\n            start_idx = torch.searchsorted(wavelengths, start, side=\"left\")\n            end_idx = torch.searchsorted(wavelengths, end, side=\"right\")\n            indices.append((start_idx.item(), end_idx.item()))\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_band_wavelengths(\n        wavelengths: torch.Tensor,\n        band_wavelengths: dict[str | tuple[str, ...], ListOfWavelengths],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Converts the ranges or list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_wavelengths (dict): A dictionary mapping segment labels to wavelength list or ranges.\n\n        Returns:\n            dict: A dictionary mapping segment labels to index ranges.\n\n        Raises:\n            ValueError: If band_wavelengths is not a dictionary.\n        \"\"\"\n        if not isinstance(band_wavelengths, dict):\n            raise ValueError(\"band_wavelengths should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, segment in band_wavelengths.items():\n            try:\n                dtype = torch_dtype_to_python_dtype(wavelengths.dtype)\n                if isinstance(segment, (float, int)):\n                    segment = [dtype(segment)]  # type: ignore\n                if isinstance(segment, list) and all(isinstance(x, (float, int)) for x in segment):\n                    segment_dtype = change_dtype_of_list(segment, dtype)\n                    indices = Lime._convert_wavelengths_list_to_indices(wavelengths, segment_dtype)  # type: ignore\n                else:\n                    if isinstance(segment, list):\n                        segment_dtype = [\n                            tuple(change_dtype_of_list(list(ranges), dtype))  # type: ignore\n                            for ranges in segment\n                        ]\n                    else:\n                        segment_dtype = tuple(change_dtype_of_list(segment, dtype))\n\n                    valid_segment_range = validate_segment_format(segment_dtype, dtype)\n                    range_indices = Lime._convert_wavelengths_to_indices(wavelengths, valid_segment_range)  # type: ignore\n                    valid_indices_format = validate_segment_format(range_indices)\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}: {e}\") from e\n\n            band_indices[segment_label] = indices\n\n        return band_indices\n\n    @staticmethod\n    def _convert_wavelengths_list_to_indices(wavelengths: torch.Tensor, ranges: list[float]) -&gt; list[int]:\n        \"\"\"Converts a list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[float]): The list of wavelengths.\n\n        Returns:\n            list[int]: The indices corresponding to the wavelengths.\n        \"\"\"\n        indices = []\n        for wavelength in ranges:\n            index = (wavelengths == wavelength).nonzero(as_tuple=False)\n            number_of_elements = torch.numel(index)\n            if number_of_elements == 1:\n                indices.append(index.item())\n            elif number_of_elements == 0:\n                raise ValueError(f\"Couldn't find wavelength of value {wavelength} in list of wavelength\")\n            else:\n                raise ValueError(f\"Wavelength of value {wavelength} was present more than once in list of wavelength\")\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_input_band_indices(\n        wavelengths: torch.Tensor,\n        input_band_indices: dict[str | tuple[str, ...], ListOfWavelengthsIndices],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Get band indices from band list or ranges indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_indices (dict[str | tuple[str, ...], ListOfWavelengthsIndices]):\n                A dictionary mapping segment labels to a list of wavelength indices.\n\n        Returns:\n            dict[str | tuple[str, ...], list[int]]: A dictionary mapping segment labels to a list of band indices.\n\n        Raises:\n            ValueError: If `band_indices` is not a dictionary.\n        \"\"\"\n        if not isinstance(input_band_indices, dict):\n            raise ValueError(\"band_indices should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, indices in input_band_indices.items():\n            try:\n                if isinstance(indices, int):\n                    indices = [indices]  # type: ignore\n                if isinstance(indices, list) and all(isinstance(x, int) for x in indices):\n                    indices: list[int] = indices  # type: ignore\n                else:\n                    valid_indices_format = validate_segment_format(indices)  # type: ignore\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)  # type: ignore\n\n                band_indices[segment_label] = indices  # type: ignore\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}\") from e\n\n        return band_indices\n\n    @staticmethod\n    def _check_overlapping_segments(hsi: HSI, dict_labels_to_indices: dict[str | tuple[str, ...], list[int]]) -&gt; None:\n        \"\"\"Check for overlapping segments in the given hsi.\n\n        Args:\n            hsi (HSI): The hsi object containing the wavelengths.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping segment labels to indices.\n\n        Returns:\n            None\n        \"\"\"\n        overlapping_segments: dict[int, str | tuple[str, ...]] = {}\n        for segment_label, indices in dict_labels_to_indices.items():\n            for idx in indices:\n                if hsi.wavelengths[idx].item() in overlapping_segments.keys():\n                    logger.warning(\n                        (\n                            f\"Segments {overlapping_segments[hsi.wavelengths[idx].item()]} \"\n                            f\"and {segment_label} are overlapping on wavelength {hsi.wavelengths[idx].item()}\"\n                        )\n                    )\n                overlapping_segments[hsi.wavelengths[idx].item()] = segment_label\n\n    @staticmethod\n    def _validate_and_create_dict_labels_to_segment_ids(\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None,\n        segment_labels: list[str | tuple[str, ...]],\n    ) -&gt; dict[str | tuple[str, ...], int]:\n        \"\"\"Validates and creates a dictionary mapping segment labels to segment IDs.\n\n        Args:\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None):\n                The existing mapping from segment labels to segment IDs, or None if it doesn't exist.\n            segment_labels (list[str | tuple[str, ...]]): The list of segment labels.\n\n        Returns:\n            dict[str | tuple[str, ...], int]: A tuple containing the validated dictionary mapping segment\n            labels to segment IDs and a boolean flag indicating whether the segment labels are hashed.\n\n        Raises:\n            ValueError: If the length of `dict_labels_to_segment_ids` doesn't match the length of `segment_labels`.\n            ValueError: If a segment label is not present in `dict_labels_to_segment_ids`.\n            ValueError: If there are non-unique segment IDs in `dict_labels_to_segment_ids`.\n        \"\"\"\n        if dict_labels_to_segment_ids is None:\n            logger.debug(\"Creating mapping from segment labels into ids\")\n            return {segment: idx + 1 for idx, segment in enumerate(segment_labels)}\n\n        logger.debug(\"Using existing mapping from segment labels into segment ids\")\n\n        if len(dict_labels_to_segment_ids) != len(segment_labels):\n            raise ValueError(\n                (\n                    f\"Incorrect dict_labels_to_segment_ids - length mismatch. Expected: \"\n                    f\"{len(segment_labels)}, Actual: {len(dict_labels_to_segment_ids)}\"\n                )\n            )\n\n        unique_segment_ids = set(dict_labels_to_segment_ids.values())\n        if len(unique_segment_ids) != len(segment_labels):\n            raise ValueError(\"Non unique segment ids in the dict_labels_to_segment_ids\")\n\n        logger.debug(\"Passed mapping is correct\")\n        return dict_labels_to_segment_ids\n\n    @staticmethod\n    def _create_single_dim_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int],\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create a one-dimensional band mask based on the given image, labels, and segment IDs.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping labels or label tuples to lists of indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int]):\n                A dictionary mapping labels or label tuples to segment IDs.\n            device (torch.device): The device to use for the tensor.\n\n        Returns:\n            torch.Tensor: The one-dimensional band mask tensor.\n\n        Raises:\n            ValueError: If the indices for a segment are out of bounds for the one-dimensional band mask.\n        \"\"\"\n        band_mask_single_dim = torch.zeros(len(hsi.wavelengths), dtype=torch.int64, device=device)\n\n        segment_labels = list(dict_labels_to_segment_ids.keys())\n\n        for segment_label in segment_labels[::-1]:\n            segment_indices = dict_labels_to_indices[segment_label]\n            segment_id = dict_labels_to_segment_ids[segment_label]\n            are_indices_valid = all(0 &lt;= idx &lt; band_mask_single_dim.shape[0] for idx in segment_indices)\n            if not are_indices_valid:\n                raise ValueError(\n                    (\n                        f\"Indices for segment {segment_label} are out of bounds for the one-dimensional band mask\"\n                        f\"of shape {band_mask_single_dim.shape}\"\n                    )\n                )\n            band_mask_single_dim[segment_indices] = segment_id\n\n        return band_mask_single_dim\n\n    @staticmethod\n    def _expand_band_mask(hsi: HSI, band_mask_single_dim: torch.Tensor, repeat_dimensions: bool) -&gt; torch.Tensor:\n        \"\"\"Expands the band mask to match the dimensions of the input hsi.\n\n        Args:\n            hsi (HSI): The input hsi.\n            band_mask_single_dim (torch.Tensor): The band mask tensor with a single dimension.\n            repeat_dimensions (bool): Whether to repeat the dimensions of the band mask to match the hsi.\n\n        Returns:\n            torch.Tensor: The expanded band mask tensor.\n        \"\"\"\n        if hsi.spectral_axis == 0:\n            band_mask = band_mask_single_dim.unsqueeze(-1).unsqueeze(-1)\n        elif hsi.spectral_axis == 1:\n            band_mask = band_mask_single_dim.unsqueeze(0).unsqueeze(-1)\n        elif hsi.spectral_axis == 2:\n            band_mask = band_mask_single_dim.unsqueeze(0).unsqueeze(0)\n        if repeat_dimensions:\n            size_image = hsi.image.size()\n            size_mask = band_mask.size()\n\n            repeat_dims = [s2 // s1 for s1, s2 in zip(size_mask, size_image)]\n            band_mask = band_mask.repeat(repeat_dims)\n\n        return band_mask\n\n    @staticmethod\n    def _create_tensor_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n        return_dict_labels_to_segment_ids: bool = True,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Create a tensor band mask from dictionaries. The band mask is created based on the given hsi, labels, and\n        segment IDs. The band mask is a tensor with the same shape as the input hsi and contains segment IDs, where each\n        segment is represented by a unique ID. The band mask will be used to attribute the hsi using the LIME method.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]): A dictionary mapping labels to indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None, optional):\n                A dictionary mapping labels to segment IDs. Defaults to None.\n            device (str | torch.device | None, optional): The device to use. Defaults to None.\n            repeat_dimensions (bool, optional): Whether to repeat dimensions. Defaults to False.\n            return_dict_labels_to_segment_ids (bool, optional):\n                Whether to return the dictionary mapping labels to segment IDs. Defaults to True.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n                The tensor band mask or a tuple containing the tensor band mask\n                and the dictionary mapping labels to segment IDs.\n        \"\"\"\n        if device is None:\n            device = hsi.device\n        segment_labels = list(dict_labels_to_indices.keys())\n\n        logger.debug(f\"Creating a band mask on the device {device} using {len(segment_labels)} segments\")\n\n        # Check for overlapping segments\n        Lime._check_overlapping_segments(hsi, dict_labels_to_indices)\n\n        # Create or validate dict_labels_to_segment_ids\n        dict_labels_to_segment_ids = Lime._validate_and_create_dict_labels_to_segment_ids(\n            dict_labels_to_segment_ids, segment_labels\n        )\n\n        # Create single-dimensional band mask\n        band_mask_single_dim = Lime._create_single_dim_band_mask(\n            hsi, dict_labels_to_indices, dict_labels_to_segment_ids, device\n        )\n\n        # Expand band mask to match image dimensions\n        band_mask = Lime._expand_band_mask(hsi, band_mask_single_dim, repeat_dimensions)\n\n        if return_dict_labels_to_segment_ids:\n            return band_mask, dict_labels_to_segment_ids\n        return band_mask\n\n    def attribute(\n        self, attribution_type: Literal[\"spatial\", \"spectral\"], image: HSI, target: int | None = None, **kwargs\n    ) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n        \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n        `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n        detailed description of the methods, please refer to the respective method documentation.\n\n        Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n        Args:\n            attribution_type (Literal[\"spatial\", \"spectral\"]): An attribution type to be executed.\n            image (HSI): an image on which the explanation is performed.\n            target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n        \"\"\"\n        if attribution_type == \"spatial\":\n            return self.get_spatial_attributes(image, target=target, **kwargs)\n        elif attribution_type == \"spectral\":\n            return self.get_spectral_attributes(image, target=target, **kwargs)\n        raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n\n    def get_spatial_attributes(\n        self,\n        hsi: HSI,\n        segmentation_mask: np.ndarray | torch.Tensor | None = None,\n        target: int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; HSISpatialAttributes:\n        \"\"\"\n        Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n        LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n        `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n        `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n        method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n        the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n                A segmentation mask according to which the attribution should be performed.\n                The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n                If None, a new segmentation mask is created using the `segmentation_method`.\n                    Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n               A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n               lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n               inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                   Defaults to None.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n        Returns:\n            HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n                the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spatial_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.score\n            1.0\n        \"\"\"\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n        segmentation_mask = ensure_torch_tensor(\n            segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n        )\n\n        segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n        hsi = hsi.to(self.device)\n        segmentation_mask = segmentation_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=segmentation_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spatial_attribution = HSISpatialAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=segmentation_mask.expand_as(hsi.image),\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        hsi: HSI,\n        band_mask: np.ndarray | torch.Tensor | None = None,\n        target=None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n    ) -&gt; HSISpectralAttributes:\n        \"\"\"\n        Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n        method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n        Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n        Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n        the band names, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n                The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n                If equals to None, the band mask is created within the function. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n            postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n                A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n                lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n                inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                Defaults to None.\n            band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n                the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spectral_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_mask.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_names\n            [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution.score\n            1.0\n        \"\"\"\n\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if band_mask is None:\n            band_mask, band_names = self.get_band_mask(hsi, band_names)\n        band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n        if band_mask.ndim != hsi.image.ndim:\n            band_mask = Lime._expand_band_mask(hsi, band_mask, repeat_dimensions=False)\n        band_mask = band_mask.int()\n\n        if band_names is None:\n            unique_segments = torch.unique(band_mask)\n            band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n        else:\n            # checking consistency of names\n            # unique_segments = torch.unique(band_mask)\n            # if isinstance(band_names, dict):\n            #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n            logger.debug(\"Band names are provided, using them. In future it there should be an option to validate them\")\n\n        band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n        hsi = hsi.to(self.device)\n        band_mask = band_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=band_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spectral_attribution = HSISpectralAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=band_mask.expand_as(hsi.image),\n            band_names=band_names,\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spectral_attribution\n\n    @staticmethod\n    def _get_slick_segmentation_mask(\n        hsi: HSI, num_interpret_features: int = 10, *args: Any, **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Creates a segmentation mask using the SLIC method.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            num_interpret_features (int, optional): Number of segments. Defaults to 10.\n            *args: Additional positional arguments to be passed to the SLIC method.\n            **kwargs: Additional keyword arguments to be passed to the SLIC method.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        segmentation_mask = slic(\n            hsi.get_image().cpu().detach().numpy(),\n            n_segments=num_interpret_features,\n            mask=hsi.spatial_binary_mask.cpu().detach().numpy(),\n            channel_axis=hsi.spectral_axis,\n            *args,\n            **kwargs,\n        )\n\n        if segmentation_mask.min() == 1:\n            segmentation_mask -= 1\n\n        segmentation_mask = torch.from_numpy(segmentation_mask)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        return segmentation_mask\n\n    @staticmethod\n    def _get_patch_segmentation_mask(hsi: HSI, patch_size: int | float = 10, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Creates a segmentation mask using the patch method - creates small squares of the same size\n            and assigns a unique value to each square.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            patch_size (int, optional): Size of the patch, the hsi size should be divisible by this value.\n                Defaults to 10.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        logger.warning(\"Patch segmentation only works for band_index = 0 now\")\n\n        if patch_size &lt; 1 or not isinstance(patch_size, (int, float)):\n            raise ValueError(\"Invalid patch_size. patch_size must be a positive integer\")\n\n        if hsi.image.shape[1] % patch_size != 0 or hsi.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the hsi\")\n\n        height, width = hsi.image.shape[1], hsi.image.shape[2]\n\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=hsi.device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * hsi.spatial_binary_mask\n        # segmentation_mask = torch.repeat_interleave(\n        # torch.unsqueeze(segmentation_mask, dim=hsi.spectral_axis),\n        # repeats=hsi.image.shape[hsi.spectral_axis], dim=hsi.spectral_axis)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime.attribute","title":"<code>attribute(attribution_type, image, target=None, **kwargs)</code>","text":"<p>A wrapper function to attribute the image using the LIME method. It executes either the <code>get_spatial_attributes</code> or <code>get_spectral_attributes</code> method based on the provided <code>attribution_type</code>. For more detailed description of the methods, please refer to the respective method documentation.</p> <p>Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>attribution_type</code> <code>Literal['spatial', 'spectral']</code> <p>An attribution type to be executed.</p> required <code>image</code> <code>HSI</code> <p>an image on which the explanation is performed.</p> required <code>target</code> <code>int | None</code> <p>Target output index for the explanation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | HSISpatialAttributes</code> <p>HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the <code>attribution_type</code> is <code>spatial</code>, the object is of type <code>HSISpatialAttributes</code>, otherwise it is of type <code>HSISpectralAttributes</code>.</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def attribute(\n    self, attribution_type: Literal[\"spatial\", \"spectral\"], image: HSI, target: int | None = None, **kwargs\n) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n    \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n    `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n    detailed description of the methods, please refer to the respective method documentation.\n\n    Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n    Args:\n        attribution_type (Literal[\"spatial\", \"spectral\"]): An attribution type to be executed.\n        image (HSI): an image on which the explanation is performed.\n        target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n    \"\"\"\n    if attribution_type == \"spatial\":\n        return self.get_spatial_attributes(image, target=target, **kwargs)\n    elif attribution_type == \"spectral\":\n        return self.get_spectral_attributes(image, target=target, **kwargs)\n    raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime.get_band_mask","title":"<code>get_band_mask(hsi, band_names=None, band_indices=None, band_wavelengths=None, device=None, repeat_dimensions=False)</code>  <code>staticmethod</code>","text":"<p>Generates a band mask based on the provided hsi and band information.</p> <p>Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask. If you provide more than one, the band mask will be created using only one using the following priority: band_names &gt; band_wavelengths &gt; band_indices.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image.</p> required <code>band_names</code> <code>None | list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The names of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_indices</code> <code>None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]]</code> <p>The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_wavelengths</code> <code>None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float]</code> <p>The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>The device to use for computation. Defaults to None.</p> <code>None</code> <code>repeat_dimensions</code> <code>bool</code> <p>Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary</p> <code>dict[tuple[str, ...] | str, int]</code> <p>mapping band names to segment IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If no band names, indices, or wavelengths are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n&gt;&gt;&gt; band_names = [\"R\", \"G\"]\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"R\": 1, \"G\": 2}\n&gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n&gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    hsi: HSI,\n    band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n    band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n    band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n    device: str | torch.device | None = None,\n    repeat_dimensions: bool = False,\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"Generates a band mask based on the provided hsi and band information.\n\n    Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n    If you provide more than one, the band mask will be created using only one using the following priority:\n    band_names &gt; band_wavelengths &gt; band_indices.\n\n    Args:\n        hsi (HSI): The input hyperspectral image.\n        band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n            The names of the spectral bands to include in the mask. Defaults to None.\n        band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n            The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n        band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n            The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n        device (str | torch.device | None, optional):\n            The device to use for computation. Defaults to None.\n        repeat_dimensions (bool, optional):\n            Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n        mapping band names to segment IDs.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If no band names, indices, or wavelengths are provided.\n\n    Examples:\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"R\": 1, \"G\": 2}\n        &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n        &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    assert (\n        band_names is not None or band_indices is not None or band_wavelengths is not None\n    ), \"No band names, indices, or wavelengths are provided.\"\n\n    # validate types\n    dict_labels_to_segment_ids = None\n    if band_names is not None:\n        logger.debug(\"Getting band mask from band names of spectral bands\")\n        if band_wavelengths is not None or band_indices is not None:\n            ignored_params = [\n                param\n                for param in [\"band_wavelengths\", \"band_indices\"]\n                if param in locals() and locals()[param] is not None\n            ]\n            ignored_params_str = \" and \".join(ignored_params)\n            logger.info(\n                f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n            )\n        try:\n            validate_band_names(band_names)\n            band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                hsi.wavelengths, band_names\n            )\n        except Exception as e:\n            raise ValueError(f\"Incorrect band names provided: {e}\") from e\n    elif band_wavelengths is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n        if band_indices is not None:\n            logger.info(\n                \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n            )\n        validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n        try:\n            band_groups = Lime._get_band_indices_from_band_wavelengths(\n                hsi.wavelengths,\n                band_wavelengths,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n            ) from e\n    elif band_indices is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n        validate_band_format(band_indices, variable_name=\"band_indices\")\n        try:\n            band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n            ) from e\n\n    return Lime._create_tensor_band_mask(\n        hsi,\n        band_groups,\n        dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n        device=device,\n        repeat_dimensions=repeat_dimensions,\n        return_dict_labels_to_segment_ids=True,\n    )\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime.get_segmentation_mask","title":"<code>get_segmentation_mask(hsi, segmentation_method='slic', **segmentation_method_params)</code>  <code>staticmethod</code>","text":"<p>Generates a segmentation mask for the given hsi using the specified segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image for which the segmentation mask needs to be generated.</p> required <code>segmentation_method</code> <code>Literal['patch', 'slic']</code> <p>The segmentation method to be used. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters specific to the chosen segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If an unsupported segmentation method is specified.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n&gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask[0, :2, :2]\ntorch.tensor([[1, 1],\n              [1, 1]])\n&gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\ntorch.tensor([[2, 2],\n              [2, 2]])\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_segmentation_mask(\n    hsi: HSI,\n    segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; torch.Tensor:\n    \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n    Args:\n        hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n        segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n            Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n    Returns:\n        torch.Tensor: The segmentation mask as a tensor.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If an unsupported segmentation method is specified.\n\n    Examples:\n        &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n        &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n        torch.tensor([[1, 1],\n                      [1, 1]])\n        &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n        torch.tensor([[2, 2],\n                      [2, 2]])\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    if segmentation_method == \"slic\":\n        return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n    elif segmentation_method == \"patch\":\n        return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n    else:\n        raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, segmentation_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, segmentation_method='slic', **segmentation_method_params)</code>","text":"<p>Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask LIME method attributes the <code>superpixels</code> provided by the segmentation mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations) method for spatial data. It returns an <code>HSISpatialAttributes</code> object that contains the hyperspectral image,, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>segmentation_mask</code> <code>ndarray | Tensor | None</code> <p>A segmentation mask according to which the attribution should be performed. The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter <code>C</code> in the <code>image.orientation</code> parameter. If None, a new segmentation mask is created using the <code>segmentation_method</code>.     Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor, Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.    Defaults to None.</p> <code>None</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters for the segmentation method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>HSISpatialAttributes</code> <code>HSISpatialAttributes</code> <p>A <code>HSISpatialAttributes</code> object that contains the image, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spatial_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; spatial_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: HSI,\n    segmentation_mask: np.ndarray | torch.Tensor | None = None,\n    target: int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; HSISpatialAttributes:\n    \"\"\"\n    Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n    LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n    `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n    `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n    method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n    the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n            A segmentation mask according to which the attribution should be performed.\n            The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n            If None, a new segmentation mask is created using the `segmentation_method`.\n                Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n           A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n           lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n           inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n               Defaults to None.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n    Returns:\n        HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n            the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spatial_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.score\n        1.0\n    \"\"\"\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    if segmentation_mask is None:\n        segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n    segmentation_mask = ensure_torch_tensor(\n        segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n    )\n\n    segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n    hsi = hsi.to(self.device)\n    segmentation_mask = segmentation_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=segmentation_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spatial_attribution = HSISpatialAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=segmentation_mask.expand_as(hsi.image),\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spatial_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.Lime.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, band_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, band_names=None)</code>","text":"<p>Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME method attributes the hsi based on <code>superbands</code> (clustered bands) provided by the band mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>band_mask</code> <code>ndarray | Tensor | None</code> <p>Band mask that is used for the spectral attribution. The band mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters <code>H</code> and <code>W</code> in the <code>image.orientation</code> parameter. If equals to None, the band mask is created within the function. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Specifies whether to show progress during the attribution process. Defaults to False.</p> <code>False</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor, Tensor], Tensor] | None</code> <p>(Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask. Defaults to None.</p> <code>None</code> <code>band_names</code> <code>list[str] | dict[str | tuple[str, ...], int] | None</code> <p>Band names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HSISpectralAttributes</code> <code>HSISpectralAttributes</code> <p>A HSISpectralAttributes object containing the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spectral_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_mask.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_names\n[\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: HSI,\n    band_mask: np.ndarray | torch.Tensor | None = None,\n    target=None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n) -&gt; HSISpectralAttributes:\n    \"\"\"\n    Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n    method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n    Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n    Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n    the band names, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n            The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n            If equals to None, the band mask is created within the function. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n        postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n            lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n            inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n            Defaults to None.\n        band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n            the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spectral_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_mask.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_names\n        [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution.score\n        1.0\n    \"\"\"\n\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if band_mask is None:\n        band_mask, band_names = self.get_band_mask(hsi, band_names)\n    band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n    if band_mask.ndim != hsi.image.ndim:\n        band_mask = Lime._expand_band_mask(hsi, band_mask, repeat_dimensions=False)\n    band_mask = band_mask.int()\n\n    if band_names is None:\n        unique_segments = torch.unique(band_mask)\n        band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n    else:\n        # checking consistency of names\n        # unique_segments = torch.unique(band_mask)\n        # if isinstance(band_names, dict):\n        #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n        logger.debug(\"Band names are provided, using them. In future it there should be an option to validate them\")\n\n    band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n    hsi = hsi.to(self.device)\n    band_mask = band_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=band_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spectral_attribution = HSISpectralAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=band_mask.expand_as(hsi.image),\n        band_names=band_names,\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spectral_attribution\n</code></pre>"},{"location":"reference/#lime","title":"Lime","text":"<p>This method is an adapted implementation of Lime, which supports handling multispectral images. This class also provides methods to generate aggregation masks used by the Lime method.</p> <ol> <li>segmentation mask - this mask is used in the spatial aggregation. The segmentation mask divides the pixels from the image into superpixels so that the objects and regions on the hyperspectral image might be analyzed as a whole.</li> <li>band mask - the band mask divides the channels into groups of channels that might be relevant for the model. Such groups used for the explanations could be Red, Green and Blue bands.</li> </ol>"},{"location":"reference/#src.meteors.attr.lime.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> <p>Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic explanation method that explains the predictions of a black-box model by approximating it with a simpler interpretable model.</p> <p>Parameters:</p> Name Type Description Default <code>explainable_model</code> <code>ExplainableModel</code> <p>The explainable model to be explained.</p> required <code>interpretable_model</code> <code>InterpretableModel</code> <p>The interpretable model used to approximate the black-box model. Defaults to <code>SkLearnLasso</code> with alpha parameter set to 0.08.</p> <code>SkLearnLasso(alpha=0.08)</code> <code>similarity_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The similarity function used by Lime. Defaults to None.</p> <code>None</code> <code>perturb_func</code> <code>Callable[[Tensor], Tensor] | None</code> <p>The perturbation function used by Lime. Defaults to None.</p> <code>None</code> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>class Lime(Explainer):\n    \"\"\"Lime class is a subclass of Explainer and represents the Lime explainer. Lime is an interpretable model-agnostic\n    explanation method that explains the predictions of a black-box model by approximating it with a simpler\n    interpretable model.\n\n    Args:\n        explainable_model (ExplainableModel): The explainable model to be explained.\n        interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            Defaults to `SkLearnLasso` with alpha parameter set to 0.08.\n        similarity_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The similarity function used by Lime.\n            Defaults to None.\n        perturb_func (Callable[[torch.Tensor], torch.Tensor] | None, optional): The perturbation function used by Lime.\n            Defaults to None.\n    \"\"\"\n\n    def __init__(\n        self,\n        explainable_model: ExplainableModel,\n        interpretable_model: InterpretableModel = SkLearnLasso(alpha=0.08),\n        similarity_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None = None,\n    ):\n        super().__init__(explainable_model)\n        self.interpretable_model = interpretable_model\n        self._attribution_method: LimeBase = self._construct_lime(\n            self.explainable_model.forward_func, interpretable_model, similarity_func, perturb_func\n        )\n\n    @staticmethod\n    def _construct_lime(\n        forward_func: Callable[[torch.Tensor], torch.Tensor],\n        interpretable_model: InterpretableModel,\n        similarity_func: Callable | None,\n        perturb_func: Callable[[torch.Tensor], torch.Tensor] | None,\n    ) -&gt; LimeBase:\n        \"\"\"Constructs the LimeBase object.\n\n        Args:\n            forward_func (Callable[[torch.Tensor], torch.Tensor]): The forward function of the explainable model.\n            interpretable_model (InterpretableModel): The interpretable model used to approximate the black-box model.\n            similarity_func (Callable | None): The similarity function used by Lime.\n            perturb_func (Callable[[torch.Tensor], torch.Tensor] | None): The perturbation function used by Lime.\n\n        Returns:\n            LimeBase: The constructed LimeBase object.\n        \"\"\"\n        return LimeBase(\n            forward_func=forward_func,\n            interpretable_model=interpretable_model,\n            similarity_func=similarity_func,\n            perturb_func=perturb_func,\n        )\n\n    @staticmethod\n    def get_segmentation_mask(\n        hsi: HSI,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; torch.Tensor:\n        \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n        Args:\n            hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n            segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n                Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n        Returns:\n            torch.Tensor: The segmentation mask as a tensor.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If an unsupported segmentation method is specified.\n\n        Examples:\n            &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n            &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n            &gt;&gt;&gt; segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n            torch.tensor([[1, 1],\n                          [1, 1]])\n            &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n            torch.tensor([[2, 2],\n                          [2, 2]])\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        if segmentation_method == \"slic\":\n            return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n        elif segmentation_method == \"patch\":\n            return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n        else:\n            raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n\n    @staticmethod\n    def get_band_mask(\n        hsi: HSI,\n        band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n        band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n        band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Generates a band mask based on the provided hsi and band information.\n\n        Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n        If you provide more than one, the band mask will be created using only one using the following priority:\n        band_names &gt; band_wavelengths &gt; band_indices.\n\n        Args:\n            hsi (HSI): The input hyperspectral image.\n            band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n                The names of the spectral bands to include in the mask. Defaults to None.\n            band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n                The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n            band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n                The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n            device (str | torch.device | None, optional):\n                The device to use for computation. Defaults to None.\n            repeat_dimensions (bool, optional):\n                Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n        Returns:\n            tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n            mapping band names to segment IDs.\n\n        Raises:\n            ValueError: If the input hsi is not an instance of the HSI class.\n            ValueError: If no band names, indices, or wavelengths are provided.\n\n        Examples:\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"R\": 1, \"G\": 2}\n            &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n            &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n            &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n            &gt;&gt;&gt; dict_labels_to_segment_ids\n            {\"RGB\": 1}\n        \"\"\"\n        if not isinstance(hsi, HSI):\n            raise ValueError(\"hsi should be an instance of HSI class\")\n\n        assert (\n            band_names is not None or band_indices is not None or band_wavelengths is not None\n        ), \"No band names, indices, or wavelengths are provided.\"\n\n        # validate types\n        dict_labels_to_segment_ids = None\n        if band_names is not None:\n            logger.debug(\"Getting band mask from band names of spectral bands\")\n            if band_wavelengths is not None or band_indices is not None:\n                ignored_params = [\n                    param\n                    for param in [\"band_wavelengths\", \"band_indices\"]\n                    if param in locals() and locals()[param] is not None\n                ]\n                ignored_params_str = \" and \".join(ignored_params)\n                logger.info(\n                    f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n                )\n            try:\n                validate_band_names(band_names)\n                band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                    hsi.wavelengths, band_names\n                )\n            except Exception as e:\n                raise ValueError(f\"Incorrect band names provided: {e}\") from e\n        elif band_wavelengths is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n            if band_indices is not None:\n                logger.info(\n                    \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n                )\n            validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n            try:\n                band_groups = Lime._get_band_indices_from_band_wavelengths(\n                    hsi.wavelengths,\n                    band_wavelengths,\n                )\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n                ) from e\n        elif band_indices is not None:\n            logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n            validate_band_format(band_indices, variable_name=\"band_indices\")\n            try:\n                band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n            except Exception as e:\n                raise ValueError(\n                    f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n                ) from e\n\n        return Lime._create_tensor_band_mask(\n            hsi,\n            band_groups,\n            dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n            device=device,\n            repeat_dimensions=repeat_dimensions,\n            return_dict_labels_to_segment_ids=True,\n        )\n\n    @staticmethod\n    def _make_band_names_indexable(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Converts a list of strings into a tuple of strings if necessary to make it indexable.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The segment name to be converted.\n\n        Returns:\n            tuple[str, ...] | str: The converted segment name.\n\n        Raises:\n            ValueError: If the segment_name is not of type list or string.\n        \"\"\"\n        if (\n            isinstance(segment_name, tuple) and all(isinstance(subitem, str) for subitem in segment_name)\n        ) or isinstance(segment_name, str):\n            return segment_name\n        elif isinstance(segment_name, list) and all(isinstance(subitem, str) for subitem in segment_name):\n            return tuple(segment_name)\n        raise ValueError(f\"Incorrect segment {segment_name} type. Should be either a list or string\")\n\n    @staticmethod\n    # @lru_cache(maxsize=32) Can't use with lists as they are not hashable\n    def _extract_bands_from_spyndex(segment_name: list[str] | tuple[str, ...] | str) -&gt; tuple[str, ...] | str:\n        \"\"\"Extracts bands from the given segment name.\n\n        Args:\n            segment_name (list[str] | tuple[str, ...] | str): The name of the segment.\n                Users may pass either band names or indices names, as in the spyndex library.\n\n        Returns:\n            tuple[str, ...] | str: A tuple of band names if multiple bands are extracted,\n                or a single band name if only one band is extracted.\n\n        Raises:\n            ValueError: If the provided band name is invalid.\n                The band name must be either in `spyndex.indices` or `spyndex.bands`.\n        \"\"\"\n        if isinstance(segment_name, str):\n            segment_name = (segment_name,)\n        elif isinstance(segment_name, list):\n            segment_name = tuple(segment_name)\n\n        band_names_segment: list[str] = []\n        for band_name in segment_name:\n            if band_name in spyndex.indices:\n                band_names_segment += list(spyndex.indices[band_name].bands)\n            elif band_name in spyndex.bands:\n                band_names_segment.append(band_name)\n            else:\n                raise ValueError(\n                    f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                )\n\n        return tuple(set(band_names_segment)) if len(band_names_segment) &gt; 1 else band_names_segment[0]\n\n    @staticmethod\n    def _get_indices_from_wavelength_indices_range(\n        wavelengths: torch.Tensor, ranges: list[tuple[int, int]] | tuple[int, int]\n    ) -&gt; list[int]:\n        \"\"\"Converts wavelength indices ranges to list indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[int, int]] | tuple[int, int]): The wavelength indices ranges.\n\n        Returns:\n            list[int]: The indices of bands corresponding to the wavelength indices ranges.\n        \"\"\"\n        validated_ranges_list = validate_segment_format(ranges)\n        validated_ranges_list = adjust_and_validate_segment_ranges(wavelengths, validated_ranges_list)\n\n        return list(\n            set(\n                chain.from_iterable(\n                    [list(range(int(validated_range[0]), int(validated_range[1]))) for validated_range in ranges]  # type: ignore\n                )\n            )\n        )\n\n    @staticmethod\n    def _get_band_wavelengths_indices_from_band_names(\n        wavelengths: torch.Tensor,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n        \"\"\"Extracts band wavelengths indices from the given band names.\n\n        This function takes a list or dictionary of band names or segments and extracts the list of wavelengths indices\n        associated with each segment. It returns a tuple containing a dictionary with mapping segment labels into\n        wavelength indices and a dictionary mapping segment labels into segment ids.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]):\n                A list or dictionary with band names or segments.\n\n        Returns:\n            tuple[dict[tuple[str, ...] | str, list[int]], dict[tuple[str, ...] | str, int]]:\n                A tuple containing the dictionary with mapping segment labels into wavelength indices and the mapping\n                from segment labels into segment ids.\n        \"\"\"\n        if isinstance(band_names, list):\n            logger.debug(\"band_names is a list of segments, creating a dictionary of segments\")\n            band_names_hashed = [Lime._make_band_names_indexable(segment) for segment in band_names]\n            dict_labels_to_segment_ids = {segment: idx + 1 for idx, segment in enumerate(band_names_hashed)}\n            segments_list = band_names_hashed\n        elif isinstance(band_names, dict):\n            dict_labels_to_segment_ids = band_names.copy()\n            segments_list = tuple(band_names.keys())  # type: ignore\n        else:\n            raise ValueError(\"Incorrect band_names type. It should be a dict or a list\")\n        segments_list_after_mapping = [Lime._extract_bands_from_spyndex(segment) for segment in segments_list]\n        band_indices: dict[tuple[str, ...] | str, list[int]] = {}\n        for original_segment, segment in zip(segments_list, segments_list_after_mapping):\n            try:\n                segment_indices_ranges: list[tuple[int, int]] = []\n                for band_name in segment:\n                    segment_indices_ranges += Lime._convert_wavelengths_to_indices(\n                        wavelengths, (spyndex.bands[band_name].min_wavelength, spyndex.bands[band_name].max_wavelength)\n                    )\n\n                segment_list = Lime._get_indices_from_wavelength_indices_range(wavelengths, segment_indices_ranges)\n                band_indices[original_segment] = segment_list\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {original_segment} and bands {segment}\") from e\n        return band_indices, dict_labels_to_segment_ids\n\n    @staticmethod\n    def _convert_wavelengths_to_indices(\n        wavelengths: torch.Tensor, ranges: list[tuple[float, float]] | tuple[float, float]\n    ) -&gt; list[tuple[int, int]]:\n        \"\"\"Converts wavelength ranges to index ranges.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[tuple[float, float]] | tuple[float, float]): The wavelength ranges.\n\n        Returns:\n            list[tuple[int, int]]: The index ranges corresponding to the wavelength ranges.\n        \"\"\"\n        indices = []\n        if isinstance(ranges, tuple):\n            ranges = [ranges]\n\n        for start, end in ranges:\n            start_idx = torch.searchsorted(wavelengths, start, side=\"left\")\n            end_idx = torch.searchsorted(wavelengths, end, side=\"right\")\n            indices.append((start_idx.item(), end_idx.item()))\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_band_wavelengths(\n        wavelengths: torch.Tensor,\n        band_wavelengths: dict[str | tuple[str, ...], ListOfWavelengths],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Converts the ranges or list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_wavelengths (dict): A dictionary mapping segment labels to wavelength list or ranges.\n\n        Returns:\n            dict: A dictionary mapping segment labels to index ranges.\n\n        Raises:\n            ValueError: If band_wavelengths is not a dictionary.\n        \"\"\"\n        if not isinstance(band_wavelengths, dict):\n            raise ValueError(\"band_wavelengths should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, segment in band_wavelengths.items():\n            try:\n                dtype = torch_dtype_to_python_dtype(wavelengths.dtype)\n                if isinstance(segment, (float, int)):\n                    segment = [dtype(segment)]  # type: ignore\n                if isinstance(segment, list) and all(isinstance(x, (float, int)) for x in segment):\n                    segment_dtype = change_dtype_of_list(segment, dtype)\n                    indices = Lime._convert_wavelengths_list_to_indices(wavelengths, segment_dtype)  # type: ignore\n                else:\n                    if isinstance(segment, list):\n                        segment_dtype = [\n                            tuple(change_dtype_of_list(list(ranges), dtype))  # type: ignore\n                            for ranges in segment\n                        ]\n                    else:\n                        segment_dtype = tuple(change_dtype_of_list(segment, dtype))\n\n                    valid_segment_range = validate_segment_format(segment_dtype, dtype)\n                    range_indices = Lime._convert_wavelengths_to_indices(wavelengths, valid_segment_range)  # type: ignore\n                    valid_indices_format = validate_segment_format(range_indices)\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}: {e}\") from e\n\n            band_indices[segment_label] = indices\n\n        return band_indices\n\n    @staticmethod\n    def _convert_wavelengths_list_to_indices(wavelengths: torch.Tensor, ranges: list[float]) -&gt; list[int]:\n        \"\"\"Converts a list of wavelengths into indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            ranges (list[float]): The list of wavelengths.\n\n        Returns:\n            list[int]: The indices corresponding to the wavelengths.\n        \"\"\"\n        indices = []\n        for wavelength in ranges:\n            index = (wavelengths == wavelength).nonzero(as_tuple=False)\n            number_of_elements = torch.numel(index)\n            if number_of_elements == 1:\n                indices.append(index.item())\n            elif number_of_elements == 0:\n                raise ValueError(f\"Couldn't find wavelength of value {wavelength} in list of wavelength\")\n            else:\n                raise ValueError(f\"Wavelength of value {wavelength} was present more than once in list of wavelength\")\n        return indices\n\n    @staticmethod\n    def _get_band_indices_from_input_band_indices(\n        wavelengths: torch.Tensor,\n        input_band_indices: dict[str | tuple[str, ...], ListOfWavelengthsIndices],\n    ) -&gt; dict[str | tuple[str, ...], list[int]]:\n        \"\"\"Get band indices from band list or ranges indices.\n\n        Args:\n            wavelengths (torch.Tensor): The tensor containing the wavelengths.\n            band_indices (dict[str | tuple[str, ...], ListOfWavelengthsIndices]):\n                A dictionary mapping segment labels to a list of wavelength indices.\n\n        Returns:\n            dict[str | tuple[str, ...], list[int]]: A dictionary mapping segment labels to a list of band indices.\n\n        Raises:\n            ValueError: If `band_indices` is not a dictionary.\n        \"\"\"\n        if not isinstance(input_band_indices, dict):\n            raise ValueError(\"band_indices should be a dictionary\")\n\n        band_indices: dict[str | tuple[str, ...], list[int]] = {}\n        for segment_label, indices in input_band_indices.items():\n            try:\n                if isinstance(indices, int):\n                    indices = [indices]  # type: ignore\n                if isinstance(indices, list) and all(isinstance(x, int) for x in indices):\n                    indices: list[int] = indices  # type: ignore\n                else:\n                    valid_indices_format = validate_segment_format(indices)  # type: ignore\n                    valid_range_indices = adjust_and_validate_segment_ranges(wavelengths, valid_indices_format)\n                    indices = Lime._get_indices_from_wavelength_indices_range(wavelengths, valid_range_indices)  # type: ignore\n\n                band_indices[segment_label] = indices  # type: ignore\n            except Exception as e:\n                raise ValueError(f\"Problem with segment {segment_label}\") from e\n\n        return band_indices\n\n    @staticmethod\n    def _check_overlapping_segments(hsi: HSI, dict_labels_to_indices: dict[str | tuple[str, ...], list[int]]) -&gt; None:\n        \"\"\"Check for overlapping segments in the given hsi.\n\n        Args:\n            hsi (HSI): The hsi object containing the wavelengths.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping segment labels to indices.\n\n        Returns:\n            None\n        \"\"\"\n        overlapping_segments: dict[int, str | tuple[str, ...]] = {}\n        for segment_label, indices in dict_labels_to_indices.items():\n            for idx in indices:\n                if hsi.wavelengths[idx].item() in overlapping_segments.keys():\n                    logger.warning(\n                        (\n                            f\"Segments {overlapping_segments[hsi.wavelengths[idx].item()]} \"\n                            f\"and {segment_label} are overlapping on wavelength {hsi.wavelengths[idx].item()}\"\n                        )\n                    )\n                overlapping_segments[hsi.wavelengths[idx].item()] = segment_label\n\n    @staticmethod\n    def _validate_and_create_dict_labels_to_segment_ids(\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None,\n        segment_labels: list[str | tuple[str, ...]],\n    ) -&gt; dict[str | tuple[str, ...], int]:\n        \"\"\"Validates and creates a dictionary mapping segment labels to segment IDs.\n\n        Args:\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None):\n                The existing mapping from segment labels to segment IDs, or None if it doesn't exist.\n            segment_labels (list[str | tuple[str, ...]]): The list of segment labels.\n\n        Returns:\n            dict[str | tuple[str, ...], int]: A tuple containing the validated dictionary mapping segment\n            labels to segment IDs and a boolean flag indicating whether the segment labels are hashed.\n\n        Raises:\n            ValueError: If the length of `dict_labels_to_segment_ids` doesn't match the length of `segment_labels`.\n            ValueError: If a segment label is not present in `dict_labels_to_segment_ids`.\n            ValueError: If there are non-unique segment IDs in `dict_labels_to_segment_ids`.\n        \"\"\"\n        if dict_labels_to_segment_ids is None:\n            logger.debug(\"Creating mapping from segment labels into ids\")\n            return {segment: idx + 1 for idx, segment in enumerate(segment_labels)}\n\n        logger.debug(\"Using existing mapping from segment labels into segment ids\")\n\n        if len(dict_labels_to_segment_ids) != len(segment_labels):\n            raise ValueError(\n                (\n                    f\"Incorrect dict_labels_to_segment_ids - length mismatch. Expected: \"\n                    f\"{len(segment_labels)}, Actual: {len(dict_labels_to_segment_ids)}\"\n                )\n            )\n\n        unique_segment_ids = set(dict_labels_to_segment_ids.values())\n        if len(unique_segment_ids) != len(segment_labels):\n            raise ValueError(\"Non unique segment ids in the dict_labels_to_segment_ids\")\n\n        logger.debug(\"Passed mapping is correct\")\n        return dict_labels_to_segment_ids\n\n    @staticmethod\n    def _create_single_dim_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int],\n        device: torch.device,\n    ) -&gt; torch.Tensor:\n        \"\"\"Create a one-dimensional band mask based on the given image, labels, and segment IDs.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]):\n                A dictionary mapping labels or label tuples to lists of indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int]):\n                A dictionary mapping labels or label tuples to segment IDs.\n            device (torch.device): The device to use for the tensor.\n\n        Returns:\n            torch.Tensor: The one-dimensional band mask tensor.\n\n        Raises:\n            ValueError: If the indices for a segment are out of bounds for the one-dimensional band mask.\n        \"\"\"\n        band_mask_single_dim = torch.zeros(len(hsi.wavelengths), dtype=torch.int64, device=device)\n\n        segment_labels = list(dict_labels_to_segment_ids.keys())\n\n        for segment_label in segment_labels[::-1]:\n            segment_indices = dict_labels_to_indices[segment_label]\n            segment_id = dict_labels_to_segment_ids[segment_label]\n            are_indices_valid = all(0 &lt;= idx &lt; band_mask_single_dim.shape[0] for idx in segment_indices)\n            if not are_indices_valid:\n                raise ValueError(\n                    (\n                        f\"Indices for segment {segment_label} are out of bounds for the one-dimensional band mask\"\n                        f\"of shape {band_mask_single_dim.shape}\"\n                    )\n                )\n            band_mask_single_dim[segment_indices] = segment_id\n\n        return band_mask_single_dim\n\n    @staticmethod\n    def _expand_band_mask(hsi: HSI, band_mask_single_dim: torch.Tensor, repeat_dimensions: bool) -&gt; torch.Tensor:\n        \"\"\"Expands the band mask to match the dimensions of the input hsi.\n\n        Args:\n            hsi (HSI): The input hsi.\n            band_mask_single_dim (torch.Tensor): The band mask tensor with a single dimension.\n            repeat_dimensions (bool): Whether to repeat the dimensions of the band mask to match the hsi.\n\n        Returns:\n            torch.Tensor: The expanded band mask tensor.\n        \"\"\"\n        if hsi.spectral_axis == 0:\n            band_mask = band_mask_single_dim.unsqueeze(-1).unsqueeze(-1)\n        elif hsi.spectral_axis == 1:\n            band_mask = band_mask_single_dim.unsqueeze(0).unsqueeze(-1)\n        elif hsi.spectral_axis == 2:\n            band_mask = band_mask_single_dim.unsqueeze(0).unsqueeze(0)\n        if repeat_dimensions:\n            size_image = hsi.image.size()\n            size_mask = band_mask.size()\n\n            repeat_dims = [s2 // s1 for s1, s2 in zip(size_mask, size_image)]\n            band_mask = band_mask.repeat(repeat_dims)\n\n        return band_mask\n\n    @staticmethod\n    def _create_tensor_band_mask(\n        hsi: HSI,\n        dict_labels_to_indices: dict[str | tuple[str, ...], list[int]],\n        dict_labels_to_segment_ids: dict[str | tuple[str, ...], int] | None = None,\n        device: str | torch.device | None = None,\n        repeat_dimensions: bool = False,\n        return_dict_labels_to_segment_ids: bool = True,\n    ) -&gt; torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"Create a tensor band mask from dictionaries. The band mask is created based on the given hsi, labels, and\n        segment IDs. The band mask is a tensor with the same shape as the input hsi and contains segment IDs, where each\n        segment is represented by a unique ID. The band mask will be used to attribute the hsi using the LIME method.\n\n        Args:\n            hsi (HSI): The input hsi.\n            dict_labels_to_indices (dict[str | tuple[str, ...], list[int]]): A dictionary mapping labels to indices.\n            dict_labels_to_segment_ids (dict[str | tuple[str, ...], int] | None, optional):\n                A dictionary mapping labels to segment IDs. Defaults to None.\n            device (str | torch.device | None, optional): The device to use. Defaults to None.\n            repeat_dimensions (bool, optional): Whether to repeat dimensions. Defaults to False.\n            return_dict_labels_to_segment_ids (bool, optional):\n                Whether to return the dictionary mapping labels to segment IDs. Defaults to True.\n\n        Returns:\n            torch.Tensor | tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n                The tensor band mask or a tuple containing the tensor band mask\n                and the dictionary mapping labels to segment IDs.\n        \"\"\"\n        if device is None:\n            device = hsi.device\n        segment_labels = list(dict_labels_to_indices.keys())\n\n        logger.debug(f\"Creating a band mask on the device {device} using {len(segment_labels)} segments\")\n\n        # Check for overlapping segments\n        Lime._check_overlapping_segments(hsi, dict_labels_to_indices)\n\n        # Create or validate dict_labels_to_segment_ids\n        dict_labels_to_segment_ids = Lime._validate_and_create_dict_labels_to_segment_ids(\n            dict_labels_to_segment_ids, segment_labels\n        )\n\n        # Create single-dimensional band mask\n        band_mask_single_dim = Lime._create_single_dim_band_mask(\n            hsi, dict_labels_to_indices, dict_labels_to_segment_ids, device\n        )\n\n        # Expand band mask to match image dimensions\n        band_mask = Lime._expand_band_mask(hsi, band_mask_single_dim, repeat_dimensions)\n\n        if return_dict_labels_to_segment_ids:\n            return band_mask, dict_labels_to_segment_ids\n        return band_mask\n\n    def attribute(\n        self, attribution_type: Literal[\"spatial\", \"spectral\"], image: HSI, target: int | None = None, **kwargs\n    ) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n        \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n        `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n        detailed description of the methods, please refer to the respective method documentation.\n\n        Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n        Args:\n            attribution_type (Literal[\"spatial\", \"spectral\"]): An attribution type to be executed.\n            image (HSI): an image on which the explanation is performed.\n            target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n        \"\"\"\n        if attribution_type == \"spatial\":\n            return self.get_spatial_attributes(image, target=target, **kwargs)\n        elif attribution_type == \"spectral\":\n            return self.get_spectral_attributes(image, target=target, **kwargs)\n        raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n\n    def get_spatial_attributes(\n        self,\n        hsi: HSI,\n        segmentation_mask: np.ndarray | torch.Tensor | None = None,\n        target: int | None = None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        **segmentation_method_params: Any,\n    ) -&gt; HSISpatialAttributes:\n        \"\"\"\n        Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n        LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n        `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n        `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n        method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n        the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n                A segmentation mask according to which the attribution should be performed.\n                The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n                If None, a new segmentation mask is created using the `segmentation_method`.\n                    Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n            postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n               A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n               lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n               inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                   Defaults to None.\n            segmentation_method (Literal[\"slic\", \"patch\"], optional):\n                Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n            **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n        Returns:\n            HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n                the segmentation mask, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n            &gt;&gt;&gt; spatial_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spatial_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n            torch.Size([1, 240, 240])\n            &gt;&gt;&gt; spatial_attribution.score\n            1.0\n        \"\"\"\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n        segmentation_mask = ensure_torch_tensor(\n            segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n        )\n\n        segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n        hsi = hsi.to(self.device)\n        segmentation_mask = segmentation_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=segmentation_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spatial_attribution = HSISpatialAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=segmentation_mask.expand_as(hsi.image),\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        hsi: HSI,\n        band_mask: np.ndarray | torch.Tensor | None = None,\n        target=None,\n        n_samples: int = 10,\n        perturbations_per_eval: int = 4,\n        verbose: bool = False,\n        postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n        band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n    ) -&gt; HSISpectralAttributes:\n        \"\"\"\n        Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n        method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n        Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n        Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n        The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n        the band names, and the score of the interpretable model used for the explanation.\n\n        Args:\n            hsi (HSI): An HSI object for which the attribution is performed.\n            band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n                The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n                The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n                If equals to None, the band mask is created within the function. Defaults to None.\n            target (int, optional): If the model creates more than one output, it analyzes the given target.\n                Defaults to None.\n            n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n            perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n                Defaults to 4.\n            verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n            postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n                A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n                lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n                inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n                Defaults to None.\n            band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n        Returns:\n            HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n                the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n        Raises:\n            ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n            AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n            AssertionError: If the hsi is not an instance of the HSI class.\n\n        Examples:\n            &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n            &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n            &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n            &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; lime = meteors.attr.Lime(\n                    explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n                )\n            &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n            &gt;&gt;&gt; spectral_attribution.hsi\n            HSI(shape=(4, 240, 240), dtype=torch.float32)\n            &gt;&gt;&gt; spectral_attribution.attributes.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_mask.shape\n            torch.Size([4, 240, 240])\n            &gt;&gt;&gt; spectral_attribution.band_names\n            [\"R\", \"G\", \"B\"]\n            &gt;&gt;&gt; spectral_attribution.score\n            1.0\n        \"\"\"\n\n        if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n            raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n        if self.explainable_model.problem_type == \"segmentation\":\n            assert postprocessing_segmentation_output, (\n                \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n                \"the `postprocessing_segmentation_output`. For a reference \"\n                \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n            )\n        elif postprocessing_segmentation_output is not None:\n            logger.warning(\n                \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n            )\n            postprocessing_segmentation_output = None\n\n        assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n        if band_mask is None:\n            band_mask, band_names = self.get_band_mask(hsi, band_names)\n        band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n        if band_mask.ndim != hsi.image.ndim:\n            band_mask = Lime._expand_band_mask(hsi, band_mask, repeat_dimensions=False)\n        band_mask = band_mask.int()\n\n        if band_names is None:\n            unique_segments = torch.unique(band_mask)\n            band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n        else:\n            # checking consistency of names\n            # unique_segments = torch.unique(band_mask)\n            # if isinstance(band_names, dict):\n            #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n            logger.debug(\"Band names are provided, using them. In future it there should be an option to validate them\")\n\n        band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n        hsi = hsi.to(self.device)\n        band_mask = band_mask.to(self.device)\n\n        lime_attributes, score = self._attribution_method.attribute(\n            inputs=hsi.get_image().unsqueeze(0),\n            target=target,\n            feature_mask=band_mask.unsqueeze(0),\n            n_samples=n_samples,\n            perturbations_per_eval=perturbations_per_eval,\n            model_postprocessing=postprocessing_segmentation_output,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        spectral_attribution = HSISpectralAttributes(\n            hsi=hsi,\n            attributes=lime_attributes.squeeze(0),\n            mask=band_mask.expand_as(hsi.image),\n            band_names=band_names,\n            score=score,\n            attribution_method=\"Lime\",\n        )\n\n        return spectral_attribution\n\n    @staticmethod\n    def _get_slick_segmentation_mask(\n        hsi: HSI, num_interpret_features: int = 10, *args: Any, **kwargs: Any\n    ) -&gt; torch.Tensor:\n        \"\"\"Creates a segmentation mask using the SLIC method.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            num_interpret_features (int, optional): Number of segments. Defaults to 10.\n            *args: Additional positional arguments to be passed to the SLIC method.\n            **kwargs: Additional keyword arguments to be passed to the SLIC method.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        segmentation_mask = slic(\n            hsi.get_image().cpu().detach().numpy(),\n            n_segments=num_interpret_features,\n            mask=hsi.spatial_binary_mask.cpu().detach().numpy(),\n            channel_axis=hsi.spectral_axis,\n            *args,\n            **kwargs,\n        )\n\n        if segmentation_mask.min() == 1:\n            segmentation_mask -= 1\n\n        segmentation_mask = torch.from_numpy(segmentation_mask)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        return segmentation_mask\n\n    @staticmethod\n    def _get_patch_segmentation_mask(hsi: HSI, patch_size: int | float = 10, *args: Any, **kwargs: Any) -&gt; torch.Tensor:\n        \"\"\"\n        Creates a segmentation mask using the patch method - creates small squares of the same size\n            and assigns a unique value to each square.\n\n        Args:\n            hsi (HSI): An HSI object for which the segmentation mask is created.\n            patch_size (int, optional): Size of the patch, the hsi size should be divisible by this value.\n                Defaults to 10.\n\n        Returns:\n            torch.Tensor: An output segmentation mask.\n        \"\"\"\n        logger.warning(\"Patch segmentation only works for band_index = 0 now\")\n\n        if patch_size &lt; 1 or not isinstance(patch_size, (int, float)):\n            raise ValueError(\"Invalid patch_size. patch_size must be a positive integer\")\n\n        if hsi.image.shape[1] % patch_size != 0 or hsi.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the hsi\")\n\n        height, width = hsi.image.shape[1], hsi.image.shape[2]\n\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=hsi.device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * hsi.spatial_binary_mask\n        # segmentation_mask = torch.repeat_interleave(\n        # torch.unsqueeze(segmentation_mask, dim=hsi.spectral_axis),\n        # repeats=hsi.image.shape[hsi.spectral_axis], dim=hsi.spectral_axis)\n        segmentation_mask = segmentation_mask.unsqueeze(dim=hsi.spectral_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.attribute","title":"<code>attribute(attribution_type, image, target=None, **kwargs)</code>","text":"<p>A wrapper function to attribute the image using the LIME method. It executes either the <code>get_spatial_attributes</code> or <code>get_spectral_attributes</code> method based on the provided <code>attribution_type</code>. For more detailed description of the methods, please refer to the respective method documentation.</p> <p>Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.</p> <p>Parameters:</p> Name Type Description Default <code>attribution_type</code> <code>Literal['spatial', 'spectral']</code> <p>An attribution type to be executed.</p> required <code>image</code> <code>HSI</code> <p>an image on which the explanation is performed.</p> required <code>target</code> <code>int | None</code> <p>Target output index for the explanation. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>HSISpectralAttributes | HSISpatialAttributes</code> <p>HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the <code>attribution_type</code> is <code>spatial</code>, the object is of type <code>HSISpatialAttributes</code>, otherwise it is of type <code>HSISpectralAttributes</code>.</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def attribute(\n    self, attribution_type: Literal[\"spatial\", \"spectral\"], image: HSI, target: int | None = None, **kwargs\n) -&gt; HSISpectralAttributes | HSISpatialAttributes:\n    \"\"\"A wrapper function to attribute the image using the LIME method. It executes either the\n    `get_spatial_attributes` or `get_spectral_attributes` method based on the provided `attribution_type`. For more\n    detailed description of the methods, please refer to the respective method documentation.\n\n    Additional, nondefault parameters, should be passed as keyword arguments to avoid misalignment of the arguments.\n\n    Args:\n        attribution_type (Literal[\"spatial\", \"spectral\"]): An attribution type to be executed.\n        image (HSI): an image on which the explanation is performed.\n        target (int | None, optional): Target output index for the explanation. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes | HSISpatialAttributes: An object containing the image, the attributions and additional information. In case the `attribution_type` is `spatial`, the object is of type `HSISpatialAttributes`, otherwise it is of type `HSISpectralAttributes`.\n    \"\"\"\n    if attribution_type == \"spatial\":\n        return self.get_spatial_attributes(image, target=target, **kwargs)\n    elif attribution_type == \"spectral\":\n        return self.get_spectral_attributes(image, target=target, **kwargs)\n    raise ValueError(f\"Unsupported attribution type: {attribution_type}. Use 'spatial' or 'spectral'\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_band_mask","title":"<code>get_band_mask(hsi, band_names=None, band_indices=None, band_wavelengths=None, device=None, repeat_dimensions=False)</code>  <code>staticmethod</code>","text":"<p>Generates a band mask based on the provided hsi and band information.</p> <p>Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask. If you provide more than one, the band mask will be created using only one using the following priority: band_names &gt; band_wavelengths &gt; band_indices.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image.</p> required <code>band_names</code> <code>None | list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The names of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_indices</code> <code>None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]]</code> <p>The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>band_wavelengths</code> <code>None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float]</code> <p>The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.</p> <code>None</code> <code>device</code> <code>str | device | None</code> <p>The device to use for computation. Defaults to None.</p> <code>None</code> <code>repeat_dimensions</code> <code>bool</code> <p>Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary</p> <code>dict[tuple[str, ...] | str, int]</code> <p>mapping band names to segment IDs.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If no band names, indices, or wavelengths are provided.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n&gt;&gt;&gt; band_names = [\"R\", \"G\"]\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"R\": 1, \"G\": 2}\n&gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n&gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n&gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n&gt;&gt;&gt; dict_labels_to_segment_ids\n{\"RGB\": 1}\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    hsi: HSI,\n    band_names: None | list[str | list[str]] | dict[tuple[str, ...] | str, int] = None,\n    band_indices: None | dict[str | tuple[str, ...], ListOfWavelengthsIndices] = None,\n    band_wavelengths: None | dict[str | tuple[str, ...], ListOfWavelengths] = None,\n    device: str | torch.device | None = None,\n    repeat_dimensions: bool = False,\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"Generates a band mask based on the provided hsi and band information.\n\n    Remember you need to provide either band_names, band_indices, or band_wavelengths to create the band mask.\n    If you provide more than one, the band mask will be created using only one using the following priority:\n    band_names &gt; band_wavelengths &gt; band_indices.\n\n    Args:\n        hsi (HSI): The input hyperspectral image.\n        band_names (None | list[str | list[str]] | dict[tuple[str, ...] | str, int], optional):\n            The names of the spectral bands to include in the mask. Defaults to None.\n        band_indices (None | dict[str | tuple[str, ...], list[tuple[int, int]] | tuple[int, int] | list[int]], optional):\n            The indices or ranges of indices of the spectral bands to include in the mask. Defaults to None.\n        band_wavelengths (None | dict[str | tuple[str, ...], list[tuple[float, float]] | tuple[float, float], list[float], float], optional):\n            The wavelengths or ranges of wavelengths of the spectral bands to include in the mask. Defaults to None.\n        device (str | torch.device | None, optional):\n            The device to use for computation. Defaults to None.\n        repeat_dimensions (bool, optional):\n            Whether to repeat the dimensions of the mask to match the input hsi shape. Defaults to False.\n\n    Returns:\n        tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]: A tuple containing the band mask tensor and a dictionary\n        mapping band names to segment IDs.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If no band names, indices, or wavelengths are provided.\n\n    Examples:\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((len(wavelengths), 10, 10)), wavelengths=wavelengths)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\"]\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_names=band_names)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"R\": 1, \"G\": 2}\n        &gt;&gt;&gt; band_indices = {\"RGB\": [0, 1, 2]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_indices=band_indices)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n        &gt;&gt;&gt; band_wavelengths = {\"RGB\": [(462.08, 465.27), (465.27, 468.47), (468.47, 471.68)]}\n        &gt;&gt;&gt; band_mask, dict_labels_to_segment_ids = mt_lime.Lime.get_band_mask(hsi, band_wavelengths=band_wavelengths)\n        &gt;&gt;&gt; dict_labels_to_segment_ids\n        {\"RGB\": 1}\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    assert (\n        band_names is not None or band_indices is not None or band_wavelengths is not None\n    ), \"No band names, indices, or wavelengths are provided.\"\n\n    # validate types\n    dict_labels_to_segment_ids = None\n    if band_names is not None:\n        logger.debug(\"Getting band mask from band names of spectral bands\")\n        if band_wavelengths is not None or band_indices is not None:\n            ignored_params = [\n                param\n                for param in [\"band_wavelengths\", \"band_indices\"]\n                if param in locals() and locals()[param] is not None\n            ]\n            ignored_params_str = \" and \".join(ignored_params)\n            logger.info(\n                f\"Only the band names will be used to create the band mask. The additional parameters {ignored_params_str} will be ignored.\"\n            )\n        try:\n            validate_band_names(band_names)\n            band_groups, dict_labels_to_segment_ids = Lime._get_band_wavelengths_indices_from_band_names(\n                hsi.wavelengths, band_names\n            )\n        except Exception as e:\n            raise ValueError(f\"Incorrect band names provided: {e}\") from e\n    elif band_wavelengths is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of wavelengths\")\n        if band_indices is not None:\n            logger.info(\n                \"Only the band wavelengths will be used to create the band mask. The band_indices will be ignored.\"\n            )\n        validate_band_format(band_wavelengths, variable_name=\"band_wavelengths\")\n        try:\n            band_groups = Lime._get_band_indices_from_band_wavelengths(\n                hsi.wavelengths,\n                band_wavelengths,\n            )\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges wavelengths provided, please check if provided wavelengths are correct: {e}\"\n            ) from e\n    elif band_indices is not None:\n        logger.debug(\"Getting band mask from band groups given by ranges of indices\")\n        validate_band_format(band_indices, variable_name=\"band_indices\")\n        try:\n            band_groups = Lime._get_band_indices_from_input_band_indices(hsi.wavelengths, band_indices)\n        except Exception as e:\n            raise ValueError(\n                f\"Incorrect band ranges indices provided, please check if provided indices are correct: {e}\"\n            ) from e\n\n    return Lime._create_tensor_band_mask(\n        hsi,\n        band_groups,\n        dict_labels_to_segment_ids=dict_labels_to_segment_ids,\n        device=device,\n        repeat_dimensions=repeat_dimensions,\n        return_dict_labels_to_segment_ids=True,\n    )\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_segmentation_mask","title":"<code>get_segmentation_mask(hsi, segmentation_method='slic', **segmentation_method_params)</code>  <code>staticmethod</code>","text":"<p>Generates a segmentation mask for the given hsi using the specified segmentation method.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>The input hyperspectral image for which the segmentation mask needs to be generated.</p> required <code>segmentation_method</code> <code>Literal['patch', 'slic']</code> <p>The segmentation method to be used. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters specific to the chosen segmentation method.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The segmentation mask as a tensor.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input hsi is not an instance of the HSI class.</p> <code>ValueError</code> <p>If an unsupported segmentation method is specified.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n&gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n&gt;&gt;&gt; segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; segmentation_mask[0, :2, :2]\ntorch.tensor([[1, 1],\n              [1, 1]])\n&gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\ntorch.tensor([[2, 2],\n              [2, 2]])\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>@staticmethod\ndef get_segmentation_mask(\n    hsi: HSI,\n    segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; torch.Tensor:\n    \"\"\"Generates a segmentation mask for the given hsi using the specified segmentation method.\n\n    Args:\n        hsi (HSI): The input hyperspectral image for which the segmentation mask needs to be generated.\n        segmentation_method (Literal[\"patch\", \"slic\"], optional): The segmentation method to be used.\n            Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters specific to the chosen segmentation method.\n\n    Returns:\n        torch.Tensor: The segmentation mask as a tensor.\n\n    Raises:\n        ValueError: If the input hsi is not an instance of the HSI class.\n        ValueError: If an unsupported segmentation method is specified.\n\n    Examples:\n        &gt;&gt;&gt; hsi = meteors.HSI(image=torch.ones((3, 240, 240)), wavelengths=[462.08, 465.27, 468.47])\n        &gt;&gt;&gt; segmentation_mask = mt_lime.Lime.get_segmentation_mask(hsi, segmentation_method=\"slic\")\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask = meteors.attr.Lime.get_segmentation_mask(hsi, segmentation_method=\"patch\", patch_size=2)\n        &gt;&gt;&gt; segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; segmentation_mask[0, :2, :2]\n        torch.tensor([[1, 1],\n                      [1, 1]])\n        &gt;&gt;&gt; segmentation_mask[0, 2:4, :2]\n        torch.tensor([[2, 2],\n                      [2, 2]])\n    \"\"\"\n    if not isinstance(hsi, HSI):\n        raise ValueError(\"hsi should be an instance of HSI class\")\n\n    if segmentation_method == \"slic\":\n        return Lime._get_slick_segmentation_mask(hsi, **segmentation_method_params)\n    elif segmentation_method == \"patch\":\n        return Lime._get_patch_segmentation_mask(hsi, **segmentation_method_params)\n    else:\n        raise ValueError(f\"Unsupported segmentation method: {segmentation_method}\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spatial_attributes","title":"<code>get_spatial_attributes(hsi, segmentation_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, segmentation_method='slic', **segmentation_method_params)</code>","text":"<p>Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask LIME method attributes the <code>superpixels</code> provided by the segmentation mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations) method for spatial data. It returns an <code>HSISpatialAttributes</code> object that contains the hyperspectral image,, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>segmentation_mask</code> <code>ndarray | Tensor | None</code> <p>A segmentation mask according to which the attribution should be performed. The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter <code>C</code> in the <code>image.orientation</code> parameter. If None, a new segmentation mask is created using the <code>segmentation_method</code>.     Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Whether to show the progress bar. Defaults to False.</p> <code>False</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor, Tensor], Tensor] | None</code> <p>A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.    Defaults to None.</p> <code>None</code> <code>segmentation_method</code> <code>Literal['slic', 'patch']</code> <p>Segmentation method used only if <code>segmentation_mask</code> is None. Defaults to \"slic\".</p> <code>'slic'</code> <code>**segmentation_method_params</code> <code>Any</code> <p>Additional parameters for the segmentation method.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>HSISpatialAttributes</code> <code>HSISpatialAttributes</code> <p>A <code>HSISpatialAttributes</code> object that contains the image, the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n&gt;&gt;&gt; spatial_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spatial_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\ntorch.Size([1, 240, 240])\n&gt;&gt;&gt; spatial_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spatial_attributes(\n    self,\n    hsi: HSI,\n    segmentation_mask: np.ndarray | torch.Tensor | None = None,\n    target: int | None = None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n    **segmentation_method_params: Any,\n) -&gt; HSISpatialAttributes:\n    \"\"\"\n    Get spatial attributes of an hsi image using the LIME method. Based on the provided hsi and segmentation mask\n    LIME method attributes the `superpixels` provided by the segmentation mask. Please refer to the original paper\n    `https://arxiv.org/abs/1602.04938` for more details or to Christoph Molnar's book\n    `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    This function attributes the hyperspectral image using the LIME (Local Interpretable Model-Agnostic Explanations)\n    method for spatial data. It returns an `HSISpatialAttributes` object that contains the hyperspectral image,,\n    the attributions, the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        segmentation_mask (np.ndarray | torch.Tensor | None, optional):\n            A segmentation mask according to which the attribution should be performed.\n            The segmentation mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimension on which the image and the mask shapes can differ is the spectral dimension, marked with letter `C` in the `image.orientation` parameter.\n            If None, a new segmentation mask is created using the `segmentation_method`.\n                Additional parameters for the segmentation method may be passed as kwargs. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Whether to show the progress bar. Defaults to False.\n        postprocessing_segmentation_output (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n           A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n           lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n           inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n               Defaults to None.\n        segmentation_method (Literal[\"slic\", \"patch\"], optional):\n            Segmentation method used only if `segmentation_mask` is None. Defaults to \"slic\".\n        **segmentation_method_params (Any): Additional parameters for the segmentation method.\n\n    Returns:\n        HSISpatialAttributes: A `HSISpatialAttributes` object that contains the image, the attributions,\n            the segmentation mask, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; segmentation_mask = torch.randint(1, 4, (1, 240, 240))\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spatial_attribution = lime.get_spatial_attributes(hsi, segmentation_mask=segmentation_mask, target=0)\n        &gt;&gt;&gt; spatial_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spatial_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.segmentation_mask.shape\n        torch.Size([1, 240, 240])\n        &gt;&gt;&gt; spatial_attribution.score\n        1.0\n    \"\"\"\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    if segmentation_mask is None:\n        segmentation_mask = self.get_segmentation_mask(hsi, segmentation_method, **segmentation_method_params)\n    segmentation_mask = ensure_torch_tensor(\n        segmentation_mask, \"Segmentation mask should be None, numpy array, or torch tensor\"\n    )\n\n    segmentation_mask = validate_mask_shape(\"segmentation\", hsi, segmentation_mask)\n\n    hsi = hsi.to(self.device)\n    segmentation_mask = segmentation_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=segmentation_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spatial_attribution = HSISpatialAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=segmentation_mask.expand_as(hsi.image),\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spatial_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.Lime.get_spectral_attributes","title":"<code>get_spectral_attributes(hsi, band_mask=None, target=None, n_samples=10, perturbations_per_eval=4, verbose=False, postprocessing_segmentation_output=None, band_names=None)</code>","text":"<p>Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME method attributes the hsi based on <code>superbands</code> (clustered bands) provided by the band mask. Please refer to the original paper <code>https://arxiv.org/abs/1602.04938</code> for more details or to Christoph Molnar's book <code>https://christophm.github.io/interpretable-ml-book/lime.html</code>.</p> <p>The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Parameters:</p> Name Type Description Default <code>hsi</code> <code>HSI</code> <p>An HSI object for which the attribution is performed.</p> required <code>band_mask</code> <code>ndarray | Tensor | None</code> <p>Band mask that is used for the spectral attribution. The band mask should have a 3D shape, which can be broadcastable to the shape of the input image. The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters <code>H</code> and <code>W</code> in the <code>image.orientation</code> parameter. If equals to None, the band mask is created within the function. Defaults to None.</p> <code>None</code> <code>target</code> <code>int</code> <p>If the model creates more than one output, it analyzes the given target. Defaults to None.</p> <code>None</code> <code>n_samples</code> <code>int</code> <p>The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.</p> <code>10</code> <code>perturbations_per_eval</code> <code>int</code> <p>The number of perturbations to evaluate at once (Simply the inner batch size). Defaults to 4.</p> <code>4</code> <code>verbose</code> <code>bool</code> <p>Specifies whether to show progress during the attribution process. Defaults to False.</p> <code>False</code> <code>postprocessing_segmentation_output</code> <code>Callable[[Tensor, Tensor], Tensor] | None</code> <p>(Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None): A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask. Defaults to None.</p> <code>None</code> <code>band_names</code> <code>list[str] | dict[str | tuple[str, ...], int] | None</code> <p>Band names. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>HSISpectralAttributes</code> <code>HSISpectralAttributes</code> <p>A HSISpectralAttributes object containing the image, the attributions, the band mask, the band names, and the score of the interpretable model used for the explanation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the Lime object is not initialized or is not an instance of LimeBase.</p> <code>AssertionError</code> <p>If explainable model type is <code>segmentation</code> and <code>postprocessing_segmentation_output</code> is not provided.</p> <code>AssertionError</code> <p>If the hsi is not an instance of the HSI class.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n&gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n&gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n&gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; lime = meteors.attr.Lime(\n        explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n    )\n&gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n&gt;&gt;&gt; spectral_attribution.hsi\nHSI(shape=(4, 240, 240), dtype=torch.float32)\n&gt;&gt;&gt; spectral_attribution.attributes.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_mask.shape\ntorch.Size([4, 240, 240])\n&gt;&gt;&gt; spectral_attribution.band_names\n[\"R\", \"G\", \"B\"]\n&gt;&gt;&gt; spectral_attribution.score\n1.0\n</code></pre> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def get_spectral_attributes(\n    self,\n    hsi: HSI,\n    band_mask: np.ndarray | torch.Tensor | None = None,\n    target=None,\n    n_samples: int = 10,\n    perturbations_per_eval: int = 4,\n    verbose: bool = False,\n    postprocessing_segmentation_output: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n    band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int] | None = None,\n) -&gt; HSISpectralAttributes:\n    \"\"\"\n    Attributes the hsi image using LIME method for spectral data. Based on the provided hsi and band mask, the LIME\n    method attributes the hsi based on `superbands` (clustered bands) provided by the band mask.\n    Please refer to the original paper `https://arxiv.org/abs/1602.04938` for more details or to\n    Christoph Molnar's book `https://christophm.github.io/interpretable-ml-book/lime.html`.\n\n    The function returns a HSISpectralAttributes object that contains the image, the attributions, the band mask,\n    the band names, and the score of the interpretable model used for the explanation.\n\n    Args:\n        hsi (HSI): An HSI object for which the attribution is performed.\n        band_mask (np.ndarray | torch.Tensor | None, optional): Band mask that is used for the spectral attribution.\n            The band mask should have a 3D shape, which can be broadcastable to the shape of the input image.\n            The only dimensions on which the image and the mask shapes can differ is the height and width dimensions, marked with letters `H` and `W` in the `image.orientation` parameter.\n            If equals to None, the band mask is created within the function. Defaults to None.\n        target (int, optional): If the model creates more than one output, it analyzes the given target.\n            Defaults to None.\n        n_samples (int, optional): The number of samples to generate/analyze in LIME. The more the better but slower. Defaults to 10.\n        perturbations_per_eval (int, optional): The number of perturbations to evaluate at once (Simply the inner batch size).\n            Defaults to 4.\n        verbose (bool, optional): Specifies whether to show progress during the attribution process. Defaults to False.\n        postprocessing_segmentation_output: (Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None):\n            A segmentation postprocessing function for segmentation problem type. This is required for segmentation problem type as\n            lime surrogate model needs to be optimized on the 1d output, and the model should be able to modify the model output with\n            inner lime active region mask as input and return the 1d output (for example number of pixel for each class) and not class mask.\n            Defaults to None.\n        band_names (list[str] | dict[str | tuple[str, ...], int] | None, optional): Band names. Defaults to None.\n\n    Returns:\n        HSISpectralAttributes: A HSISpectralAttributes object containing the image, the attributions,\n            the band mask, the band names, and the score of the interpretable model used for the explanation.\n\n    Raises:\n        ValueError: If the Lime object is not initialized or is not an instance of LimeBase.\n        AssertionError: If explainable model type is `segmentation` and `postprocessing_segmentation_output` is not provided.\n        AssertionError: If the hsi is not an instance of the HSI class.\n\n    Examples:\n        &gt;&gt;&gt; simple_model = lambda x: torch.rand((x.shape[0], 2))\n        &gt;&gt;&gt; hsi = mt.HSI(image=torch.ones((4, 240, 240)), wavelengths=[462.08, 465.27, 468.47, 471.68])\n        &gt;&gt;&gt; band_mask = torch.randint(1, 4, (4, 1, 1)).repeat(1, 240, 240)\n        &gt;&gt;&gt; band_names = [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; lime = meteors.attr.Lime(\n                explainable_model=ExplainableModel(simple_model, \"regression\"), interpretable_model=SkLearnLasso(alpha=0.1)\n            )\n        &gt;&gt;&gt; spectral_attribution = lime.get_spectral_attributes(hsi, band_mask=band_mask, band_names=band_names, target=0)\n        &gt;&gt;&gt; spectral_attribution.hsi\n        HSI(shape=(4, 240, 240), dtype=torch.float32)\n        &gt;&gt;&gt; spectral_attribution.attributes.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_mask.shape\n        torch.Size([4, 240, 240])\n        &gt;&gt;&gt; spectral_attribution.band_names\n        [\"R\", \"G\", \"B\"]\n        &gt;&gt;&gt; spectral_attribution.score\n        1.0\n    \"\"\"\n\n    if self._attribution_method is None or not isinstance(self._attribution_method, LimeBase):\n        raise ValueError(\"Lime object not initialized\")  # pragma: no cover\n\n    if self.explainable_model.problem_type == \"segmentation\":\n        assert postprocessing_segmentation_output, (\n            \"postprocessing_segmentation_output is required for segmentation problem type, please provide \"\n            \"the `postprocessing_segmentation_output`. For a reference \"\n            \"we provided an example function to use `agg_segmentation_postprocessing` from `meteors.utils.utils` module\"\n        )\n    elif postprocessing_segmentation_output is not None:\n        logger.warning(\n            \"postprocessing_segmentation_output is provided but the problem is not segmentation, will be ignored\"\n        )\n        postprocessing_segmentation_output = None\n\n    assert isinstance(hsi, HSI), \"hsi should be an instance of HSI class\"\n\n    if band_mask is None:\n        band_mask, band_names = self.get_band_mask(hsi, band_names)\n    band_mask = ensure_torch_tensor(band_mask, \"Band mask should be None, numpy array, or torch tensor\")\n    if band_mask.ndim != hsi.image.ndim:\n        band_mask = Lime._expand_band_mask(hsi, band_mask, repeat_dimensions=False)\n    band_mask = band_mask.int()\n\n    if band_names is None:\n        unique_segments = torch.unique(band_mask)\n        band_names = {str(segment): idx for idx, segment in enumerate(unique_segments)}\n    else:\n        # checking consistency of names\n        # unique_segments = torch.unique(band_mask)\n        # if isinstance(band_names, dict):\n        #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n        logger.debug(\"Band names are provided, using them. In future it there should be an option to validate them\")\n\n    band_mask = validate_mask_shape(\"band\", hsi, band_mask)\n\n    hsi = hsi.to(self.device)\n    band_mask = band_mask.to(self.device)\n\n    lime_attributes, score = self._attribution_method.attribute(\n        inputs=hsi.get_image().unsqueeze(0),\n        target=target,\n        feature_mask=band_mask.unsqueeze(0),\n        n_samples=n_samples,\n        perturbations_per_eval=perturbations_per_eval,\n        model_postprocessing=postprocessing_segmentation_output,\n        show_progress=verbose,\n        return_input_shape=True,\n    )\n\n    spectral_attribution = HSISpectralAttributes(\n        hsi=hsi,\n        attributes=lime_attributes.squeeze(0),\n        mask=band_mask.expand_as(hsi.image),\n        band_names=band_names,\n        score=score,\n        attribution_method=\"Lime\",\n    )\n\n    return spectral_attribution\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.adjust_and_validate_segment_ranges","title":"<code>adjust_and_validate_segment_ranges(wavelengths, segment_ranges)</code>","text":"<p>Adjusts and validates segment ranges against the wavelength dimension.</p> <p>This function ensures that each segment range is within the bounds of the wavelength dimension. It attempts to adjust out-of-bounds ranges when possible and raises an error for ranges that cannot be adjusted.</p> <p>Parameters:</p> Name Type Description Default <code>wavelengths</code> <code>Tensor</code> <p>The wavelengths tensor. Its length determines the valid range for segments.</p> required <code>segment_ranges</code> <code>list[tuple[int, int]]</code> <p>A list of segment ranges to be validated and potentially adjusted. Each tuple represents (start, end) indices.</p> required <p>Returns:</p> Type Description <code>list[tuple[int, int]]</code> <p>list[tuple[int, int]]: The list of validated and potentially adjusted segment ranges.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a segment range is entirely out of bounds and cannot be adjusted.</p> <p>Warns:</p> Type Description <code>UserWarning</code> <p>If a segment range is partially out of bounds and is adjusted.</p> Notes <ul> <li>Segment ranges are inclusive of both start and end indices.</li> <li>Ranges extending below 0 are adjusted to start at 0 if possible.</li> <li>Ranges extending beyond the wavelength dimension are truncated if possible.</li> <li>Adjustments are only made if at least part of the range is within bounds.</li> </ul> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def adjust_and_validate_segment_ranges(\n    wavelengths: torch.Tensor, segment_ranges: list[tuple[int, int]]\n) -&gt; list[tuple[int, int]]:\n    \"\"\"Adjusts and validates segment ranges against the wavelength dimension.\n\n    This function ensures that each segment range is within the bounds of the wavelength\n    dimension. It attempts to adjust out-of-bounds ranges when possible and raises an\n    error for ranges that cannot be adjusted.\n\n    Args:\n        wavelengths (torch.Tensor): The wavelengths tensor. Its length determines the\n            valid range for segments.\n        segment_ranges (list[tuple[int, int]]): A list of segment ranges to be validated\n            and potentially adjusted. Each tuple represents (start, end) indices.\n\n    Returns:\n        list[tuple[int, int]]: The list of validated and potentially adjusted segment ranges.\n\n    Raises:\n        ValueError: If a segment range is entirely out of bounds and cannot be adjusted.\n\n    Warns:\n        UserWarning: If a segment range is partially out of bounds and is adjusted.\n\n    Notes:\n        - Segment ranges are inclusive of both start and end indices.\n        - Ranges extending below 0 are adjusted to start at 0 if possible.\n        - Ranges extending beyond the wavelength dimension are truncated if possible.\n        - Adjustments are only made if at least part of the range is within bounds.\n    \"\"\"\n    max_index = len(wavelengths)\n    adjusted_ranges = []\n\n    for start, end in segment_ranges:\n        if start &lt; 0:\n            if end &gt; 0:\n                warnings.warn(f\"Adjusting segment start from {start} to 0\")\n                start = 0\n            else:\n                raise ValueError(f\"Segment range {(start, end)} is out of bounds\")\n\n        if end &gt; max_index:\n            if start &lt; max_index:\n                warnings.warn(f\"Adjusting segment end from {(start, end)} to {(start, max_index)}\")\n                end = max_index\n            else:\n                raise ValueError(f\"Segment range {(start, end)} is out of bounds\")\n        adjusted_ranges.append((start, end))\n    return adjusted_ranges  # type: ignore\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.validate_band_format","title":"<code>validate_band_format(bands, variable_name)</code>","text":"<p>Validate the band format for a given variable.</p> <p>Parameters:</p> Name Type Description Default <code>bands</code> <code>dict[str | tuple[str, ...], BandType]</code> <p>A dictionary containing band ranges or list of wavelengths. The keys can be either a string or a tuple of strings. The values can be a single value, a tuple of two values, or a list of values.</p> required <code>variable_name</code> <code>str</code> <p>The name of the variable being validated.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the keys are not a string or a tuple of strings, or if the values do not match the expected types.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def validate_band_format(bands: dict[str | tuple[str, ...], BandType], variable_name: str) -&gt; None:\n    \"\"\"Validate the band format for a given variable.\n\n    Args:\n        bands (dict[str | tuple[str, ...], BandType]): A dictionary containing band ranges or list of wavelengths.\n            The keys can be either a string or a tuple of strings. The values can be a single value, a tuple of two values, or a list of values.\n        variable_name (str): The name of the variable being validated.\n\n    Raises:\n        TypeError: If the keys are not a string or a tuple of strings, or if the values do not match the expected types.\n\n    Returns:\n        None\n    \"\"\"\n    for keys, band_ranges in bands.items():\n        if not (isinstance(keys, str) or (isinstance(keys, tuple) and all(isinstance(key, str) for key in keys))):\n            raise TypeError(f\"{variable_name} keys should be string or tuple of strings\")\n        if isinstance(band_ranges, (int, float)):\n            continue\n        elif (\n            isinstance(band_ranges, tuple)\n            and len(band_ranges) == 2\n            and all(isinstance(item, (int, float)) for item in band_ranges)\n            and band_ranges[0] &lt; band_ranges[1]\n        ):\n            continue\n        elif isinstance(band_ranges, list) and (\n            all(\n                isinstance(item, tuple)\n                and len(item) == 2\n                and all(isinstance(subitem, (int, float)) for subitem in item)\n                and item[0] &lt; item[1]\n                for item in band_ranges\n            )\n            or all(isinstance(item, (int, float)) for item in band_ranges)\n        ):\n            continue\n        raise TypeError(\n            (\n                f\"{variable_name} should be either a value, list of values, \"\n                \"tuple of two values or list of tuples of two values.\"\n            )\n        )\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.validate_band_names","title":"<code>validate_band_names(band_names)</code>","text":"<p>Validates the band names provided.</p> <p>Parameters:</p> Name Type Description Default <code>band_names</code> <code>list[str | list[str]] | dict[tuple[str, ...] | str, int]</code> <p>The band names to validate. It can be either a list of strings or lists of strings, or a dictionary with keys as tuples or strings and values as integers.</p> required <p>Raises:</p> Type Description <code>TypeError</code> <p>If the band_names is not a list or a dictionary, or if the items in the list or the keys in the dictionary are not of the expected types.</p> <p>Returns:</p> Type Description <code>None</code> <p>None</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def validate_band_names(band_names: list[str | list[str]] | dict[tuple[str, ...] | str, int]) -&gt; None:\n    \"\"\"Validates the band names provided.\n\n    Args:\n        band_names (list[str | list[str]] | dict[tuple[str, ...] | str, int]): The band names to validate.\n            It can be either a list of strings or lists of strings, or a dictionary with keys as tuples or strings\n            and values as integers.\n\n    Raises:\n        TypeError: If the band_names is not a list or a dictionary, or if the items in the list or the keys in the\n            dictionary are not of the expected types.\n\n    Returns:\n        None\n    \"\"\"\n    if isinstance(band_names, dict):\n        for key, item in band_names.items():\n            if not (\n                isinstance(key, str) or (isinstance(key, tuple) and all(isinstance(subitem, str) for subitem in key))\n            ):\n                raise TypeError(\"All keys in band_names dictionary should be str or tuple of str.\")\n            if not isinstance(item, int):\n                raise TypeError(\"All values in band_names dictionary should be int.\")\n    elif isinstance(band_names, list):\n        for item in band_names:  # type: ignore\n            if not (\n                isinstance(item, str) or (isinstance(item, list) and all(isinstance(subitem, str) for subitem in item))\n            ):\n                raise TypeError(\"All items in band_names list should be str or list of str.\")\n    else:\n        raise TypeError(\"band_names should be either a list or a dictionary.\")\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.validate_mask_shape","title":"<code>validate_mask_shape(mask_type, hsi, mask)</code>","text":"<p>Validate mask (segmentation or band mask) shape against the hyperspectral image.</p> <p>Parameters:</p> Name Type Description Default <code>mask_type</code> <code>Literal[&amp;quot;spatial&amp;quot;, &amp;quot;spectral&amp;quot;]</code> <p>a problem type specifying type of the attribution for the mask is segmentation or band mask</p> required <code>hsi</code> <code>HSI</code> <p>An original hyperspectral image for which the mask is created</p> required <code>mask</code> <code>Tensor</code> <p>A segmentation or band mask to be validated.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>In case the shapes cannot be broadcasted, or the image and band mask orientation is invalid</p> <p>Returns:     torch.Tensor: The validated mask</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def validate_mask_shape(mask_type: Literal[\"segmentation\", \"band\"], hsi: HSI, mask: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Validate mask (segmentation or band mask) shape against the hyperspectral image.\n\n    Args:\n        mask_type (Literal[&amp;quot;spatial&amp;quot;, &amp;quot;spectral&amp;quot;]): a problem type specifying type of the attribution for the mask is segmentation or band mask\n        hsi (HSI): An original hyperspectral image for which the mask is created\n        mask (torch.Tensor): A segmentation or band mask to be validated.\n\n    Raises:\n        ValueError: In case the shapes cannot be broadcasted, or the image and band mask orientation is invalid\n    Returns:\n        torch.Tensor: The validated mask\n    \"\"\"\n\n    if mask_type not in [\"segmentation\", \"band\"]:\n        raise ValueError(f\"Unsupported mask type passed to validation: {mask_type}\")\n\n    image_shape = hsi.image.shape\n    mask_shape = mask.shape\n\n    if len(mask_shape) == 2 and mask_type == \"segmentation\":\n        logger.warning(\"The segmentation mask is 2D, adding a new dimension to match the image shape\")\n        mask = mask.unsqueeze(hsi.spectral_axis)\n        mask_shape = mask.shape\n    elif len(mask_shape) != 3:\n        raise ValueError(f\"Mask should be a 3D tensor, but got shape: {mask_shape}\")\n\n    try:\n        broadcasted_shape = torch.broadcast_shapes(image_shape, mask_shape)\n    except RuntimeError as e:\n        raise ValueError(\n            f\"Cannot broadcast image and mask of shapes {image_shape} and {mask_shape} respectively: {e}\"\n        ) from e\n\n    if broadcasted_shape != image_shape:\n        raise ValueError(f\"Image and mask shapes are not compatible: {image_shape} and {mask_shape}\")\n\n    # check on which dims the shapes match - the segmentation mask can differ only in the band dimension, band mask can differ in the height and width dimensions\n    shape_matches = [broadcasted_shape[i] == mask_shape[i] for i in range(3)]\n    orientation_mismatches = {hsi.orientation[i] for i in range(3) if not shape_matches[i]}\n\n    if mask_type == \"segmentation\" and (\"H\" in orientation_mismatches or \"W\" in orientation_mismatches):\n        raise ValueError(\n            f\"Image and mask orientation mismatch: {hsi.orientation} and {mask_shape}.\"\n            + \"Segmentation mask should differ only in the band dimension\"\n        )\n\n    if mask_type == \"band\" and \"C\" in orientation_mismatches:\n        raise ValueError(\n            f\"Image and mask orientation mismatch: {hsi.orientation} and {mask_shape}.\"\n            + \"Band mask should differ only in the height and width dimensions\"\n        )\n\n    return mask\n</code></pre>"},{"location":"reference/#src.meteors.attr.lime.validate_segment_format","title":"<code>validate_segment_format(segment, dtype=int)</code>","text":"<p>Validates the format of the segment.</p> <p>Parameters:</p> Name Type Description Default <code>segment</code> <code>tuple[int | float, int | float] | list[tuple[int | float, int | float]]</code> <p>The segment to validate.</p> required <code>dtype</code> <code>Type</code> <p>The data type of the segment range. Defaults to int.</p> <code>int</code> <p>Returns:</p> Type Description <code>list[tuple[IntOrFloat, IntOrFloat]]</code> <p>list[tuple[int | float, int | float]]: The validated segment range.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the segment range is not in the correct format.</p> Source code in <code>src/meteors/attr/lime.py</code> <pre><code>def validate_segment_format(\n    segment: tuple[IntOrFloat, IntOrFloat] | list[tuple[IntOrFloat, IntOrFloat]], dtype: Type = int\n) -&gt; list[tuple[IntOrFloat, IntOrFloat]]:\n    \"\"\"Validates the format of the segment.\n\n    Args:\n        segment (tuple[int | float, int | float] | list[tuple[int | float, int | float]]):\n            The segment to validate.\n        dtype (Type, optional): The data type of the segment range. Defaults to int.\n\n    Returns:\n        list[tuple[int | float, int | float]]: The validated segment range.\n\n    Raises:\n        ValueError: If the segment range is not in the correct format.\n    \"\"\"\n    if (\n        isinstance(segment, tuple)\n        and len(segment) == 2\n        and all(isinstance(x, dtype) for x in segment)\n        and segment[0] &lt; segment[1]\n    ):\n        logger.debug(\"Converting tuple segment to list of tuples\")\n        segment = [segment]  # Standardize single tuple to list of tuples\n    elif not (\n        isinstance(segment, list)\n        and all(\n            isinstance(sub_segment, tuple)\n            and len(sub_segment) == 2\n            and all(isinstance(part, dtype) for part in sub_segment)\n            and sub_segment[0] &lt; sub_segment[1]\n            for sub_segment in segment\n        )\n    ):\n        raise ValueError(\n            (\n                f\"Each segment range should be a tuple or list of two numbers of data type {dtype} (start, end). \"\n                f\"Where start &lt; end. But got: {segment}\"\n            )\n        )\n    return segment\n</code></pre>"},{"location":"reference/#lime-base","title":"Lime Base","text":"<p>The Lime Base class was adapted from the Captum Lime implementation. This adaptation builds upon the original work, extending and customizing it for specific use cases within this project. To see the original implementation, please refer to the Captum repository.</p>"},{"location":"reference/#integrated-gradients","title":"Integrated Gradients","text":"<p>The Integrated Gradients method is based on the <code>captum</code> implementation and is an implementation of an idea coming from the original paper on Integrated Gradients, where more details about this method can be found.</p>"},{"location":"reference/#inputxgradients","title":"InputXGradients","text":"<p>The InputXGradients method is a straightforward approach to computing attribution. It simply multiplies the input image with the gradient with respect to the input. This method is based on the <code>captum</code> implementation</p>"},{"location":"reference/#occlusion","title":"Occlusion","text":"<p>This attribution method perturbs the input by replacing the contiguous rectangular region with a given baseline and computing the difference in output. In our case, features are located in multiple regions, and attribution from different hyper-rectangles is averaged.</p> <p>The implementation of this method is also based on the <code>captum</code> repository.</p> <p>More details about this approach can be found in the original paper</p>"},{"location":"reference/#saliency","title":"Saliency","text":"<p>This baseline method for computing input attribution calculates gradients with respect to inputs. It also has an option to return the absolute value of the gradients, which is the default behaviour.</p> <p>Implementation of this method is based on the <code>captum</code> repository</p>"},{"location":"reference/#noise-tunnel","title":"Noise Tunnel","text":"<p>This attribution method works on top of a different one to better approximate its explanations. The Noise Tunnel (Smooth Grad) adds Gaussian noise to each input in the batch and applies the given attribution algorithm to each modified sample.</p> <p>This method is based on the <code>captum</code> implementation</p>"},{"location":"reference/#hyper-noise-tunnel","title":"Hyper Noise Tunnel","text":"<p>Hyper Noise Tunnel is our novel method, designed specifically to explain hyperspectral satellite images. It is inspired by the behaviour of the classical Noise Tunnel (Smooth Grad) method, but instead of sampling noise into the original image, it randomly removes some of the bands. In the process, the created noised samples are close to the distribution of the original image yet differ enough to smoothen the produced attribution map.</p>"},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.BaseHyperNoiseTunnel","title":"<code>BaseHyperNoiseTunnel</code>","text":"<p>               Bases: <code>Attribution</code></p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>class BaseHyperNoiseTunnel(Attribution):\n    def __init__(self, model: GradientAttribution):\n        self.attribute_main = model.attribute\n        sig = inspect.signature(self.attribute_main)\n        if \"abs\" in sig.parameters:\n            self.attribute_main = partial(self.attribute_main, abs=False)\n\n    @staticmethod\n    def perturb_input(\n        input, baseline, n_samples: int = 1, perturbation_prob: float = 0.5, num_perturbed_bands: int | None = None\n    ):\n        \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n        that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n        `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n        set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n        of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n        Args:\n            input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n            baseline (torch.Tensor): A baseline tensor to replace the perturbed bands.\n            n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n            perturbation_prob (float, optional): A probability that each band will be perturbed intependently. Defaults to 0.5.\n            num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n        Returns:\n            torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n        \"\"\"\n        if input.dim() != 3 and input.dim() != 4:\n            raise ValueError(\"Input must be in the format (C, H, W) or (N, C, H, W)\")\n\n        # validate the baseline against the input\n        if baseline.shape != input.shape:\n            raise ValueError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n        if n_samples &lt; 1:\n            raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n        if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n            raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n        # the perturbation\n        perturbed_input = input.clone().unsqueeze(0)\n        # repeat the perturbed_input on the first dimension n_samples times\n        perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n        n_samples_x_channels_shape = (\n            n_samples,\n            input.shape[0],\n        )  # shape of the tensor containing the perturbed channels for each sample\n\n        channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n        if num_perturbed_bands is None:\n            channel_perturbation_probabilities = (\n                torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n            )\n            channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n        else:\n            if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n                raise ValueError(\n                    f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n                )\n\n            channels_to_be_perturbed = torch_random_choice(\n                input.shape[0], num_perturbed_bands, n_samples, device=input.device\n            )\n\n        # now having chosen the perturbed channels, we can replace them with the baseline\n\n        reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n        perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n        perturbed_input.requires_grad_(True)\n\n        return perturbed_input\n\n    def attribute(\n        self,\n        inputs: Tensor,\n        baselines: Union[Tensor, int, float],\n        target: int | None = None,\n        additional_forward_args: Any = None,\n        n_samples: int = 5,\n        steps_per_batch: int = 1,\n        method: str = \"smoothgrad\",\n        perturbation_prob: float = 0.5,\n        num_perturbed_bands: int | None = None,\n    ) -&gt; Tensor:\n        if method not in [\"smoothgrad\", \"smoothgrad_sq\", \"vargrad\"]:\n            raise ValueError(\"Method must be one of 'smoothgrad', 'smoothgrad_sq', 'vargrad'\")\n\n        if inputs.dim() == 3:\n            inputs = inputs.unsqueeze(0)\n        elif inputs.dim() != 4:\n            raise ValueError(\"Input must be in the format (N, C, H, W)\")\n\n        if not isinstance(baselines, Tensor) and not isinstance(baselines, int) and not isinstance(baselines, float):\n            raise ValueError(\"Baselines must be a tensor or a scalar\")\n\n        if isinstance(baselines, int) or isinstance(baselines, float):\n            baselines = torch.zeros_like(inputs, device=inputs.device).squeeze(0) + baselines\n        elif baselines.dim() == 4:\n            baselines = baselines.squeeze(0)\n        elif baselines.dim() != 3:\n            raise ValueError(\"Baselines must be in the format (C, H, W)\")\n\n        attributions = torch.empty((n_samples,) + inputs.shape, device=inputs.device)\n\n        for batch in range(0, inputs.shape[0]):\n            input = inputs[batch]\n            perturbed_input = BaseHyperNoiseTunnel.perturb_input(\n                input, baselines, n_samples, perturbation_prob, num_perturbed_bands\n            )\n            for i in range(0, n_samples, steps_per_batch):\n                perturbed_batch = perturbed_input[i : i + steps_per_batch]\n                attributions[i : i + steps_per_batch, batch] = self.attribute_main(\n                    perturbed_batch, target=target, additional_forward_args=additional_forward_args\n                )\n            else:\n                steps_left = n_samples % steps_per_batch\n                if steps_left:\n                    perturbed_batch = perturbed_input[-steps_left:]\n                    attributions[-steps_left:, batch] = self.attribute_main(\n                        perturbed_batch, target=target, additional_forward_args=additional_forward_args\n                    )\n\n        if method == \"smoothgrad\":\n            return attributions.mean(dim=0)\n        elif method == \"smoothgrad_sq\":\n            return (attributions**2).mean(dim=0)\n        else:\n            return (attributions**2 - attributions.mean(dim=0) ** 2).mean(dim=0)\n</code></pre>"},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.BaseHyperNoiseTunnel.perturb_input","title":"<code>perturb_input(input, baseline, n_samples=1, perturbation_prob=0.5, num_perturbed_bands=None)</code>  <code>staticmethod</code>","text":"<p>The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands that will be masked out and replaced with the baseline. The parameters <code>num_perturbed_bands</code> and <code>perturbation_prob</code> control the number of bands that will be perturbed (masked). If <code>num_perturbed_bands</code> is set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number of bands will be drawn from a binomial distribution with <code>perturbation_prob</code> as the probability of success.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Tensor</code> <p>An input tensor to be perturbed. It should have the shape (C, H, W).</p> required <code>baseline</code> <code>Tensor</code> <p>A baseline tensor to replace the perturbed bands.</p> required <code>n_samples</code> <code>int</code> <p>A number of samples to be drawn - number of perturbed inputs to be generated.</p> <code>1</code> <code>perturbation_prob</code> <code>float</code> <p>A probability that each band will be perturbed intependently. Defaults to 0.5.</p> <code>0.5</code> <code>num_perturbed_bands</code> <code>int | None</code> <p>A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability <code>perturbation_prob</code> each. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>torch.Tensor: A perturbed tensor, which contains <code>n_samples</code> perturbed inputs.</p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>@staticmethod\ndef perturb_input(\n    input, baseline, n_samples: int = 1, perturbation_prob: float = 0.5, num_perturbed_bands: int | None = None\n):\n    \"\"\"The perturbation function used in the hyper noise tunnel. It randomly selects a subset of the input bands\n    that will be masked out and replaced with the baseline. The parameters `num_perturbed_bands` and\n    `perturbation_prob` control the number of bands that will be perturbed (masked). If `num_perturbed_bands` is\n    set, it will be used as the number of bands to perturb, which will be randomly selected. Otherwise, the number\n    of bands will be drawn from a binomial distribution with `perturbation_prob` as the probability of success.\n\n    Args:\n        input (torch.Tensor): An input tensor to be perturbed. It should have the shape (C, H, W).\n        baseline (torch.Tensor): A baseline tensor to replace the perturbed bands.\n        n_samples (int): A number of samples to be drawn - number of perturbed inputs to be generated.\n        perturbation_prob (float, optional): A probability that each band will be perturbed intependently. Defaults to 0.5.\n        num_perturbed_bands (int | None, optional): A number of perturbed bands in the whole image. If set to None, the bands are perturbed with probability `perturbation_prob` each. Defaults to None.\n\n    Returns:\n        torch.Tensor: A perturbed tensor, which contains `n_samples` perturbed inputs.\n    \"\"\"\n    if input.dim() != 3 and input.dim() != 4:\n        raise ValueError(\"Input must be in the format (C, H, W) or (N, C, H, W)\")\n\n    # validate the baseline against the input\n    if baseline.shape != input.shape:\n        raise ValueError(f\"Baseline shape {baseline.shape} does not match input shape {input.shape}\")\n\n    if n_samples &lt; 1:\n        raise ValueError(\"Number of perturbated samples to be generated must be greater than 0\")\n\n    if perturbation_prob &lt; 0 or perturbation_prob &gt; 1:\n        raise ValueError(\"Perturbation probability must be in the range [0, 1]\")\n\n    # the perturbation\n    perturbed_input = input.clone().unsqueeze(0)\n    # repeat the perturbed_input on the first dimension n_samples times\n    perturbed_input = perturbed_input.repeat_interleave(n_samples, dim=0)\n\n    n_samples_x_channels_shape = (\n        n_samples,\n        input.shape[0],\n    )  # shape of the tensor containing the perturbed channels for each sample\n\n    channels_to_be_perturbed: torch.Tensor = torch.zeros(n_samples_x_channels_shape, device=input.device).bool()\n\n    if num_perturbed_bands is None:\n        channel_perturbation_probabilities = (\n            torch.ones(n_samples_x_channels_shape, device=input.device) * perturbation_prob\n        )\n        channels_to_be_perturbed = torch.bernoulli(channel_perturbation_probabilities).bool()\n\n    else:\n        if num_perturbed_bands &lt; 0 or num_perturbed_bands &gt; input.shape[0]:\n            raise ValueError(\n                f\"Cannot perturb {num_perturbed_bands} bands in the input with {input.shape[0]} channels. The number of perturbed bands must be in the range [0, {input.shape[0]}]\"\n            )\n\n        channels_to_be_perturbed = torch_random_choice(\n            input.shape[0], num_perturbed_bands, n_samples, device=input.device\n        )\n\n    # now having chosen the perturbed channels, we can replace them with the baseline\n\n    reshaped_baseline = baseline.unsqueeze(0).repeat_interleave(n_samples, dim=0)\n    perturbed_input[channels_to_be_perturbed] = reshaped_baseline[channels_to_be_perturbed]\n\n    perturbed_input.requires_grad_(True)\n\n    return perturbed_input\n</code></pre>"},{"location":"reference/#src.meteors.attr.hyper_noise_tunnel.torch_random_choice","title":"<code>torch_random_choice(n, k, n_samples, device=None)</code>","text":"<p>Randomly selects <code>k</code> elements from the range [0, n) without replacement.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The range of the selection.</p> required <code>k</code> <code>int</code> <p>The number of elements to select.</p> required <code>n_samples</code> <code>int</code> <p>The number of samples to be drawn.</p> required <code>device</code> <code>device | str | None</code> <p>The device to which the tensor will be moved.</p> <code>None</code> <p>Returns:     torch.Tensor: A tensor of shape (n_samples,n) containing True for the selected elements and False for the rest. Each row contains k True values.</p> Source code in <code>src/meteors/attr/hyper_noise_tunnel.py</code> <pre><code>def torch_random_choice(n: int, k: int, n_samples: int, device: torch.device | str | None = None) -&gt; torch.Tensor:\n    \"\"\"Randomly selects `k` elements from the range [0, n) without replacement.\n\n    Args:\n        n (int): The range of the selection.\n        k (int): The number of elements to select.\n        n_samples (int): The number of samples to be drawn.\n        device (torch.device | str | None): The device to which the tensor will be moved.\n    Returns:\n        torch.Tensor: A tensor of shape (n_samples,n) containing True for the selected elements and False for the rest. Each row contains k True values.\n    \"\"\"\n    if k &gt; n:\n        raise ValueError(f\"Cannot select {k} elements from the range [0, {n})\")\n    if k == n:\n        return torch.ones((n_samples, n), device=device).bool()\n    result = torch.zeros((n_samples, n), device=device).bool()\n    for i in range(n_samples):\n        result[i, torch.randperm(n)[:k]] = True\n    return result\n</code></pre>"},{"location":"reference/#hyperspectral-image","title":"HyperSpectral Image","text":"<p>The HyperSpectral Image is a dataclass that stores information about a specific hyperspectral image. It contains all the necessary data for the image to be preprocessed, such as: \u00a0- wavelengths of all the image bands \u00a0- orientation of the image (information on which image axis corresponds to width, height or channel dimension) \u00a0- binary mask - which is the mask of the image that can cover unnecessary regions for the model</p> <p>Additionally, this class facilitates plotting the images and helps to integrate images into our attribution methods.</p>"},{"location":"reference/#src.meteors.hsi.HSI","title":"<code>HSI</code>","text":"<p>               Bases: <code>BaseModel</code></p> Source code in <code>src/meteors/hsi.py</code> <pre><code>class HSI(BaseModel):\n    image: Annotated[  # Should always be a first field\n        torch.Tensor,\n        PlainValidator(ensure_image_tensor),\n        Field(description=\"Hyperspectral image. Converted to torch tensor.\"),\n    ]\n    wavelengths: Annotated[\n        torch.Tensor,\n        PlainValidator(ensure_wavelengths_tensor),\n        Field(description=\"Wavelengths present in the image. Defaults to None.\"),\n    ]\n    orientation: Annotated[\n        tuple[str, str, str],\n        PlainValidator(validate_orientation),\n        Field(\n            description=(\n                'Orientation of the image - sequence of three one-letter strings in any order: \"C\", \"H\", \"W\" '\n                'meaning respectively channels, height and width of the image. Defaults to (\"C\", \"H\", \"W\").'\n            ),\n        ),\n    ] = (\"C\", \"H\", \"W\")\n    device: Annotated[\n        torch.device,\n        PlainValidator(resolve_inference_device_hsi),\n        Field(\n            validate_default=True,\n            exclude=True,\n            description=\"Device to be used for inference. If None, the device of the input image will be used. Defaults to None.\",\n        ),\n    ] = None\n    binary_mask: Annotated[\n        torch.Tensor,\n        PlainValidator(process_and_validate_binary_mask),\n        Field(\n            validate_default=True,\n            description=(\n                \"Binary mask used to cover not important parts of the base image, masked parts have values equals to 0. \"\n                \"Converted to torch tensor. Defaults to None.\"\n            ),\n        ),\n    ] = None\n\n    @property\n    def spectral_axis(self) -&gt; int:\n        \"\"\"Returns the index of the spectral (wavelength) axis based on the current data orientation.\n\n        In hyperspectral imaging, the spectral axis represents the dimension along which\n        different spectral bands or wavelengths are arranged. This property dynamically\n        determines the index of this axis based on the current orientation of the data.\n\n        Returns:\n            int: The index of the spectral axis in the current data structure.\n                - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first)\n                - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last)\n                - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)\n\n        Note:\n            The orientation is typically represented as a string where:\n            - 'C' represents the spectral/wavelength dimension\n            - 'H' represents the height (rows) of the image\n            - 'W' represents the width (columns) of the image\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI()\n            &gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            0\n            &gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n            &gt;&gt;&gt; hsi_image.spectral_axis\n            2\n        \"\"\"\n        return get_channel_axis(self.orientation)\n\n    @property\n    def spatial_binary_mask(self) -&gt; torch.Tensor:\n        \"\"\"Returns a 2D spatial representation of the binary mask.\n\n        This property extracts a single 2D slice from the 3D binary mask, assuming that\n        the mask is identical across all spectral bands. It handles different data\n        orientations by first ensuring the spectral dimension is the last dimension\n        before extracting the 2D spatial mask.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the spatial binary mask.\n                The shape will be (H, W) where H is height and W is width of the image.\n\n        Note:\n            - This assumes that the binary mask is consistent across all spectral bands.\n            - The returned mask is always 2D, regardless of the original data orientation.\n\n        Examples:\n            &gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n            &gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n            &gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n            &gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\n            torch.Size([100, 100])\n        \"\"\"\n        mask = self.binary_mask if self.binary_mask is not None else torch.ones_like(self.image)\n        return mask.select(dim=self.spectral_axis, index=0)\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)\n\n    @model_validator(mode=\"after\")\n    def validate_image_data(self) -&gt; Self:\n        \"\"\"Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.\n\n        Returns:\n            Self: The instance of the class.\n        \"\"\"\n        validate_shapes(self.wavelengths, self.image, self.spectral_axis)\n        return self\n\n    def to(self, device: str | torch.device) -&gt; Self:\n        \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n        Args:\n            device (str or torch.device): The device to move the image and binary mask to.\n\n        Returns:\n            Self: The updated HSI object.\n\n        Examples:\n            &gt;&gt;&gt; # Create an HSI object\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n            &gt;&gt;&gt; # Move the image to cpu\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cpu')\n            &gt;&gt;&gt; # Move the image to cuda\n            &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n            &gt;&gt;&gt; hsi_image.device\n            device(type='cuda', index=0)\n        \"\"\"\n        self.image = self.image.to(device)\n        self.binary_mask = self.binary_mask.to(device)\n        self.device = self.image.device\n        return self\n\n    def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n        \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n        Returns:\n            torch.Tensor: The hyperspectral image data.\n\n        Notes:\n            - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; image = hsi_image.get_image()\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n            &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n            &gt;&gt;&gt; image.shape\n            torch.Size([10, 100, 100])\n        \"\"\"\n        if apply_mask and self.binary_mask is not None:\n            return self.image * self.binary_mask\n        return self.image\n\n    def get_rgb_image(\n        self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n        This method creates a 3-channel RGB image by selecting appropriate bands\n        corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n        Args:\n            apply_mask (bool, optional): Whether to apply the binary mask to the image.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n                cutoff to the image. Defaults to False.\n            output_channel_axis (int | None, optional): The axis where the RGB channels\n                should be placed in the output tensor. If None, uses the current spectral\n                axis of the hyperspectral data. Defaults to None.\n\n        Returns:\n            torch.Tensor: The RGB representation of the hyperspectral image.\n                Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n                the specified output_channel_axis, where H is height and W is width.\n\n        Notes:\n            - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n            - Each band is normalized independently before combining into the RGB image.\n            - If apply_mask is True, masked areas will be set to zero in the output.\n            - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([3, 100, 100])\n\n            &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n            &gt;&gt;&gt; rgb_image.shape\n            torch.Size([100, 100, 3])\n        \"\"\"\n        if output_channel_axis is None:\n            output_channel_axis = self.spectral_axis\n\n        rgb_img = torch.stack(\n            [\n                self.extract_band_by_name(\n                    band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n                )\n                for band in [\"R\", \"G\", \"B\"]\n            ],\n            dim=self.spectral_axis,\n        )\n\n        return (\n            rgb_img\n            if output_channel_axis == self.spectral_axis\n            else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n        )\n\n    def _extract_central_slice_from_band(\n        self,\n        band_wavelengths: torch.Tensor,\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts and processes the central wavelength band from a given range in the hyperspectral image.\n\n        This method selects the central band from a specified range of wavelengths,\n        applies optional processing steps (masking, normalization, and minimum cutoff),\n        and returns the resulting 2D image slice.\n\n        Args:\n            band_wavelengths (torch.Tensor): The selected wavelengths that define the whole band\n                from which the central slice will be extracted.\n                All of the passed wavelengths must be present in the image.\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff.\n                If True, sets the minimum non-zero value to zero after normalization.\n                Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the processed central wavelength band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Notes:\n            - The central wavelength is determined as the middle index of the provided wavelengths list.\n            - If normalization is applied, it's done before masking and cutoff operations.\n            - The binary mask, if applied, is expected to have the same spatial dimensions as the image.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(13, 100, 100), wavelengths=np.linspace(400, 1000, 13))\n            &gt;&gt;&gt; band_wavelengths = torch.tensor([500, 600, 650, 700])\n            &gt;&gt;&gt; central_slice = hsi_image._extract_central_slice_from_band(band_wavelengths)\n            &gt;&gt;&gt; central_slice.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract a slice without normalization or masking\n            &gt;&gt;&gt; raw_band = hsi_image._extract_central_slice_from_band(band_wavelengths, apply_mask=False, normalize=False)\n        \"\"\"\n        # check if all wavelengths from the `band_wavelengths` are present in the image\n        if not all(wave in self.wavelengths for wave in band_wavelengths):\n            raise ValueError(\"All of the passed wavelengths must be present in the image\")\n\n        # sort the `band_wavelengths` to ensure the central band is selected\n        band_wavelengths = torch.sort(band_wavelengths).values\n\n        start_index = np.where(self.wavelengths == band_wavelengths[0])[0][0]\n        relative_center_band_index = len(band_wavelengths) // 2\n        central_band_index = start_index + relative_center_band_index\n\n        # Ensure the spectral dimension is the last\n        image = self.image if self.spectral_axis == 2 else torch.moveaxis(self.image, self.spectral_axis, 2)\n\n        slice = image[..., central_band_index]\n\n        if normalize:\n            if apply_min_cutoff:\n                slice_min = slice[slice != 0].min()\n            else:\n                slice_min = slice.min()\n\n            slice_max = slice.max()\n            if slice_max &gt; slice_min:  # Avoid division by zero\n                slice = (slice - slice_min) / (slice_max - slice_min)\n\n            if apply_min_cutoff:\n                slice[slice == slice.min()] = 0  # Set minimum values to zero\n\n        if apply_mask:\n            mask = (\n                self.binary_mask if self.spectral_axis == 2 else torch.moveaxis(self.binary_mask, self.spectral_axis, 2)\n            )\n            slice = slice * mask[..., central_band_index]\n\n        return slice\n\n    def extract_band_by_name(\n        self,\n        band_name: str,\n        selection_method: str = \"center\",\n        apply_mask: bool = True,\n        apply_min_cutoff: bool = False,\n        normalize: bool = True,\n    ) -&gt; torch.Tensor:\n        \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n        This method uses the spyndex library to map standardized band names to wavelength ranges,\n        then extracts the corresponding band from the hyperspectral data.\n\n        Args:\n            band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n            selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n                Currently, only \"center\" is supported, which selects the central wavelength.\n                Defaults to \"center\".\n            apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n                Defaults to True.\n            apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n                If True, sets the minimum non-zero value to zero. Defaults to False.\n            normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n                Defaults to True.\n\n        Returns:\n            torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n                Shape will be (H, W), where H is height and W is width of the image.\n\n        Raises:\n            ValueError: If the specified band name is not found in the spyndex library.\n            NotImplementedError: If a selection method other than \"center\" is specified.\n\n        Notes:\n            - The spyndex library is used to map band names to wavelength ranges.\n            - Currently, only the \"center\" selection method is implemented, which chooses\n            the central wavelength within the specified range.\n            - Processing steps are applied in the order: normalization, cutoff, masking.\n\n        Examples:\n            &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n            &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n            &gt;&gt;&gt; red_band.shape\n            torch.Size([100, 100])\n\n            &gt;&gt;&gt; # Extract NIR band without normalization or masking\n            &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n        \"\"\"\n        band_info = spyndex.bands.get(band_name)\n        if band_info is None:\n            raise ValueError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n        min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n        selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n        if selection_method == \"center\":\n            return self._extract_central_slice_from_band(\n                selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n            )\n        else:\n            raise NotImplementedError(\n                f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n            )\n\n    def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n        \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n        Args:\n            target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n                This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n            inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n        Returns:\n            Self: The updated HSI object with the new orientation.\n\n        Raises:\n            ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n        \"\"\"\n        target_orientation = validate_orientation(target_orientation)\n\n        if inplace:\n            hsi = self\n        else:\n            hsi = self.model_copy()\n\n        if target_orientation == self.orientation:\n            return hsi\n\n        permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n        # permute the image\n        hsi.image = hsi.image.permute(permute_dims)\n\n        # permute the binary mask\n        if hsi.binary_mask is not None:\n            hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n        hsi.orientation = target_orientation\n\n        return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spatial_binary_mask","title":"<code>spatial_binary_mask: torch.Tensor</code>  <code>property</code>","text":"<p>Returns a 2D spatial representation of the binary mask.</p> <p>This property extracts a single 2D slice from the 3D binary mask, assuming that the mask is identical across all spectral bands. It handles different data orientations by first ensuring the spectral dimension is the last dimension before extracting the 2D spatial mask.</p> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the spatial binary mask. The shape will be (H, W) where H is height and W is width of the image.</p> Note <ul> <li>This assumes that the binary mask is consistent across all spectral bands.</li> <li>The returned mask is always 2D, regardless of the original data orientation.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # If self.binary_mask has shape (100, 100, 5) with spectral_axis=2:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(100, 100, 5), orientation=(\"H\", \"W\", \"C\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n&gt;&gt;&gt; If self.binary_mask has shape (5, 100, 100) with spectral_axis=0:\n&gt;&gt;&gt; hsi_image = HSI(binary_mask=torch.rand(5, 100, 100), orientation=(\"C\", \"H\", \"W\"))\n&gt;&gt;&gt; hsi_image.spatial_binary_mask.shape\ntorch.Size([100, 100])\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.spectral_axis","title":"<code>spectral_axis: int</code>  <code>property</code>","text":"<p>Returns the index of the spectral (wavelength) axis based on the current data orientation.</p> <p>In hyperspectral imaging, the spectral axis represents the dimension along which different spectral bands or wavelengths are arranged. This property dynamically determines the index of this axis based on the current orientation of the data.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the spectral axis in the current data structure. - 0 for 'CHW' or 'CWH' orientations (Channel/Wavelength first) - 2 for 'HWC' or 'WHC' orientations (Channel/Wavelength last) - 1 for 'HCW' or 'WCH' orientations (Channel/Wavelength in the middle)</p> Note <p>The orientation is typically represented as a string where: - 'C' represents the spectral/wavelength dimension - 'H' represents the height (rows) of the image - 'W' represents the width (columns) of the image</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI()\n&gt;&gt;&gt; hsi_image.orientation = \"CHW\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n0\n&gt;&gt;&gt; hsi_image.orientation = \"HWC\"\n&gt;&gt;&gt; hsi_image.spectral_axis\n2\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.change_orientation","title":"<code>change_orientation(target_orientation, inplace=False)</code>","text":"<p>Changes the orientation of the hsi data to the target orientation.</p> <p>Parameters:</p> Name Type Description Default <code>target_orientation</code> <code>(tuple[str, str, str], list[str], str)</code> <p>The target orientation for the hsi data. This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".</p> required <code>inplace</code> <code>bool</code> <p>Whether to modify the hsi data in place or return a new object.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object with the new orientation.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the target orientation is not a valid tuple of three one-letter strings.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def change_orientation(self, target_orientation: tuple[str, str, str] | list[str] | str, inplace=False) -&gt; Self:\n    \"\"\"Changes the orientation of the hsi data to the target orientation.\n\n    Args:\n        target_orientation (tuple[str, str, str], list[str], str): The target orientation for the hsi data.\n            This should be a tuple of three one-letter strings in any order: \"C\", \"H\", \"W\".\n        inplace (bool, optional): Whether to modify the hsi data in place or return a new object.\n\n    Returns:\n        Self: The updated HSI object with the new orientation.\n\n    Raises:\n        ValueError: If the target orientation is not a valid tuple of three one-letter strings.\n    \"\"\"\n    target_orientation = validate_orientation(target_orientation)\n\n    if inplace:\n        hsi = self\n    else:\n        hsi = self.model_copy()\n\n    if target_orientation == self.orientation:\n        return hsi\n\n    permute_dims = [hsi.orientation.index(dim) for dim in target_orientation]\n\n    # permute the image\n    hsi.image = hsi.image.permute(permute_dims)\n\n    # permute the binary mask\n    if hsi.binary_mask is not None:\n        hsi.binary_mask = hsi.binary_mask.permute(permute_dims)\n\n    hsi.orientation = target_orientation\n\n    return hsi\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.extract_band_by_name","title":"<code>extract_band_by_name(band_name, selection_method='center', apply_mask=True, apply_min_cutoff=False, normalize=True)</code>","text":"<p>Extracts a single spectral band from the hyperspectral image based on a standardized band name.</p> <p>This method uses the spyndex library to map standardized band names to wavelength ranges, then extracts the corresponding band from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>band_name</code> <code>str</code> <p>The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").</p> required <code>selection_method</code> <code>str</code> <p>The method to use for selecting the band within the wavelength range. Currently, only \"center\" is supported, which selects the central wavelength. Defaults to \"center\".</p> <code>'center'</code> <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the extracted band. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff after normalization. If True, sets the minimum non-zero value to zero. Defaults to False.</p> <code>False</code> <code>normalize</code> <code>bool</code> <p>Whether to normalize the band values to the [0, 1] range. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A 2D tensor representing the extracted and processed spectral band. Shape will be (H, W), where H is height and W is width of the image.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified band name is not found in the spyndex library.</p> <code>NotImplementedError</code> <p>If a selection method other than \"center\" is specified.</p> Notes <ul> <li>The spyndex library is used to map band names to wavelength ranges.</li> <li>Currently, only the \"center\" selection method is implemented, which chooses the central wavelength within the specified range.</li> <li>Processing steps are applied in the order: normalization, cutoff, masking.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n&gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n&gt;&gt;&gt; red_band.shape\ntorch.Size([100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; # Extract NIR band without normalization or masking\n&gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def extract_band_by_name(\n    self,\n    band_name: str,\n    selection_method: str = \"center\",\n    apply_mask: bool = True,\n    apply_min_cutoff: bool = False,\n    normalize: bool = True,\n) -&gt; torch.Tensor:\n    \"\"\"Extracts a single spectral band from the hyperspectral image based on a standardized band name.\n\n    This method uses the spyndex library to map standardized band names to wavelength ranges,\n    then extracts the corresponding band from the hyperspectral data.\n\n    Args:\n        band_name (str): The standardized name of the band to extract (e.g., \"Red\", \"NIR\", \"SWIR1\").\n        selection_method (str, optional): The method to use for selecting the band within the wavelength range.\n            Currently, only \"center\" is supported, which selects the central wavelength.\n            Defaults to \"center\".\n        apply_mask (bool, optional): Whether to apply the binary mask to the extracted band.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity cutoff after normalization.\n            If True, sets the minimum non-zero value to zero. Defaults to False.\n        normalize (bool, optional): Whether to normalize the band values to the [0, 1] range.\n            Defaults to True.\n\n    Returns:\n        torch.Tensor: A 2D tensor representing the extracted and processed spectral band.\n            Shape will be (H, W), where H is height and W is width of the image.\n\n    Raises:\n        ValueError: If the specified band name is not found in the spyndex library.\n        NotImplementedError: If a selection method other than \"center\" is specified.\n\n    Notes:\n        - The spyndex library is used to map band names to wavelength ranges.\n        - Currently, only the \"center\" selection method is implemented, which chooses\n        the central wavelength within the specified range.\n        - Processing steps are applied in the order: normalization, cutoff, masking.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(200, 100, 100), wavelengths=np.linspace(400, 2500, 200))\n        &gt;&gt;&gt; red_band = hsi_image.extract_band_by_name(\"Red\")\n        &gt;&gt;&gt; red_band.shape\n        torch.Size([100, 100])\n\n        &gt;&gt;&gt; # Extract NIR band without normalization or masking\n        &gt;&gt;&gt; nir_band = hsi_image.extract_band_by_name(\"NIR\", apply_mask=False, normalize=False)\n    \"\"\"\n    band_info = spyndex.bands.get(band_name)\n    if band_info is None:\n        raise ValueError(f\"Band name '{band_name}' not found in the spyndex library\")\n\n    min_wave, max_wave = band_info.min_wavelength, band_info.max_wavelength\n    selected_wavelengths = self.wavelengths[(self.wavelengths &gt;= min_wave) &amp; (self.wavelengths &lt;= max_wave)]\n\n    if selection_method == \"center\":\n        return self._extract_central_slice_from_band(\n            selected_wavelengths, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=normalize\n        )\n    else:\n        raise NotImplementedError(\n            f\"Selection method '{selection_method}' is not supported. Only 'center' is currently available.\"\n        )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_image","title":"<code>get_image(apply_mask=True)</code>","text":"<p>Returns the hyperspectral image data with optional masking applied.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <p>Returns:     torch.Tensor: The hyperspectral image data.</p> Notes <ul> <li>If apply_mask is True, the binary mask will be applied to the image based on the <code>binary_mask</code> attribute.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; image = hsi_image.get_image()\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n&gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n&gt;&gt;&gt; image.shape\ntorch.Size([10, 100, 100])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_image(self, apply_mask: bool = True) -&gt; torch.Tensor:\n    \"\"\"Returns the hyperspectral image data with optional masking applied.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n    Returns:\n        torch.Tensor: The hyperspectral image data.\n\n    Notes:\n        - If apply_mask is True, the binary mask will be applied to the image based on the `binary_mask` attribute.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; image = hsi_image.get_image()\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n        &gt;&gt;&gt; image = hsi_image.get_image(apply_mask=False)\n        &gt;&gt;&gt; image.shape\n        torch.Size([10, 100, 100])\n    \"\"\"\n    if apply_mask and self.binary_mask is not None:\n        return self.image * self.binary_mask\n    return self.image\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.get_rgb_image","title":"<code>get_rgb_image(apply_mask=True, apply_min_cutoff=False, output_channel_axis=None)</code>","text":"<p>Extracts an RGB representation from the hyperspectral image data.</p> <p>This method creates a 3-channel RGB image by selecting appropriate bands corresponding to red, green, and blue wavelengths from the hyperspectral data.</p> <p>Parameters:</p> Name Type Description Default <code>apply_mask</code> <code>bool</code> <p>Whether to apply the binary mask to the image. Defaults to True.</p> <code>True</code> <code>apply_min_cutoff</code> <code>bool</code> <p>Whether to apply a minimum intensity cutoff to the image. Defaults to False.</p> <code>False</code> <code>output_channel_axis</code> <code>int | None</code> <p>The axis where the RGB channels should be placed in the output tensor. If None, uses the current spectral axis of the hyperspectral data. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The RGB representation of the hyperspectral image. Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on the specified output_channel_axis, where H is height and W is width.</p> Notes <ul> <li>The RGB bands are extracted using predefined wavelength ranges for R, G, and B.</li> <li>Each band is normalized independently before combining into the RGB image.</li> <li>If apply_mask is True, masked areas will be set to zero in the output.</li> <li>If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.</li> </ul> <p>Examples:</p> <pre><code>&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([3, 100, 100])\n</code></pre> <pre><code>&gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n&gt;&gt;&gt; rgb_image.shape\ntorch.Size([100, 100, 3])\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_rgb_image(\n    self, apply_mask: bool = True, apply_min_cutoff: bool = False, output_channel_axis: int | None = None\n) -&gt; torch.Tensor:\n    \"\"\"Extracts an RGB representation from the hyperspectral image data.\n\n    This method creates a 3-channel RGB image by selecting appropriate bands\n    corresponding to red, green, and blue wavelengths from the hyperspectral data.\n\n    Args:\n        apply_mask (bool, optional): Whether to apply the binary mask to the image.\n            Defaults to True.\n        apply_min_cutoff (bool, optional): Whether to apply a minimum intensity\n            cutoff to the image. Defaults to False.\n        output_channel_axis (int | None, optional): The axis where the RGB channels\n            should be placed in the output tensor. If None, uses the current spectral\n            axis of the hyperspectral data. Defaults to None.\n\n    Returns:\n        torch.Tensor: The RGB representation of the hyperspectral image.\n            Shape will be either (H, W, 3), (3, H, W), or (H, 3, W) depending on\n            the specified output_channel_axis, where H is height and W is width.\n\n    Notes:\n        - The RGB bands are extracted using predefined wavelength ranges for R, G, and B.\n        - Each band is normalized independently before combining into the RGB image.\n        - If apply_mask is True, masked areas will be set to zero in the output.\n        - If apply_min_cutoff is True, a minimum intensity threshold is applied to each band.\n\n    Examples:\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 100, 100), wavelengths=np.linspace(400, 1000, 10))\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image()\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(output_channel_axis=0)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([3, 100, 100])\n\n        &gt;&gt;&gt; rgb_image = hsi_image.get_rgb_image(apply_mask=False, apply_min_cutoff=True)\n        &gt;&gt;&gt; rgb_image.shape\n        torch.Size([100, 100, 3])\n    \"\"\"\n    if output_channel_axis is None:\n        output_channel_axis = self.spectral_axis\n\n    rgb_img = torch.stack(\n        [\n            self.extract_band_by_name(\n                band, apply_mask=apply_mask, apply_min_cutoff=apply_min_cutoff, normalize=True\n            )\n            for band in [\"R\", \"G\", \"B\"]\n        ],\n        dim=self.spectral_axis,\n    )\n\n    return (\n        rgb_img\n        if output_channel_axis == self.spectral_axis\n        else torch.moveaxis(rgb_img, self.spectral_axis, output_channel_axis)\n    )\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.to","title":"<code>to(device)</code>","text":"<p>Moves the image and binary mask (if available) to the specified device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str or device</code> <p>The device to move the image and binary mask to.</p> required <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The updated HSI object.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # Create an HSI object\n&gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n&gt;&gt;&gt; # Move the image to cpu\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cpu')\n&gt;&gt;&gt; # Move the image to cuda\n&gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n&gt;&gt;&gt; hsi_image.device\ndevice(type='cuda', index=0)\n</code></pre> Source code in <code>src/meteors/hsi.py</code> <pre><code>def to(self, device: str | torch.device) -&gt; Self:\n    \"\"\"Moves the image and binary mask (if available) to the specified device.\n\n    Args:\n        device (str or torch.device): The device to move the image and binary mask to.\n\n    Returns:\n        Self: The updated HSI object.\n\n    Examples:\n        &gt;&gt;&gt; # Create an HSI object\n        &gt;&gt;&gt; hsi_image = HSI(image=torch.rand(10, 10, 10), wavelengths=np.arange(10))\n        &gt;&gt;&gt; # Move the image to cpu\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cpu\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cpu')\n        &gt;&gt;&gt; # Move the image to cuda\n        &gt;&gt;&gt; hsi_image = hsi_image.to(\"cuda\")\n        &gt;&gt;&gt; hsi_image.device\n        device(type='cuda', index=0)\n    \"\"\"\n    self.image = self.image.to(device)\n    self.binary_mask = self.binary_mask.to(device)\n    self.device = self.image.device\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.hsi.HSI.validate_image_data","title":"<code>validate_image_data()</code>","text":"<p>Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.</p> <p>Returns:</p> Name Type Description <code>Self</code> <code>Self</code> <p>The instance of the class.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>@model_validator(mode=\"after\")\ndef validate_image_data(self) -&gt; Self:\n    \"\"\"Validates the image data by checking the shape of the wavelengths, image, and spectral_axis.\n\n    Returns:\n        Self: The instance of the class.\n    \"\"\"\n    validate_shapes(self.wavelengths, self.image, self.spectral_axis)\n    return self\n</code></pre>"},{"location":"reference/#src.meteors.hsi.ensure_image_tensor","title":"<code>ensure_image_tensor(image)</code>","text":"<p>Ensures the input image is a PyTorch tensor, converting it if necessary.</p> <p>This function validates that the input is either a numpy array or a PyTorch tensor, and converts numpy arrays to PyTorch tensors. If the input is already a PyTorch tensor, it is returned unchanged.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray | Tensor</code> <p>The input image to be converted/validated.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The image as a PyTorch tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the image is neither a numpy array nor a PyTorch tensor.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def ensure_image_tensor(image: np.ndarray | torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Ensures the input image is a PyTorch tensor, converting it if necessary.\n\n    This function validates that the input is either a numpy array or a PyTorch tensor,\n    and converts numpy arrays to PyTorch tensors. If the input is already a PyTorch tensor,\n    it is returned unchanged.\n\n    Args:\n        image (np.ndarray | torch.Tensor): The input image to be converted/validated.\n\n    Returns:\n        torch.Tensor: The image as a PyTorch tensor.\n\n    Raises:\n        TypeError: If the image is neither a numpy array nor a PyTorch tensor.\n    \"\"\"\n    if not isinstance(image, (torch.Tensor, np.ndarray)):\n        raise TypeError(\"Image must be either a numpy array or a PyTorch tensor\")\n\n    if isinstance(image, np.ndarray):\n        image = torch.from_numpy(image)\n    return image\n</code></pre>"},{"location":"reference/#src.meteors.hsi.ensure_wavelengths_tensor","title":"<code>ensure_wavelengths_tensor(wavelengths)</code>","text":"<p>Converts the input wavelengths to a PyTorch tensor.</p> <p>This function takes wavelength data in various formats and ensures it is converted to a PyTorch tensor. It accepts PyTorch tensors, NumPy arrays, lists, or tuples of numeric values.</p> <p>Parameters:</p> Name Type Description Default <code>wavelengths</code> <code>Tensor | ndarray | list[int | float] | tuple[int | float]</code> <p>The input wavelengths in any of the supported formats.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: The wavelengths as a PyTorch tensor.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If the input is not a PyTorch tensor, NumPy array, list, or tuple.</p> <code>ValueError</code> <p>If the wavelengths cannot be converted to a PyTorch tensor.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def ensure_wavelengths_tensor(\n    wavelengths: torch.Tensor | np.ndarray | list[int | float] | tuple[int | float],\n) -&gt; torch.Tensor:\n    \"\"\"Converts the input wavelengths to a PyTorch tensor.\n\n    This function takes wavelength data in various formats and ensures it is converted\n    to a PyTorch tensor. It accepts PyTorch tensors, NumPy arrays, lists, or tuples of\n    numeric values.\n\n    Args:\n        wavelengths (torch.Tensor | np.ndarray | list[int | float] | tuple[int | float]):\n            The input wavelengths in any of the supported formats.\n\n    Returns:\n        torch.Tensor: The wavelengths as a PyTorch tensor.\n\n    Raises:\n        TypeError: If the input is not a PyTorch tensor, NumPy array, list, or tuple.\n        ValueError: If the wavelengths cannot be converted to a PyTorch tensor.\n    \"\"\"\n    if not isinstance(wavelengths, (torch.Tensor, np.ndarray, list, tuple)):\n        raise TypeError(\"Wavelengths must be a PyTorch tensor, NumPy array, list, or tuple of numeric values\")\n\n    try:\n        if not isinstance(wavelengths, torch.Tensor):\n            wavelengths = torch.as_tensor(wavelengths)\n    except Exception as e:\n        raise ValueError(f\"Failed to convert wavelengths to a PyTorch tensor: {str(e)}\") from e\n\n    return wavelengths\n</code></pre>"},{"location":"reference/#src.meteors.hsi.get_channel_axis","title":"<code>get_channel_axis(orientation)</code>","text":"<p>Returns the index of the channel axis in the orientation list.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>The index of the band axis.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def get_channel_axis(orientation: tuple[str, str, str]) -&gt; int:\n    \"\"\"Returns the index of the channel axis in the orientation list.\n\n    Returns:\n        int: The index of the band axis.\n    \"\"\"\n    return orientation.index(\"C\")\n</code></pre>"},{"location":"reference/#src.meteors.hsi.process_and_validate_binary_mask","title":"<code>process_and_validate_binary_mask(mask, info)</code>","text":"<p>Processes and validates a binary mask, ensuring it's in the correct format and shape.</p> <p>This function handles various input types for binary masks, including None (creates a mask of ones), 'artificial' (creates a mask based on the first channel of the image), numpy arrays, and PyTorch tensors. It ensures the output is a PyTorch tensor of the correct shape and type.</p> <p>Parameters:</p> Name Type Description Default <code>mask</code> <code>ndarray | Tensor | None | str</code> <p>The input binary mask or mask specification. - If None: Creates a mask of ones with the same shape as the image. - If 'artificial': Creates a mask based on the first channel of the image. - If numpy array or PyTorch tensor: Converts to a boolean PyTorch tensor.</p> required <code>info</code> <code>ValidationInfo</code> <p>A dataclass containing additional required information: - 'image': The reference image (PyTorch tensor) for shape and device. - 'orientation': The orientation of the image data. - 'device': The PyTorch device to use.</p> required <p>Returns:</p> Type Description <code>Tensor</code> <p>torch.Tensor: A boolean PyTorch tensor representing the validated and processed binary mask.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input mask is invalid or if required information is missing from info.</p> <code>ValueError</code> <p>If the resulting binary mask doesn't match the shape of the reference image.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def process_and_validate_binary_mask(\n    mask: np.ndarray | torch.Tensor | None | str, info: ValidationInfo\n) -&gt; torch.Tensor:\n    \"\"\"Processes and validates a binary mask, ensuring it's in the correct format and shape.\n\n    This function handles various input types for binary masks, including None (creates a mask of ones),\n    'artificial' (creates a mask based on the first channel of the image), numpy arrays, and PyTorch tensors.\n    It ensures the output is a PyTorch tensor of the correct shape and type.\n\n    Args:\n        mask (np.ndarray | torch.Tensor | None | str): The input binary mask or mask specification.\n            - If None: Creates a mask of ones with the same shape as the image.\n            - If 'artificial': Creates a mask based on the first channel of the image.\n            - If numpy array or PyTorch tensor: Converts to a boolean PyTorch tensor.\n        info (ValidationInfo): A dataclass containing additional required information:\n            - 'image': The reference image (PyTorch tensor) for shape and device.\n            - 'orientation': The orientation of the image data.\n            - 'device': The PyTorch device to use.\n\n    Returns:\n        torch.Tensor: A boolean PyTorch tensor representing the validated and processed binary mask.\n\n    Raises:\n        ValueError: If the input mask is invalid or if required information is missing from info.\n        ValueError: If the resulting binary mask doesn't match the shape of the reference image.\n    \"\"\"\n    if mask is not None and not isinstance(mask, (torch.Tensor, np.ndarray, str)):\n        raise ValueError(\"Binary mask must be None, a PyTorch tensor, a numpy array, or the string 'artificial'\")\n\n    if \"image\" not in info.data or \"orientation\" not in info.data or \"device\" not in info.data:\n        raise ValueError(\"Missing required information in ValidationInfo\")\n\n    image: torch.Tensor = info.data[\"image\"]\n    spectral_axis: int = get_channel_axis(info.data[\"orientation\"])\n    device: torch.device = info.data[\"device\"]\n\n    if mask is None:\n        binary_mask = torch.ones_like(image, dtype=torch.bool, device=device)\n    elif isinstance(mask, np.ndarray):\n        binary_mask = torch.as_tensor(mask, device=device, dtype=torch.bool)\n    elif isinstance(mask, str):\n        if mask == \"artificial\":\n            binary_mask = torch.index_select(image, 0, torch.tensor([0], device=device)).bool()[0]\n            binary_mask = torch.repeat_interleave(\n                binary_mask.unsqueeze(dim=spectral_axis),\n                repeats=image.shape[spectral_axis],\n                dim=spectral_axis,\n            )\n        else:\n            raise ValueError(\"String mask specification must be 'artificial'\")\n    else:\n        binary_mask = mask.bool().to(device)\n\n    if binary_mask.shape != image.shape:\n        try:\n            binary_mask = binary_mask.expand_as(image)\n        except RuntimeError:\n            raise ValueError(f\"Binary mask shape {binary_mask.shape} does not match image shape {image.shape}\")\n\n    return binary_mask\n</code></pre>"},{"location":"reference/#src.meteors.hsi.resolve_inference_device_hsi","title":"<code>resolve_inference_device_hsi(device, info)</code>","text":"<p>Resolves and returns the device to be used for inference.</p> <p>This function determines the appropriate PyTorch device for inference based on the input parameters and available information. It handles three scenarios: 1. If a specific device is provided, it validates and returns it. 2. If no device is specified (None), it uses the device of the input tensor image. 3. If a string is provided, it attempts to convert it to a torch.device.</p> <p>Parameters:</p> Name Type Description Default <code>device</code> <code>str | device | None</code> <p>The desired device for inference. If None, the device of the input hsi will be used.</p> required <code>info</code> <code>ValidationInfo</code> <p>An object containing additional validation information, including the input hsi data.</p> required <p>Returns:</p> Type Description <code>device</code> <p>torch.device: The resolved PyTorch device for inference.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If no device is specified and the hsi is not present in the info data, or if the provided device string is invalid.</p> <code>TypeError</code> <p>If the provided device is neither None, a string, nor a torch.device.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def resolve_inference_device_hsi(device: str | torch.device | None, info: ValidationInfo) -&gt; torch.device:\n    \"\"\"Resolves and returns the device to be used for inference.\n\n    This function determines the appropriate PyTorch device for inference based on the input\n    parameters and available information. It handles three scenarios:\n    1. If a specific device is provided, it validates and returns it.\n    2. If no device is specified (None), it uses the device of the input tensor image.\n    3. If a string is provided, it attempts to convert it to a torch.device.\n\n    Args:\n        device (str | torch.device | None): The desired device for inference.\n            If None, the device of the input hsi will be used.\n        info (ValidationInfo): An object containing additional validation information,\n            including the input hsi data.\n\n    Returns:\n        torch.device: The resolved PyTorch device for inference.\n\n    Raises:\n        ValueError: If no device is specified and the hsi is not present in the info data,\n            or if the provided device string is invalid.\n        TypeError: If the provided device is neither None, a string, nor a torch.device.\n    \"\"\"\n    if device is None:\n        if \"image\" not in info.data:\n            raise ValueError(\"Hyperspectral image tensor is not present in the data, INTERNAL ERROR\")\n        image: torch.Tensor = info.data[\"image\"]\n        device = image.device\n    elif isinstance(device, str):\n        try:\n            device = torch.device(device)\n        except Exception as e:\n            raise ValueError(f\"Device {device} is not valid\") from e\n    if not isinstance(device, torch.device):\n        raise TypeError(\"Device should be a string or torch device\")\n\n    logger.debug(f\"Device for inference: {device.type}\")\n    return device\n</code></pre>"},{"location":"reference/#src.meteors.hsi.validate_orientation","title":"<code>validate_orientation(value)</code>","text":"<p>Validates the orientation tuple.</p> <p>Parameters:</p> Name Type Description Default <code>value</code> <code>tuple[str, str, str] | list[str] | str</code> <p>The orientation value to be validated. It should be a tuple of three one-letter strings.</p> required <p>Returns:</p> Type Description <code>tuple[str, str, str]</code> <p>tuple[str, str, str]: The validated orientation value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the value is not a tuple of three one-letter strings or if it does not contain 'W', 'H', and 'C' in any order.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def validate_orientation(value: tuple[str, str, str] | list[str] | str) -&gt; tuple[str, str, str]:\n    \"\"\"Validates the orientation tuple.\n\n    Args:\n        value (tuple[str, str, str] | list[str] | str):\n            The orientation value to be validated. It should be a tuple of three one-letter strings.\n\n    Returns:\n        tuple[str, str, str]: The validated orientation value.\n\n    Raises:\n        ValueError: If the value is not a tuple of three one-letter strings\n            or if it does not contain 'W', 'H', and 'C' in any order.\n    \"\"\"\n    if not isinstance(value, tuple):\n        value = tuple(value)  # type: ignore\n\n    if len(value) != 3 or set(value) != {\"H\", \"W\", \"C\"}:\n        raise ValueError(\"Orientation must be a tuple of 'H', 'W', and 'C' in any order.\")\n    return value  # type: ignore\n</code></pre>"},{"location":"reference/#src.meteors.hsi.validate_shapes","title":"<code>validate_shapes(wavelengths, image, spectral_axis)</code>","text":"<p>Validates that the length of the wavelengths matches the number of channels in the image tensor.</p> <p>Parameters:</p> Name Type Description Default <code>wavelengths</code> <code>Tensor</code> <p>Array of wavelengths.</p> required <code>image</code> <code>Tensor</code> <p>Image tensor.</p> required <code>spectral_axis</code> <code>int</code> <p>Index of the band axis in the image tensor.</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If the length of wavelengths does not correspond to the number of channels in the image tensor.</p> Source code in <code>src/meteors/hsi.py</code> <pre><code>def validate_shapes(wavelengths: torch.Tensor, image: torch.Tensor, spectral_axis: int) -&gt; None:\n    \"\"\"Validates that the length of the wavelengths matches the number of channels in the image tensor.\n\n    Args:\n        wavelengths (torch.Tensor): Array of wavelengths.\n        image (torch.Tensor): Image tensor.\n        spectral_axis (int): Index of the band axis in the image tensor.\n\n    Raises:\n        ValueError: If the length of wavelengths does not correspond to the number of channels in the image tensor.\n    \"\"\"\n    if wavelengths.shape[0] != image.shape[spectral_axis]:\n        raise ValueError(\"Length of wavelengths must match the number of channels in the image.\")\n</code></pre>"},{"location":"tutorials/lime/","title":"Lime","text":""}]}