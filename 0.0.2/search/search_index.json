{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"meteors","text":"<p>draft of the package for creating explanations of hyperspectral and multispectral images</p> <p>Documentation: https://xai4space.github.io/meteors/latest/</p>"},{"location":"#how-to-run","title":"How to Run","text":"<pre><code>rye pin &lt;python version between '3.9' and '3.12'&gt;\nrye sync\n</code></pre>"},{"location":"changelog/","title":"Changelog \ud83d\udcdd","text":""},{"location":"changelog/#hyperxai-001","title":"HyperXAI 0.0.1","text":"<ul> <li>Prepared a simple draft of package along with some ideas and sample files for implementation of LIME for hyperspectral images.</li> <li>Segmentation mask for LIME using slic</li> <li>Spatial attributions using LIME</li> </ul>"},{"location":"changelog/#hyperxai-002","title":"HyperXAI 0.0.2","text":"<ul> <li>Refined package structure - simple modules for models and visualisation, installation using toml file</li> <li>Spectral attributions using LIME</li> <li>CUDA compatibility of LIME</li> </ul>"},{"location":"changelog/#hyperxai-meteors-001","title":"HyperXAI -&gt; meteors 0.0.1","text":"<ul> <li>Renamed package to meteors</li> </ul>"},{"location":"how-to-guides/","title":"How-to guide","text":"<p>TODO: Add a how-to guide here.</p>"},{"location":"quickstart/","title":"Quickstart","text":"<p>TODO: Add a quickstart guide here.</p>"},{"location":"reference/","title":"API Reference","text":"<p>Structure:</p> <ul> <li>API Reference</li> <li>Lime</li> </ul>"},{"location":"reference/#lime","title":"Lime","text":""},{"location":"reference/#src.meteors.lime.ImageSpatialAttributes","title":"<code>ImageSpatialAttributes</code>","text":"<p>               Bases: <code>ImageAttributes</code></p> Source code in <code>src/meteors/lime.py</code> <pre><code>class ImageSpatialAttributes(ImageAttributes):\n    segmentation_mask: Annotated[\n        np.ndarray | torch.Tensor,\n        Field(\n            kw_only=False,\n            validate_default=True,\n            description=\"Segmentation mask used for the explanation.\",\n        ),\n    ]\n\n    _flattened_segmentation_mask: torch.Tensor = None\n\n    @model_validator(mode=\"after\")\n    def validate_segmentation_mask(self) -&gt; Self:\n        if isinstance(self.segmentation_mask, np.ndarray):\n            self.segmentation_mask = torch.tensor(self.segmentation_mask, device=self._device)\n\n        if self.segmentation_mask.device != self._device:\n            self.segmentation_mask = self.segmentation_mask.to(self._device)  # move to the device\n\n        return self\n\n    def to(self, device: torch.device) -&gt; Self:\n        super().to(device)\n        self.segmentation_mask = self.segmentation_mask.to(device)\n        return self\n\n    def get_flattened_segmentation_mask(self) -&gt; torch.tensor:\n        \"\"\"segmentation mask is after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor\"\"\"\n        if self._flattened_segmentation_mask is None:\n            self._flattened_segmentation_mask = self.segmentation_mask.select(dim=self.image.band_axis, index=0)\n        return self._flattened_segmentation_mask\n\n    def get_flattened_attributes(self) -&gt; torch.tensor:\n        \"\"\"attributions for spatial case are after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor\"\"\"\n        if self._flattened_attributes is None:\n            self._flattened_attributes = self.attributes.select(dim=self.image.band_axis, index=0)\n        return self._flattened_attributes\n</code></pre>"},{"location":"reference/#src.meteors.lime.ImageSpatialAttributes.get_flattened_attributes","title":"<code>get_flattened_attributes()</code>","text":"<p>attributions for spatial case are after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor</p> Source code in <code>src/meteors/lime.py</code> <pre><code>def get_flattened_attributes(self) -&gt; torch.tensor:\n    \"\"\"attributions for spatial case are after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor\"\"\"\n    if self._flattened_attributes is None:\n        self._flattened_attributes = self.attributes.select(dim=self.image.band_axis, index=0)\n    return self._flattened_attributes\n</code></pre>"},{"location":"reference/#src.meteors.lime.ImageSpatialAttributes.get_flattened_segmentation_mask","title":"<code>get_flattened_segmentation_mask()</code>","text":"<p>segmentation mask is after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor</p> Source code in <code>src/meteors/lime.py</code> <pre><code>def get_flattened_segmentation_mask(self) -&gt; torch.tensor:\n    \"\"\"segmentation mask is after all only two dimensional tensor with some repeated values, this function returns only two-dimensional tensor\"\"\"\n    if self._flattened_segmentation_mask is None:\n        self._flattened_segmentation_mask = self.segmentation_mask.select(dim=self.image.band_axis, index=0)\n    return self._flattened_segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.lime.ImageSpectralAttributes","title":"<code>ImageSpectralAttributes</code>","text":"<p>               Bases: <code>ImageAttributes</code></p> Source code in <code>src/meteors/lime.py</code> <pre><code>class ImageSpectralAttributes(ImageAttributes):\n    band_mask: Annotated[\n        np.ndarray | torch.Tensor,\n        Field(\n            kw_only=False,\n            validate_default=True,\n            description=\"Band mask used for the explanation.\",\n        ),\n    ]\n    band_names: Annotated[\n        dict[str, int],\n        Field(\n            kw_only=False,\n            validate_default=True,\n            description=\"Dictionary that translates the band names into the segment values.\",\n        ),\n    ]\n\n    _flattened_band_mask = None\n\n    @model_validator(mode=\"after\")\n    def validate_band_mask(self) -&gt; Self:\n        if isinstance(self.band_mask, np.ndarray):\n            self.band_mask = torch.tensor(self.band_mask, device=self._device)\n\n        if self.band_mask.device != self._device:\n            self.band_mask = self.band_mask.to(self._device)\n\n        if 0 not in self.band_names.values() and 0 in torch.unique(self.band_mask):\n            self.band_names[\"not_included\"] = 0\n\n        return self\n\n    def to(self, device: torch.device) -&gt; Self:\n        super().to(device)\n        self.band_mask = self.band_mask.to(device)\n        return self\n\n    def get_flattened_band_mask(self) -&gt; torch.tensor:\n        \"\"\"band mask is after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor\"\"\"\n        if self._flattened_band_mask is None:\n            dims_to_select = [2, 1, 0]\n            dims_to_select.remove(self.image.band_axis)\n            self._flattened_band_mask = self.band_mask.select(dim=dims_to_select[0], index=0).select(\n                dim=dims_to_select[1], index=0\n            )\n        return self._flattened_band_mask\n\n    def get_flattened_attributes(self) -&gt; torch.tensor:\n        \"\"\"attributions for spectral case are after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor\"\"\"\n        if self._flattened_attributes is None:\n            dims_to_select = [2, 1, 0]\n            dims_to_select.remove(self.image.band_axis)\n            self._flattened_attributes = self.attributes.select(dim=dims_to_select[0], index=0).select(\n                dim=dims_to_select[1], index=0\n            )\n        return self._flattened_attributes\n</code></pre>"},{"location":"reference/#src.meteors.lime.ImageSpectralAttributes.get_flattened_attributes","title":"<code>get_flattened_attributes()</code>","text":"<p>attributions for spectral case are after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor</p> Source code in <code>src/meteors/lime.py</code> <pre><code>def get_flattened_attributes(self) -&gt; torch.tensor:\n    \"\"\"attributions for spectral case are after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor\"\"\"\n    if self._flattened_attributes is None:\n        dims_to_select = [2, 1, 0]\n        dims_to_select.remove(self.image.band_axis)\n        self._flattened_attributes = self.attributes.select(dim=dims_to_select[0], index=0).select(\n            dim=dims_to_select[1], index=0\n        )\n    return self._flattened_attributes\n</code></pre>"},{"location":"reference/#src.meteors.lime.ImageSpectralAttributes.get_flattened_band_mask","title":"<code>get_flattened_band_mask()</code>","text":"<p>band mask is after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor</p> Source code in <code>src/meteors/lime.py</code> <pre><code>def get_flattened_band_mask(self) -&gt; torch.tensor:\n    \"\"\"band mask is after all only one dimensional tensor with some repeated values, this function returns only one-dimensional tensor\"\"\"\n    if self._flattened_band_mask is None:\n        dims_to_select = [2, 1, 0]\n        dims_to_select.remove(self.image.band_axis)\n        self._flattened_band_mask = self.band_mask.select(dim=dims_to_select[0], index=0).select(\n            dim=dims_to_select[1], index=0\n        )\n    return self._flattened_band_mask\n</code></pre>"},{"location":"reference/#src.meteors.lime.Lime","title":"<code>Lime</code>","text":"<p>               Bases: <code>Explainer</code></p> Source code in <code>src/meteors/lime.py</code> <pre><code>class Lime(Explainer):\n    # should it be any different than base lime?\n    explainable_model: ExplainableModel\n    interpretable_model: InterpretableModel\n    similarity_func: Callable | None = None\n    perturb_func: Callable | None = None\n\n    _lime = None\n\n    @model_validator(mode=\"after\")\n    def construct_lime(self) -&gt; Self:\n        self._lime = LimeBase(\n            forward_func=self.explainable_model.forward_func,\n            interpretable_model=self.interpretable_model,\n            similarity_func=self.similarity_func,\n            perturb_func=self.perturb_func,\n        )\n\n        return self\n\n    def to(self, device: torch.device) -&gt; Self:\n        super().to(device)\n        # self.interpretable_model.to(device)\n        return self\n\n    @staticmethod\n    def get_segmentation_mask(\n        image: Image,\n        segmentation_method: Literal[\"patch\", \"slic\"] = \"slic\",\n        segmentation_method_params={},\n    ) -&gt; torch.Tensor:\n        if segmentation_method == \"slic\":\n            return Lime.__get_slick_segmentation_mask(image, **segmentation_method_params)\n        if segmentation_method == \"patch\":\n            return Lime.__get_patch_segmentation_mask(image, **segmentation_method_params)\n        raise NotImplementedError(\"Only slic and patch methods are supported for now\")\n\n    @staticmethod\n    def get_band_mask(\n        image: Image,\n        band_names: list[str | Sequence[str]] | dict[tuple[str, ...] | str, int],\n    ) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n        \"\"\"function generates band mask - an array that corresponds to the image, which values are different segments.\n        Args:\n            image (Image): A Hyperspectral image\n            band_names ((List[str | List[str]]) | (Dict[str | List[str], Iterable[int]])): list of band names that should be treated as one segment or dictionary containing\n\n        Returns:\n            Tuple[torch.Tensor, Dict[str, int]]: a tuple which consists of an actual band mask and a dictionary that translates the band names into the segment values\n        \"\"\"\n\n        if isinstance(band_names, dict):\n            band_names_dict = {tuple(k) if not isinstance(k, str) else k: v for k, v in band_names.items()}\n        else:\n            band_names_dict = Lime.__get_band_dict_from_list(band_names)  # type: ignore\n\n        band_names_simplified = {\n            str(segment[0]) if isinstance(segment, tuple) and len(segment) == 1 else segment: value\n            for segment, value in band_names_dict.items()\n        }\n\n        return (\n            Lime.__get_band_mask_from_names_dict(image, band_names_dict),  # type: ignore\n            band_names_simplified,  # type: ignore\n        )\n\n    @staticmethod\n    def __get_band_dict_from_list(\n        band_names_list: list[str | Sequence[str]],\n    ) -&gt; dict[tuple[str, ...], int]:\n        band_names_dict = {}\n        for idx, segment in enumerate(band_names_list):\n            if isinstance(segment, str):\n                segment = [segment]\n            segment = tuple(segment)\n\n            band_names_dict[segment] = idx + 1\n        return band_names_dict\n\n    @staticmethod\n    def __get_band_mask_from_names_dict(image: Image, band_names: dict[tuple[str, ...] | str, int]) -&gt; torch.Tensor:\n        grouped_band_names = Lime.__get_grouped_band_names(band_names)\n\n        device = image.image.device\n        resolution_segments = Lime.__get_resolution_segments(image.wavelengths, grouped_band_names, device=device)\n\n        axis = [0, 1, 2]\n        axis.remove(image.band_axis)\n\n        band_mask = resolution_segments.unsqueeze(axis[0]).unsqueeze(axis[1])\n        size_image = image.image.size()\n        size_mask = band_mask.size()\n\n        repeat_dims = [s2 // s1 for s1, s2 in zip(size_mask, size_image)]\n        band_mask = band_mask.repeat(repeat_dims)\n\n        return band_mask\n\n    @staticmethod\n    def __get_grouped_band_names(\n        band_names: dict[tuple[str, ...] | str, int],\n    ) -&gt; dict[tuple[str, ...], int]:\n        # function extracts band names or indices based on the spyndex library\n        # also checks if the given names are valid\n\n        grouped_band_names = {}\n\n        for segment in band_names.keys():\n            band_names_segment: list[str] = []\n            if isinstance(segment, str):\n                segment = tuple([segment])\n\n            for band_name in segment:\n                if band_name in spyndex.indices:\n                    band_names_segment = band_names_segment + (spyndex.indices[band_name].bands)\n                elif band_name in spyndex.bands:\n                    band_names_segment.append(band_name)\n                else:\n                    raise ValueError(\n                        f\"Invalid band name {band_name}, band name must be either in `spyndex.indices` or `spyndex.bands`\"\n                    )\n\n            grouped_band_names[tuple(band_names_segment)] = band_names[segment]\n\n        return grouped_band_names\n\n    @staticmethod\n    def __get_resolution_segments(\n        wavelengths: np.ndarray,\n        band_names: dict[tuple[str, ...], int],\n        device=\"cpu\",\n    ) -&gt; torch.Tensor:\n        resolution_segments = torch.zeros(len(wavelengths), dtype=torch.int64, device=device)\n\n        segments = list(band_names.keys())\n        for segment in segments[::-1]:\n            for band_name in segment:\n                min_wavelength = spyndex.bands[band_name].min_wavelength\n                max_wavelength = spyndex.bands[band_name].max_wavelength\n\n                for wave_idx, wave_val in enumerate(wavelengths):\n                    if min_wavelength &lt;= wave_val &lt;= max_wavelength:\n                        resolution_segments[wave_idx] = band_names[segment]\n\n        unique_segments = torch.unique(resolution_segments)\n        for segment in band_names.keys():\n            if band_names[segment] not in unique_segments:\n                display_name = segment\n                print(f\"bands {display_name} not found in the wavelengths or bands are overlapping\")\n        return resolution_segments\n\n    def get_spatial_attributes(\n        self,\n        image: Image,\n        segmentation_mask: np.ndarray | torch.Tensor | None = None,\n        target=None,\n        segmentation_method: Literal[\"slic\", \"patch\"] = \"slic\",\n        segmentation_method_params: dict | None = {},\n    ) -&gt; ImageSpatialAttributes:\n        assert self._lime is not None, \"Lime object not initialized\"\n\n        assert self.explainable_model.problem_type == \"regression\", \"For now only the regression problem is supported\"\n\n        if segmentation_mask is None:\n            segmentation_mask = self.get_segmentation_mask(image, segmentation_method, segmentation_method_params)\n\n        if isinstance(image.image, np.ndarray):\n            image.image = torch.tensor(image.image, device=self._device)\n        elif isinstance(image.image, torch.Tensor):\n            image.image = image.image.to(self._device)\n\n        if isinstance(image.binary_mask, np.ndarray):\n            image.binary_mask = torch.tensor(image.image, device=self._device)\n        elif isinstance(image.binary_mask, torch.Tensor):\n            image.binary_mask = image.binary_mask.to(self._device)\n\n        if isinstance(segmentation_mask, np.ndarray):\n            segmentation_mask = torch.tensor(segmentation_mask, device=self._device)\n        elif isinstance(segmentation_mask, torch.Tensor):\n            segmentation_mask = segmentation_mask.to(self._device)\n\n        assert (\n            segmentation_mask.device == self._device\n        ), f\"Segmentation mask should be on the same device as explainable model {self._device}\"\n        assert (\n            image.image.device == self._device\n        ), f\"Image data should be on the same device as explainable model {self._device}\"\n\n        assert isinstance(self._lime, LimeBase), \"Lime object not initialized\"\n\n        lime_attributes, score = self._lime.attribute(\n            inputs=image.image.unsqueeze(0),\n            target=target,\n            feature_mask=segmentation_mask.unsqueeze(0),\n            n_samples=10,\n            perturbations_per_eval=4,\n            show_progress=True,\n            return_input_shape=True,\n        )\n\n        spatial_attribution = ImageSpatialAttributes(\n            image=image,\n            attributes=lime_attributes[0],\n            segmentation_mask=segmentation_mask,\n            score=score,\n        )\n\n        return spatial_attribution\n\n    def get_spectral_attributes(\n        self,\n        image: Image,\n        band_mask: np.ndarray | torch.Tensor | None = None,\n        target=None,\n        band_names: list[str] | dict[str | tuple[str, ...], int] | None = None,\n        verbose=False,\n    ) -&gt; ImageSpectralAttributes:\n        assert self._lime is not None, \"Lime object not initialized\"\n\n        assert self.explainable_model.problem_type == \"regression\", \"For now only the regression problem is supported\"\n\n        if isinstance(image.image, np.ndarray):\n            image.image = torch.tensor(image.image, device=self._device)\n        elif isinstance(image.image, torch.Tensor):\n            image.image = image.image.to(self._device)\n\n        if isinstance(image.binary_mask, np.ndarray):\n            image.binary_mask = torch.tensor(image.image, device=self._device)\n        elif isinstance(image.binary_mask, torch.Tensor):\n            image.binary_mask = image.binary_mask.to(self._device)\n\n        assert (\n            image.image.device == self._device\n        ), f\"Image data should be on the same device as explainable model {self._device}\"\n\n        if band_mask is None:\n            band_mask, band_names = self.get_band_mask(image, band_names)  # type: ignore\n        elif band_names is None:\n            unique_segments = torch.unique(band_mask)\n            band_names = {segment: idx for idx, segment in enumerate(unique_segments)}\n        else:\n            # checking consistency of names\n            # unique_segments = torch.unique(band_mask)\n            # if isinstance(band_names, dict):\n            #     assert set(unique_segments).issubset(set(band_names.values())), \"Incorrect band names\"\n            pass\n\n        if isinstance(band_mask, np.ndarray):\n            band_mask = torch.tensor(band_mask, device=self._device)\n        else:\n            band_mask = band_mask.to(self._device)\n\n        assert (\n            band_mask.device == self._device\n        ), f\"Band mask should be on the same device as explainable model {self._device}\"\n\n        lime_attributes, score = self._lime.attribute(\n            inputs=image.image.unsqueeze(0),\n            target=target,\n            feature_mask=band_mask.unsqueeze(0),\n            n_samples=10,\n            perturbations_per_eval=4,\n            show_progress=verbose,\n            return_input_shape=True,\n        )\n\n        lime_attributes = lime_attributes[0]\n\n        spectral_attribution = ImageSpectralAttributes(\n            image=image,\n            attributes=lime_attributes,\n            band_mask=band_mask,\n            band_names=band_names,\n            score=score,\n        )\n\n        return spectral_attribution\n\n    @staticmethod\n    def __get_slick_segmentation_mask(image: Image, num_interpret_features: int, *args, **kwargs) -&gt; torch.tensor:\n        device = image.image.device\n        numpy_image = np.array(image.image.to(\"cpu\"))\n        segmentation_mask = slic(\n            numpy_image,\n            n_segments=num_interpret_features,\n            mask=np.array(image.get_flattened_binary_mask().to(\"cpu\")),\n            channel_axis=image.band_axis,\n            *args,\n            **kwargs,\n        )\n\n        if np.min(segmentation_mask) == 1:\n            segmentation_mask -= 1\n\n        # segmentation_mask = np.repeat(np.expand_dims(segmentation_mask, axis=image.band_axis), repeats=image.image.shape[image.band_axis], axis=image.band_axis)\n        segmentation_mask = torch.tensor(segmentation_mask, dtype=torch.int64, device=device)\n        segmentation_mask = torch.unsqueeze(segmentation_mask, dim=image.band_axis)\n        # segmentation_mask = torch.repeat_interleave(torch.unsqueeze(segmentation_mask, dim=image.band_axis), repeats=image.image.shape[image.band_axis], dim=image.band_axis)\n        return segmentation_mask\n\n    @staticmethod\n    def __get_patch_segmentation_mask(image: Image, patch_size=10, *args, **kwargs) -&gt; torch.tensor:\n        print(\"Patch segmentation only works for band_index = 0 now\")\n\n        device = image.image.device\n        if image.image.shape[1] % patch_size != 0 or image.image.shape[2] % patch_size != 0:\n            raise ValueError(\"Invalid patch_size. patch_size must be a factor of both width and height of the image\")\n\n        height, width = image.image.shape[1], image.image.shape[2]\n\n        mask_zero = torch.tensor(image.image.bool()[0], device=device)\n        idx_mask = torch.arange(height // patch_size * width // patch_size, device=device).reshape(\n            height // patch_size, width // patch_size\n        )\n        idx_mask += 1\n        segmentation_mask = torch.repeat_interleave(idx_mask, patch_size, dim=0)\n        segmentation_mask = torch.repeat_interleave(segmentation_mask, patch_size, dim=1)\n        segmentation_mask = segmentation_mask * mask_zero\n        # segmentation_mask = torch.repeat_interleave(torch.unsqueeze(segmentation_mask, dim=image.band_axis), repeats=image.image.shape[image.band_axis], dim=image.band_axis)\n        segmentation_mask = torch.unsqueeze(segmentation_mask, dim=image.band_axis)\n\n        mask_idx = np.unique(segmentation_mask).tolist()\n        for idx, mask_val in enumerate(mask_idx):\n            segmentation_mask[segmentation_mask == mask_val] = idx\n\n        return segmentation_mask\n</code></pre>"},{"location":"reference/#src.meteors.lime.Lime.get_band_mask","title":"<code>get_band_mask(image, band_names)</code>  <code>staticmethod</code>","text":"<p>function generates band mask - an array that corresponds to the image, which values are different segments. Args:     image (Image): A Hyperspectral image     band_names ((List[str | List[str]]) | (Dict[str | List[str], Iterable[int]])): list of band names that should be treated as one segment or dictionary containing</p> <p>Returns:</p> Type Description <code>tuple[Tensor, dict[tuple[str, ...] | str, int]]</code> <p>Tuple[torch.Tensor, Dict[str, int]]: a tuple which consists of an actual band mask and a dictionary that translates the band names into the segment values</p> Source code in <code>src/meteors/lime.py</code> <pre><code>@staticmethod\ndef get_band_mask(\n    image: Image,\n    band_names: list[str | Sequence[str]] | dict[tuple[str, ...] | str, int],\n) -&gt; tuple[torch.Tensor, dict[tuple[str, ...] | str, int]]:\n    \"\"\"function generates band mask - an array that corresponds to the image, which values are different segments.\n    Args:\n        image (Image): A Hyperspectral image\n        band_names ((List[str | List[str]]) | (Dict[str | List[str], Iterable[int]])): list of band names that should be treated as one segment or dictionary containing\n\n    Returns:\n        Tuple[torch.Tensor, Dict[str, int]]: a tuple which consists of an actual band mask and a dictionary that translates the band names into the segment values\n    \"\"\"\n\n    if isinstance(band_names, dict):\n        band_names_dict = {tuple(k) if not isinstance(k, str) else k: v for k, v in band_names.items()}\n    else:\n        band_names_dict = Lime.__get_band_dict_from_list(band_names)  # type: ignore\n\n    band_names_simplified = {\n        str(segment[0]) if isinstance(segment, tuple) and len(segment) == 1 else segment: value\n        for segment, value in band_names_dict.items()\n    }\n\n    return (\n        Lime.__get_band_mask_from_names_dict(image, band_names_dict),  # type: ignore\n        band_names_simplified,  # type: ignore\n    )\n</code></pre>"},{"location":"tutorials/lime/","title":"Lime","text":""}]}